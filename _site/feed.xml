<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-04-17T15:49:28+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">KEEPMIND</title><subtitle>A place I record so that I don&apos;t forget.</subtitle><author><name>Joonsu Ryu</name></author><entry><title type="html">빙과 블루레이</title><link href="http://localhost:4000/Unboxing/hyouka-bluray/" rel="alternate" type="text/html" title="빙과 블루레이" /><published>2023-01-31T00:00:00+09:00</published><updated>2023-01-31T00:00:00+09:00</updated><id>http://localhost:4000/Unboxing/hyouka-bluray</id><content type="html" xml:base="http://localhost:4000/Unboxing/hyouka-bluray/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지 올렸던 많은 블루레이 관련 포스트에서 짐작하셨을 수도 있지만, 저는 재밌게 본 작품은 블루레이로 소장하고 있습니다.&lt;/p&gt;

&lt;p&gt;소장하는 이유에는 여러가지가 있지만, 가장 큰 이유는 정발되는 애니메이션 블루레이에는 한국어 더빙판이 포함되어 있는 경우가 많기 때문입니다.&lt;/p&gt;

&lt;p&gt;한국어 더빙에 대한 호불호는 사람마다 갈리지만, 저는 한국어 더빙만이 주는 또 다른 느낌이 있기 때문에 선호하는 편입니다.&lt;/p&gt;

&lt;p&gt;빙과를 포함한 몇몇 블루레이는 예전부터 구하고 싶었지만, 매물이 없어서 그동안 구하지 못하고 있었는데요, 다행히 미라지 엔터테인먼트에서 매진되었던 많은 애니메이션 작품들을 작년 12월에 재발매해주어서 쉽게 구할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;다른 작품들도 구매할 예정인데, 한번에 다 구매하기에는 부담이 되어서 하나씩 구매하려고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/01.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/02.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;슬립 케이스의 디자인은 책 느낌이 나는 깔끔한 디자인입니다.&lt;/p&gt;

&lt;p&gt;작 중에서 나왔던 고전부의 문집과 비슷한 모양인 것 같네요.&lt;/p&gt;

&lt;p&gt;원래 처음 발매할 때는 11.5화도 포함되어 있었는데, 제가 구매한 파이널 에디션은 11.5화가 포함되어 있지 않습니다.&lt;/p&gt;

&lt;p&gt;특별화이기 때문에 메인 스토리와 연관은 없지만, 그래도 뭔가 빠져 있다는 것은 아쉬운 느낌이네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;블루레이는 총 6개가 들어있습니다.&lt;/p&gt;

&lt;p&gt;디스크가 총 11장 들어있고, 총 22화까지 있으니 디스크 한 장에 2화씩 들어있을 것 같네요.&lt;/p&gt;

&lt;p&gt;각 디스크의 표지는 해당 화에 중요한 인물들이 그려져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/06.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 첫 번째 케이스입니다. 주역들이 고전부에 가입하게 되는 에피소드인데, 이바라 마야카가 빠져있네요.&lt;/p&gt;

&lt;p&gt;그 캐릭터도 나름 매력있었는데 왜 빠져있을까요? 안나오는 것도 아니었는데.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;디스크 디자인은 깔끔하게 되어 있습니다.&lt;/p&gt;

&lt;p&gt;괜히 어설프게 캐릭터 박아놓는 것보다 이게 훨씬 나은 것 같아요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;배경에는 고전부 부실이 그려져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째 케이스는 오레키 호타로와 분쟁이 있었던 선배와의 모습이 나와 있습니다.&lt;/p&gt;

&lt;p&gt;뒤에서 안절부절 못하는 치탄다 에루가 귀엽네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/12.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/13.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째 디스크는 파란색입니다.&lt;/p&gt;

&lt;p&gt;배경은 치탄다 에루의 집인 것 같네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/14.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/15.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;세 번째 디스크에는 바보의 엔드 크레디트 편의 핵심 인물인 이리스 후유미가 있습니다.&lt;/p&gt;

&lt;p&gt;이리스 후유미가 쿨하고 섹시한 캐릭터라 좋았는데, 이후에 잘 나오지 않아서 아쉬웠어요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/16.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/17.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;디스크는 보라색, 배경에는 바보의 엔드 크레디트에 나온 영화의 건물이 그려져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/18.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/19.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;네 번째 케이스는 칸야제 에피소드의 한 장면이 나와 있습니다.&lt;/p&gt;

&lt;p&gt;오레키 호타로가 왜 밀가루를 들고 있는 이유는 스포일러가 될 수 있으니 적지 않겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/20.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/21.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;디스크는 다홍색으로 보이고 배경에는 칸야제 장식으로 꾸며진 고등학교가 그려져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/22.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/23.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다섯 번째 케이스에는 오레키 호타로와 후쿠베 사토시 뒤에 무녀 옷을 입은 이바라 마야카가 있습니다.&lt;/p&gt;

&lt;p&gt;치탄다 에루도 신년 에피소드에서 나름 귀엽게 나왔는데 빠져있네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/24.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/25.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;디스크는 회색으로 나와있는데, 케이스 안쪽에는 뜬금없이 도서실이 그러져 있습니다.&lt;/p&gt;

&lt;p&gt;이 부분에서 도서실이 그렇게 중요한 장소가 아니었던 것으로 기억하는데, 무슨 이유일까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/26.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/27.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막 디스크의 표지는 후쿠베 사토시의 멱살을 잡고 있는 오레키 호타로가 그려져 있습니다.&lt;/p&gt;

&lt;p&gt;둘 사이의 관계는 초기 에피소드에서도 뭔가 찜찜한 구석이 있었는데 터질게 터졌다는 느낌이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/28.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/072/29.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;보통 일본 애니메이션은 1쿨당 12화가 일반적인데, 빙과를 포함한 몇몇 애니메이션은 1쿨당 11화로 구성되어 있습니다.&lt;/p&gt;

&lt;p&gt;그래서 이렇게 블루레이 디스크가 한 개만 들어있습니다.&lt;/p&gt;

&lt;p&gt;마지막 디스크는 황금색이고, 배경은 작중 배경의 도시 같은데 정확히 어떤 에피소드에 나왔는지 기억이 안나네요.&lt;/p&gt;

&lt;p&gt;마을 축제 에피소드의 배경 같은데 확실하진 않습니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">POPUP PARADE 마마마 피규어</title><link href="http://localhost:4000/Unboxing/puella-magi-madoka-magica-figure/" rel="alternate" type="text/html" title="POPUP PARADE 마마마 피규어" /><published>2023-01-08T00:00:00+09:00</published><updated>2023-01-08T00:00:00+09:00</updated><id>http://localhost:4000/Unboxing/puella-magi-madoka-magica-figure</id><content type="html" xml:base="http://localhost:4000/Unboxing/puella-magi-madoka-magica-figure/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최근 제가 올린 게시물을 보시면 알겠지만, 저는 마법소녀 마도카☆마기카를 굉장이 재밌게 보았습니다.&lt;/p&gt;

&lt;p&gt;그래서 마마마에 관련된 이런 저런 굿즈를 모으고 있는데요, 마침 타이밍 좋게 마마마 피규어가 새롭게 출시되어 이 참에 구매했습니다.&lt;/p&gt;

&lt;p&gt;원래 피규어는 상당히 고가의 물건인데, POPUP PARADE 시리즈는 피규어 입문자를 위해 저가로 판매하는 브랜드라고 합니다.&lt;/p&gt;

&lt;p&gt;사실 지금까지 피규어를 사 본 적이 없어서 저가 피규어와 고가 피규어의 차이를 몰라서 그냥 구매했습니다.&lt;/p&gt;

&lt;p&gt;피규어 관련 커뮤니티에서도 가격 만큼의 값을 한다는 평가가 많았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저는 매니아하우스에서 구매했습니다.&lt;/p&gt;

&lt;p&gt;다만 분명히 같은 시리즈인데 마도카/호무라/마미는 입고가 완료되었지만, 사야카와 쿄코는 예약구매로 팔고 있었습니다.&lt;/p&gt;

&lt;p&gt;사야카는 10월, 쿄코는 12월에나 입고된다고 하네요.&lt;/p&gt;

&lt;p&gt;같은 시리즈인데 출시 날짜가 다른 것도 이상하지만, 왜 쿄코와 사야카만 값이 더 비싼지 모르겠네요.&lt;/p&gt;

&lt;p&gt;아무래도 이 둘의 인기가 나머지 셋 보다 더 좋아서일까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/02.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 마도카/호무라/마미를 배송받고 찍어본 사진입니다.&lt;/p&gt;

&lt;p&gt;개당 4만원 정도라 큰 기대는 하지 않았는데, 생각보다 사이즈가 컸습니다. (사실 해피밀 장난감 정도의 크기를 예상했습니다)&lt;/p&gt;

&lt;p&gt;피규어 크기도 생각보다 컸지만, 피규어를 보호하기 위함인지 박스의 크기도 굉장히 컸습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 주인공인 카나메 마도카의 피규어입니다.&lt;/p&gt;

&lt;p&gt;사진을 이상하게 찍어서 색감이 좀 이상한데, 실제로는 박스 아래쪽이 핑크색입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구성품은 마도카 본체, 받침대, 그리고 무기(활)로 나뉘어져 있습니다.&lt;/p&gt;

&lt;p&gt;무기 구조가 좀 특이한데, 활이 통째로 들어있는 것이 아니라, 윗부분/아랫부분으로 나뉘어서 손 위/아래에 끼우는 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;완성한 모습의 정면입니다.&lt;/p&gt;

&lt;p&gt;조립은 어린아이도 쉽게 할 수 있을 정도로 쉽습니다.&lt;/p&gt;

&lt;p&gt;다만 무기를 조립할 때 어떤 부분이 윗부분인지 헷갈렸습니다.&lt;/p&gt;

&lt;p&gt;다행히 제품 소개에 완성 이미지가 있기 때문에, 그걸 보고 확인하면 되긴 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/06.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마도카의 뒷모습입니다.&lt;/p&gt;

&lt;p&gt;애니에서는 뒷 모습을 볼 일이 없어서 몰랐는데, 등 부분에 하트 모양으로 옷이 파여있었네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 본 모습입니다.&lt;/p&gt;

&lt;p&gt;이 부분이 사실 유일하게 조금 아쉬운 부분인데, 머리 부분에 접합선이 너무 적나라하게 나와있습니다.&lt;/p&gt;

&lt;p&gt;그래서 가급적이면 이 피규어는 시선보다 낮은 위치에 배치하지 않는 것이 좋습니다.&lt;/p&gt;

&lt;p&gt;한번 눈에 띄면 계속 거슬리거든요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;치마의 구조가 굉장히 신기합니다.&lt;/p&gt;

&lt;p&gt;애니에서도 그렇고 피규어도 그렇고 꼭 중세 유럽의 귀족 치마처럼 펑 퍼진 느낌인데, 아래쪽에서 보면 그 구조를 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;치마에 대해서 잘 모르긴 하지만, 저렇게 여러겹으로 만들어진 구조라 치마가 퍼진 것 같습니다.&lt;/p&gt;

&lt;p&gt;근데 실제로 이런 치마가 있을까요? 있어도 불편하지 않을까요?&lt;/p&gt;

&lt;p&gt;그리고 뭔가 치마에 비해 다리가 가늘어서 재밌네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째는 진주인공이나 마찬가지인 아케미 호무라의 피규어입니다.&lt;/p&gt;

&lt;p&gt;애니에서는 안경을 쓴 모습과 냉정한 모습 두 가지로 나오는데, 아쉽게도 이 피규어는 안경 쓴 모습으로 구현되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;작중에서 호무라의 대표적인 무기라고 할 만한 것이 없었기 때문에, 무기는 없고 피규어 본체와 받침대만 들어있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;호무라 뒤에는 허리춤에 묶은 리본이 있는데, 이 리본을 보호하기 위함인지 별도의 비닐로 조심스럽게 포장되어 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/12.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정면 모습입니다. 조립은 그냥 받침에 피규어를 끼우기만 하면 되서 매우 간단합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/13.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒷모습도 리본을 제외하면 딱히 특이한 점은 없었습니다.&lt;/p&gt;

&lt;p&gt;솔직한 감상으로는 다섯명 중 가장 제작비가 적을 것 같은 느낌입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/14.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그나마 볼만한 점은 옷의 주름이 매우 잘 표현되어 있다는 것입니다.&lt;/p&gt;

&lt;p&gt;사진과 같은 허리 부분 뿐만 아니라 이곳 저곳 옷이 접힌 부분이 자연스러워 보였습니다.&lt;/p&gt;

&lt;p&gt;물론 이 점은 다른 캐릭터도 동일합니다만, 호무라의 옷에 주름이 많아서 그런지 특히 눈에 잘 띄었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/15.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 보면 마도카와 마찬가지로 머리의 접합선이 그대로 나와있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/16.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래에서 보면 마도카와 다르게 치마의 속이 차있지는 않습니다.&lt;/p&gt;

&lt;p&gt;다만 스타킹의 색이 상당히 짙어서 그런지 팬티는 구현되어 있지 않습니다. 저런걸 팬티 스타킹이라고 하던가요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/17.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로는 토모에 마미입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/18.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구성품은 토모에 마미 본체, 받침대, 그리고 무기(총 2자루)로 구성되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/19.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;무기 2개가 얼핏 보면 똑같이 보이지만, 자세히 보면 조금 다릅니다.&lt;/p&gt;

&lt;p&gt;하나는 바닥에 꽂아야 하고 하나는 팔에 장착해야 하는데, 총구 끝을 보면 구분이 가능합니다.&lt;/p&gt;

&lt;p&gt;총구가 조금 더 긴 왼쪽이 바닥에 꽂는 부품입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/20.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/21.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 총 조립이 상당히 어렵습니다.&lt;/p&gt;

&lt;p&gt;손에 튀어나온 부분을 총의 홈에 끼워 맞춰야하는데, 이 작업이 생각보다 쉽지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/22.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;간신히 조립을 완성한 모습을 보습니다.&lt;/p&gt;

&lt;p&gt;손에 들고 있는 총이 머리 뒤로 넘어가야 하는데, 총이 머리와 머리카락 사이에 들어가게 만들어야 해서 상당히 힘들었습니다.&lt;/p&gt;

&lt;p&gt;게다가 총이 꽉 맞는 것이 아니라 손으로 건드리면 흔들거릴 정도라서 불안합니다.&lt;/p&gt;

&lt;p&gt;피규어를 처음 사보는거라 마감이 나쁜 것인지, 아니면 제가 받은 제품이 불량인지를 모르겠습니다.&lt;/p&gt;

&lt;p&gt;그리고 바닥에 끼우는 총도, 본체를 조립하기 전에 먼저 끼워야 합니다.&lt;/p&gt;

&lt;p&gt;저는 조립 순서를 몰라서 한번 분해하고 다시 조립했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/23.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒷모습입니다. 허리에 리본이 가장 눈에 띄네요.&lt;/p&gt;

&lt;p&gt;개인적으로 호무라의 리본보다 훨씬 마음에 듭니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/24.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정면 부분의 디자인도 괜찮게 되어 있습니다.&lt;/p&gt;

&lt;p&gt;코르셋 부분이 도색하기 어려웠을 것 같은데, 가격에 비해 상당히 괜찮은 마감이라고 생각합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/25.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마미의 소울젬인 브로치도 제대로 구현되어 있습니다.&lt;/p&gt;

&lt;p&gt;다만 단순 도색이라 그런지 원작처럼 반짝반짝 빛나지는 않습니다.&lt;/p&gt;

&lt;p&gt;사실 보석 부분의 도색 마감 자체가 조금 별로긴 한데, 이 가격대에 그런 것까지 기대할 순 없을 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/26.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 마도카나 호무라와 다르게 팬티가 구현되어 있습니다.&lt;/p&gt;

&lt;p&gt;사실 마미가 작품 내 비중이 많은 편이 아니기 때문에 반역의 이야기 극장판에서 서비스씬 비슷한 장면도 나왔는데요, 그래서 이런 컨셉(?)이 유지된 것일지도 모르겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/27.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사진에서도 티가 납니다만, 미키 사야카는 다른 캐릭터들에 비해 포장 사이즈 자체가 큽니다.&lt;/p&gt;

&lt;p&gt;왜 그런지 생각해봤는데, 다른 캐릭터들과 달리 사야카는 처음부터 무기가 부착되어 나오기 때문이 아닌가 싶습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/28.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구성품은 단순히 본체와 받침대 뿐입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/29.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;받침대는 뒷발만으로 전체를 지탱하는 구조로 되어 있습니다.&lt;/p&gt;

&lt;p&gt;자세가 뭔가 묘한데, 앞발이 공중에 떠있습니다.&lt;/p&gt;

&lt;p&gt;실제로 저런 자세로는 서있기 어렵지 않을까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/30.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게다가 미키 사야카는 왜인지 정면을 쳐다보는 구조로 되어 있지 않습니다.&lt;/p&gt;

&lt;p&gt;왜 얘만 옆을 보는 구조일까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/31.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 사야카의 복장은 나머지 캐릭터에 비해 굉장히 노출도가 높습니다.&lt;/p&gt;

&lt;p&gt;애니에서는 대부분 망토로 가려져서 알아채기 힘든데, 이렇게 보면 상의는 오프숄더에 배꼽도 나와 있습니다.&lt;/p&gt;

&lt;p&gt;게다가 치마도 미니스커트 수준으로 짧습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/32.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사야카의 상징인 음표 모양의 브로치도 제대로 구현되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/33.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒷모습은 망토가 몸 대부분을 가려주고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/34.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;역시 사야카도 머리 접합선은 피해갈 수 없었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/35.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사야카의 치마가 비대칭 형태인데, 앞이 길고 뒤가 짧은 구조입니다.&lt;/p&gt;

&lt;p&gt;자세 때문인지는 몰라도 이게 상당히 위험한 옷인데요, 망토가 없다면 뒤에서도 팬티가 살짝 보일 만한 각도입니다.&lt;/p&gt;

&lt;p&gt;당연히 아래에서 보면 이렇게 적나라하게 팬티가 보입니다.&lt;/p&gt;

&lt;p&gt;오른쪽에서 보면 허벅지 전체가 드러날 정도네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/36.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 사쿠라 쿄코입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/37.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구성품은 본체, 무기(창), 받침대입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/38.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;머리카락이 상당히 역동적으로 흩날려 보이는 구조인데, 그래서 그런지 머리카락이 비닐로 한 겹 더 포장되어 있습니다.&lt;/p&gt;

&lt;p&gt;그런데 팔은 왜 비닐로 감싸져있는지 모르겠네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/39.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;무기는 별도의 조립이 필요 없이 바로 장착이 가능하게끔 되어 있습니다.&lt;/p&gt;

&lt;p&gt;하단 부분에 약간 튀어나온 부분이 있는데, 저 부분을 손에 장착시키는 구조인 것으로 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/40.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;완성한 모습입니다.&lt;/p&gt;

&lt;p&gt;사야카 때도 느낀거지만, 쿄코도 자세가 뭔가 기묘하네요.&lt;/p&gt;

&lt;p&gt;실제로 창을 저런 자세로 잡으면 등이 굉장히 아플텐데요.&lt;/p&gt;

&lt;p&gt;사야카 관련 굿즈나 일러스트를 보면 겨드랑이를 강조한 자세가 많던데, 이것도 그렇네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/41.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금보니 자세도 자세지만 고개도 뭔가 부자연스럽다는 느낌이 들긴 하네요.&lt;/p&gt;

&lt;p&gt;그리고 움직이는 자세가 아님에도 불구하고 바람이 부는듯 치마가 뒤로 펄럭이고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/42.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;창은 생각보다 장착하기 쉽지 않았습니다.&lt;/p&gt;

&lt;p&gt;저렇게 팔과 몸 사이에 창을 끼워 넣어 손에 장착하는 구조인데, 딱 맞지 않고 저렇게 틈이 남습니다.&lt;/p&gt;

&lt;p&gt;이렇게 만들거면 차라리 사야카처럼 그냥 일체형으로 만들어줬으면 좋았을 텐데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/43.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒤에서 본 자세는 무난합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/44.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금보니 쿄코의 속치마도 사야카처럼 굉장히 짧습니다.&lt;/p&gt;

&lt;p&gt;옆에서 보는데도 불구하고 팬티가 살짝 보일 정도네요.&lt;/p&gt;

&lt;p&gt;쿄코의 저 겉치마가 사야카의 망토처럼 뒤를 가려주는 역할인가 봅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/45.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다른 캐릭터와 마찬가지로 팬티는 흰색입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/46.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/071/47.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다섯 명을 선반에 장식해 보았습니다.&lt;/p&gt;

&lt;p&gt;원래는 아래 그림처럼 애니메이션 구도로 장식하고 싶었는데, 선반 가로 길이의 한계 때문에 일렬로 놓지는 못하겠네요.&lt;/p&gt;

&lt;p&gt;저는 사야카와 쿄코를 좋아해서 그 두 명을 앞에 놓고 나머지 3명을 뒤에 장식해 놓았습니다.&lt;/p&gt;

&lt;p&gt;다 합쳐서 20만원 정도 들었는데, 저는 생각보다 만족스러웠습니다.&lt;/p&gt;

&lt;p&gt;사실 고품질 피규어 하나에 20~30만원 정도 하는데, 20만원으로 세트 전체를 구매한다는 것 자체가 가성비가 괜찮은 것 같습니다.&lt;/p&gt;

&lt;p&gt;물론 고가의 피규어보다는 퀄리티가 떨어지겠지만, 멀리서 보면 그다지 세세한 부분이 눈에 들어오지는 않습니다.&lt;/p&gt;

&lt;p&gt;POPUP PARADE 시리즈는 저처럼 피규어에 처음 입문하시는 분들에게 나쁘지 않은 선택인 것 같습니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">Amazon에서 구매한 PSP</title><link href="http://localhost:4000/Unboxing/psp-from-amazon/" rel="alternate" type="text/html" title="Amazon에서 구매한 PSP" /><published>2022-12-17T00:00:00+09:00</published><updated>2022-12-17T00:00:00+09:00</updated><id>http://localhost:4000/Unboxing/psp-from-amazon</id><content type="html" xml:base="http://localhost:4000/Unboxing/psp-from-amazon/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저는 어지간하면 중고 제품을 구매하지 않습니다. 특히 전자기기는 무조건 신품으로 사는 편인데, 겉보기만으로는 기기의 상태를 제대로 알기 힘들기 때문입니다.&lt;/p&gt;

&lt;p&gt;하지만 신품을 구하는 것이 불가능하거나, 가능하더라도 가격이 너무 비싼 경우에는 어쩔 수 없이 중고를 구매합니다. 저는 보통 소장용으로 구매하기 때문에, 중고를 구매할 때도 상태를 굉장히 꼼꼼히 확인하는 편입니다.&lt;/p&gt;

&lt;p&gt;중고를 구매할 때 제가 제일 먼저 알아보는 곳은 일본 아마존입니다. 미국 아마존과는 달리 일본 아마존은 제품의 상태를 신뢰할 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;미국 아마존에서 Very Good은 실제로 Very Good인 상태와 거리가 멀지만, 일본 아마존에서는 진짜 그 상태로 배달이 됩니다. (Like New는 실제로 신품 같은 경우가 많았습니다.)&lt;/p&gt;

&lt;p&gt;지난번 포스트에서 마마마 PSP 게임을 샀는데, 저는 PSP를 갖고 있지 않았습니다. 그래서 이 참에 기기를 하나 마련하기 위해 일본 아마존을 찾아보았습니다. 마침 괜찮은 가격에 Like New 제품이 있었기 때문에 악세서리와 같이 구매하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PSP 본체는 약 11만원, 파우치는 약 2만 5천원, 보호필름은 7천원 정도에 구매했습니다.&lt;/p&gt;

&lt;p&gt;파우치의 가격이 좀 비싼 감이 있긴 하지만, 일본 제품이라 믿고 구매했습니다.&lt;/p&gt;

&lt;p&gt;보호필름은 예비용을 포함해 2개 구매했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/02.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;제품을 배송 받고, 택배 상자에서 제품을 꺼낸 모습입니다.&lt;/p&gt;

&lt;p&gt;파우치와 보호 필름은 제품 사진 그대로 왔는데 PSP 본체는 랩 같은 재질로 둘둘 말아져 도착했습니다.&lt;/p&gt;

&lt;p&gt;그래도 Like New라 꼼꼼히 포장해줄 것으로 기대했는데, 제가 너무 많이 기대했나 봅니다.&lt;/p&gt;

&lt;p&gt;솔직히 이 상태로 온 것은 조금 실망스러웠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;보시다시피 구성품이 전부 랩을 돌돌 감아져 있습니다.&lt;/p&gt;

&lt;p&gt;맨 아래에 있는 것이 본체고, 그 위에 충전기와 배터리가 있네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;랩을 벗겨보니 구성품의 상태는 나름 괜찮습니다.&lt;/p&gt;

&lt;p&gt;충전기와 배터리, 본체 모두 개별 포장이 되어 있네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/06.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본체의 상태는 놀랄 만큼 괜찮았습니다.&lt;/p&gt;

&lt;p&gt;역시 Like New라고 부를 수 있을 정도네요.&lt;/p&gt;

&lt;p&gt;사진에서도 볼 수 있듯이 액정에 흠집 하나 없이 깨끗했습니다.&lt;/p&gt;

&lt;p&gt;그 외에 부분도 눈에 띄는 흠집은 없었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒷 부분에 디스크를 넣는 부분을 열어봤습니다.&lt;/p&gt;

&lt;p&gt;PSP는 다른 휴대용 게임기와 다르게 칩이 아니라 UMD를 사용하는데요, 이 시스템이 참 재밌습니다.&lt;/p&gt;

&lt;p&gt;조그만 디스크가 들어있는 플라스틱 케이스를 넣는 구조거든요.&lt;/p&gt;

&lt;p&gt;이번에 어떤 느낌인지 제대로 볼 수 있을 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;윗 부분에는 기기로 데이터를 넣기 위한 USB Type-B 포트와, 정체 모를 WLAN 스위치가 있네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아랫 부분에는 이어폰 잭과 충전 포트가 있습니다.&lt;/p&gt;

&lt;p&gt;상단의 USB Type-B로도 충전이 가능하다고는 하는데, USB Type-B 케이블 자체를 구하기 어렵기 때문에 쓸 일은 거의 없을 것 같습니다.&lt;/p&gt;

&lt;p&gt;이어폰 잭 옆에 있는 사각형 구멍은 뭘 의미하는지 잘 모르겠네요.&lt;/p&gt;

&lt;h2 id=&quot;파우치&quot;&gt;파우치&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 기기를 담아둘 파우치입니다.&lt;/p&gt;

&lt;p&gt;가격이 조금 비싼 편이었지만 기기의 색과 깔맞춤을 하기 위해 구매했습니다.&lt;/p&gt;

&lt;p&gt;PSP 전 세대와 Vita 까지 사용할 수 있는 크기라고 나와있네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/12.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;제품 사진으로 볼 때는 잘 몰랐는데, 받아보고 나니 뭔가 싼마이한 느낌이 드네요.&lt;/p&gt;

&lt;p&gt;솔직히 이것도 조금 후회됩니다.&lt;/p&gt;

&lt;p&gt;그냥 네이버 쇼핑에서 파는 싸구려 파우치를 살껄 하는 생각이 드네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/13.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파우치를 열어보고 제품을 넣어보니 그럭저럭 비싼 값을 하는 것 같습니다.&lt;/p&gt;

&lt;p&gt;파우치의 두께가 매우 두꺼워서, 어지간히 세게 떨어트리지 않는 이상 제품에 문제는 없을 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;메모리카드--보호필름&quot;&gt;메모리카드 &amp;amp; 보호필름&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/14.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 제가 간과한 점이 있습니다.&lt;/p&gt;

&lt;p&gt;Vita와 마찬가지로 게임을 세이브하기 위해서는 별도의 저장 장치가 필요했습니다.&lt;/p&gt;

&lt;p&gt;하필 메모리스틱이라고 하는 독자 규격 저장장치라 구매를 안 할 수가 없네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/15.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결국 메모리스틱을 새로 구매했습니다.&lt;/p&gt;

&lt;p&gt;다행히 메모리스틱이 더 이상 잘 쓰이지 않는 저장 장치라 가격은 엄청 비싸지는 않았습니다.&lt;/p&gt;

&lt;p&gt;16기가 기준으로 6만원-8만원 정도에 팔고 있습니다.&lt;/p&gt;

&lt;p&gt;다만 일반적인 SD 카드와 비교하면 매우 비싸긴 하네요.&lt;/p&gt;

&lt;h2 id=&quot;기기-세팅&quot;&gt;기기 세팅&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/16.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일본에서 구매한 기기이기 때문에 당연히 기기도 일본어만 가능할 줄 알았는데, 놀랍게도 한국어 설정이 있었습니다.&lt;/p&gt;

&lt;p&gt;다만 저는 일본판 게임을 플레이할 예정이기 때문에 큰 상관은 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/17.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;메모리스틱은 16기가지만, 실제로 사용할 수 있는 용량은 약 14기가로 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/18.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지난 포스트에서 구매한 마법소녀 마도카☆마기카 포터블 디스크를 넣었습니다.&lt;/p&gt;

&lt;p&gt;다만 생각지도 못했던 단점을 하나 발견했는데요, UMD가 디스크를 읽는 방식이다보니 생각보다 이로 인한 소음이 큽니다.&lt;/p&gt;

&lt;p&gt;아직 본격적으로 플레이하지 않아서 잘 모르겠지만, 만약 게임을 플레이하는 내내 이런 소음이 들린다면 무시할 수 없을 정도일 것 같습니다.&lt;/p&gt;

&lt;p&gt;이어폰을 끼면 좀 낫겠지만, 저는 집에서까지 이어폰을 끼면서 플레이하고 싶지는 않네요.&lt;/p&gt;

&lt;p&gt;이 부분은 좀 더 사용해보고 난 뒤에야 판단할 수 있을 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/070/19.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이상하게 생각보다 UMD의 플라스틱 부분이 빠르게 흠집이 납니다.&lt;/p&gt;

&lt;p&gt;분명 새 디스크였는데 눈에 선명할 정도로 플라스틱에 흠집이 나있네요.&lt;/p&gt;

&lt;p&gt;다행히 디스크 자체에 흠집이 생긴건 아닙니다만, 원인을 도통 모르겠습니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">포켓몬스터 스칼렛/바이올렛 더블팩</title><link href="http://localhost:4000/Unboxing/pokemon-scarlet-violet-double-pack/" rel="alternate" type="text/html" title="포켓몬스터 스칼렛/바이올렛 더블팩" /><published>2022-12-09T00:00:00+09:00</published><updated>2022-12-09T00:00:00+09:00</updated><id>http://localhost:4000/Unboxing/pokemon-scarlet-violet-double-pack</id><content type="html" xml:base="http://localhost:4000/Unboxing/pokemon-scarlet-violet-double-pack/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그동안 바빠서 게임을 못하고 있었는데, 어느새 소드/실드가 발매된지 3년이나 지났네요. 그런데 저는 아직도 소드/실드를 못했습니다. 해야지 해야지 하면서도 계속 미루다보니 결국 이렇게 되었네요. 하지만 그렇다고 포켓몬 신작이 발매되었는데 그냥 넘어갈 수는 없죠. 그래서 이번에도 스칼렛/바이올렛 더블팩을 구매했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/01.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저는 분명히 예약 구매를 했지만, 소드/실드 때처럼 예약 구매 특전 같은 것은 따로 오지 않았습니다. 택배 상자를 열어보니 그냥 딱 이렇게 더블팩 박스만 하나 달랑 들어있네요. 점점 닌텐도가 배짱 장사를 하고 있나 하는 생각이 듭니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/02.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒷면에는 게임에 대한 내용이 간단하게 정리되어 있습니다. 어차피 그 나물에 그 밥일테지만, 그럼에도 불구하고 어느 정도의 재미는 보장한다는 것은 인정할 수밖에 없습니다. 예전부터 포켓몬을 제대로 하려면 기기가 2대 필요했는데, 이거 때문에 스위치 라이트를 하나 살까 고민중입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;옆면에는 큼지막한 글씨로 포켓몬스터 스칼렛 바이올렛 더블팩 이라는 글자가 적혀 있습니다. 소드/실드 버전에서도 있었던 것 같은데, 뭔가 묘하게 싼티가 나는 느낌을 지울 수가 없네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;바닥면에는 한국 국내 전용 패키지라는 말과 함께 스타팅 포켓몬 3마리가 나와 있습니다. 이름은 아직 모릅니다만, 풀 속성은 어차피 하드 모드겠죠? 불 속성과 물 속성 중 취향에 맞는 포켓몬을 고르면 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구성품으로는 스칼렛/바이올렛 게임이 각각 들어있고, 더블팩 특전으로 몬스터볼 100개가 담긴 코드 2개가 들어있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/06.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;특전 코드를 확대해 보았습니다. 게임 초반에는 몬스터볼 한 개 구매하는 것도 벅차기 때문에 유용하긴 합니다만, 일회성 코드라는게 조금 아쉽습니다. 일반적인 콘솔 게임 특전처럼 새 게임을 시작하도 얻을 수 있도록 만들어줬으면 더 좋았을텐데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임 패키지 뒷면입니다. 왼쪽이 스칼렛, 오른쪽이 바이올렛입니다. 전체적인 디자인은 동일하나, 자세히 보시면 주인공의 의상이 게임 이름에 맞게 변하는 것을 알 수 있습니다. 스칼렛은 붉은색 느낌, 바이올렛은 보라색 느낌입니다. 다시 보니 맨 위에도 다른 그림이 삽입되어 있네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;케이스 내부 디자인은 영 그렇습니다. 예전에 올린 소드/실드 개봉기에서 내부 디자인을 혹평했었는데요, 다시 보니 그게 선녀라고 생각될 정도네요. 그 때는 그래도 대충 포켓몬 느낌은 났었는데, 이번에는 영… 스칼렛/바이올렛 모두 내부 디자인은 동일합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;패키지의 컨셉대로 스칼렛은 붉은색, 바이올렛은 보라색으로 디자인되어 있습니다. 한국용 제품이다보니 당연히 일련번호도 KOR이 붙어 있습니다.&lt;/p&gt;

&lt;p&gt;사실 포켓몬 게임을 살 때마다 느끼는 것이긴 합니다만, 멀쩡한 게임 한 개를 뜯어서 2개로 나눠파는 상술은 참 악질이라는 생각이 듭니다. 각각의 가격은 다른 패키지와 동일하게 받아먹는데 말이죠. 특히 최근들어 포켓몬 시리즈의 완성도에 대해 계속 불만이 나오는 시점에서는 더욱 돈이 아깝게 느껴집니다. 게다가 이번 작품은 출시 첫날부터 버그에 대한 이슈가 터져나왔죠. 반 년 정도 지나면 버그 이슈도 어느 정도 해결될 것 같으니, 그 때가 되면 슬슬 플레이해볼 생각입니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">마법소녀 마도카☆마기카 굿즈</title><link href="http://localhost:4000/Unboxing/puella-magi-madoka-magica-goods/" rel="alternate" type="text/html" title="마법소녀 마도카☆마기카 굿즈" /><published>2022-10-22T00:00:00+09:00</published><updated>2022-10-22T00:00:00+09:00</updated><id>http://localhost:4000/Unboxing/puella-magi-madoka-magica-goods</id><content type="html" xml:base="http://localhost:4000/Unboxing/puella-magi-madoka-magica-goods/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예전에 마법소녀 마도카 마기카를 감명깊게 봤기 때문에 요즘 관련 굿즈를 모으고 있습니다. 피규어도 구매했지만 12월에나 전부 도착할 것 같아서, 먼저 도착한 물품들만 후기를 작성해보도록 하겠습니다. 오늘 소개할 관련 굿즈는 블루레이와 PSP, Vita 게임입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/01.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;굿즈를 모아놓고 찍어본 사진입니다. 오른쪽부터 Vita 게임, PSP 게임, 그리고 블루레입니다. PSP 게임이 유난히 커 보이는데, 그 이유는 나중에 말씀드리겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;블루레이&quot;&gt;블루레이&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/02.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/06.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 블루레이입니다. 저는 감명깊게 본 작품들은 대부분 블루레이로 소장하는데, 마마마는 나온지 오래 된 애니메이션이다보니 구하기 어려웠습니다. 특히나 한국은 블루레이 시장이 작아 초반을 놓치면 구하기 어렵습니다. 저는 다행히 알라딘에서 미개봉 중고로 구매하였습니다. 보통 알라딘에서 파는 미개봉 중고품은 원래 가격보다 웃돈을 주고 구매해야 하지만, 의외로 인기가 별로 없는지 저는 오히려 더 저렴한 가격으로 구매했습니다. 원가는 66,000원이지만, 저는 그보다 약간 싼 58000원으로 구매했습니다. 현재도 비슷한 가격으로 판매되는 것으로 보아 정말 잘 안팔리나 봅니다. 아마 한국어 더빙이 없기 때문이 아닐까 생각하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;블루레이는 총 3개로 구성되어 있습니다. 왼쪽부터 전반부, 중반부, 후반부인데요, 각 에피소드 별 핵심 인물들이 나타나 있는 것 같습니다. 전반부의 핵심 인물인 토모에 마미가 빠진 것이 의외네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 전반부 블루레이입니다. 토모에 마미가 왜 없나 했더니 내부에 있었네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/12.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/13.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/14.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/15.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;중반부 블루레이입니다. 중반부 스토리의 핵심은 미키 사야카와 사쿠라 쿄코였기 때문에 잘 구성된 디자인이라고 생각합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/16.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/17.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/18.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/19.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;후반부 블루레이입니다. 스포일러가 될 수 있으니 간단하게만 말하면 아케미 호무라와 얼티밋 마도카가 나와있습니다. 내부 디자인은 마도카로 인해 구원받은 마법소녀를 나타낸 것 같네요.&lt;/p&gt;

&lt;h2 id=&quot;배틀-펜타그램-vita-게임&quot;&gt;배틀 펜타그램 (VITA 게임)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/20.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/21.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/22.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로는 비타 게임입니다. 정식 이름은 The Battle Pentagram인데요, 극장판 애니메이션을 기준으로 만들어진 작품입니다. 극장판을 기준으로 했다지만 어차피 극장편도 총집편이었기 때문에 크게 다를 것은 없습니다. 다만 스토리는 독자적인 노선을 타는 것으로 알고 있습니다.&lt;/p&gt;

&lt;p&gt;이 게임은 일반판과 한정판, 두 종류로 발매되었는데, 저는 한정판을 구매했습니다. 일반판은 아직도 세 재품을 구할 수 있었고, 한정판은 중고밖에 없었지만 가격 차이가 그다지 크지 않았기 때문입니다. 게다가 한정판 특전을 보니 한정판을 사는게 낫겠다는 생각이 들었습니다. 왜 그런지는 구성품을 하나하나 소개하면서 말씀드리겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저는 일본 아마존에서 구매했는데, 상태는 Like New 라고 나와있었으며, 가격은 6080엔이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/24.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/25.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반판은 게임 본편 하나만 들어있지만, 한정판은 위와 같이 특별한 포장 안에 게임이 들어있는 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/26.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/27.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일본 아마존의 장점은 Like New 라고 분류된 제품이 진짜 새 것이나 마찬가지라는 점입니다.&lt;/p&gt;

&lt;p&gt;게임 본편은 아예 뜯어보지도 않은 상태였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/28.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임 본편을 뜯어보니 한정판 초회 특전으로 토모에 마미와 이야기할 수 있는 전화번호가 들어있었습니다. 전화번호는 혹시 몰라서 가리긴 했는데, 전화해보니 사용할 수 없는 번호라고 하네요. 저것도 기간 제한이 있었나 봅니다. 9년전 게임이라 이해할 만 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/29.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임 칩은 일반적인 비타 게임과 동일합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/30.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 한정판 구성품입니다. 왼쪽에 있는건 캐릭터 카드, 그리고 아케미 호무라와 이야기할 수 있는 전화번호(물론 안되겠죠?), 한정판 특전 코스튬이 있습니다. 아마 중고라 코스튬 코드는 사용 불가능할 것 같습니다. 사용하지 않았더라도 워낙 발매된 지 오래된 게임이라 유효 기간이 지났을 것 같네요. 사실 이런건 별로 중요하지 않고, 한정판의 존재 가치는 바로 오른쪽 물건입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/31.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/32.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오른쪽의 내용은 오리지널 사운드 트랙이 담긴 CD와 일러스트집입니다. 특히 일러스트 집은 게임을 플레이하기 전과 게임을 플레이하고 난 후의 느낌이 다르기 때문에 좋아하는 특전입니다. 하나 아쉬운 것은 페이지 수가 생각보다 적네요.&lt;/p&gt;

&lt;h2 id=&quot;마마마-포터블-psp-게임&quot;&gt;마마마 포터블 (PSP 게임)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/33.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/34.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/35.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 PSP 게임인 마법소녀 마도카☆마기카 포터블입니다. 이 게임은 워낙 인기가 없었는지 2012년에 출시된 게임에 한정판임에도 불구하고 아직도 신제품이 팔리고 있었습니다. 그래서 구하는 것이 크게 어렵지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/36.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;심지어 가격도 출시가와 비슷한 6천엔 정도입니다. (다만 지금은 가격이 조금 오른 것 같습니다)&lt;/p&gt;

&lt;p&gt;이 한정판의 가장 큰 특징은 피규어가 동봉되었다는 것입니다. 그렇기 때문에 포장 크기도 가장 큰데요. 앞면에서는 보이지 않지만 뒷면을 보면 마도카 피규어가 있는 것을 볼 수 있습니다. 피규어는 반프레스토에서 출시한 피그마 시리즈인데, 그래서인지 패키지에도 반프레스토의 로고가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/37.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;옆면을 열어보면 가운데에 피규어가 들어있고, 양 옆으로 두 개의 디스크가 들어있습니다. 사실상 포장 대부분은 피규어라고 생각하시면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/38.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/39.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/40.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 게임 본편입니다. 케이스의 크기는 스위치 게임과 비슷합니다. 저는 PSP를 어렸을 때 친구가 사용하는 것만 보고 지금까지 써본 적은 없는데, 말로만 듣던 UMD를 실물로 본건 이번에 처음입니다. UMD는 작은 CD가 들어있는 모양인데 디스크에 큐베의 얼굴이 박혀 있어서 귀엽습니다. 왼쪽에는 (요즘 게임에서 보기 힘든) 게임 설명서가 동봉되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/41.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임 설명서는 위와 같이 간단한 캐릭터 소개가 나와 있습니다. 아직 일본어 실력이 미천하여 어떤 내용인지는 잘 모르겠네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/42.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/43.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째로 들어있는 디스크는 스페셜 영상과 오리지널 사운드 트랙, 성우 인터뷰 영상과 같은 부가 요소가 동봉되어 있습니다. 열심히 일본어를 공부해서 언젠가 꼭 보고 싶습니다. 오른쪽 아래에 블루레이 로고가 있는 것으로 보아 블루레이 디스크인 것 같네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/44.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;케이스를 개봉하면 블루레이 디스크가 나오는데, 디스크의 디자인은 게임 UMD 디스크의 디자인과 동일합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/45.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;디스크를 제거하고 내부 디자인을 보니 캐릭터 일러스트가 선화로 그려져 있네요.&lt;/p&gt;

&lt;p&gt;피규어는 이미 갖고 있기 때문에 꺼내지 않았습니다. 꺼내봤자 놀 곳도 딱히 없구요. 보관하기에는 그냥 케이스에 넣어두는 것이 좋을 것 같습니다. 요즘 한창 일본어를 공부하고 있는데, N1을 딴 이후에 천천히 플레이해 볼 예정입니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">Policy Gradient Methods</title><link href="http://localhost:4000/rl/policy-gradient-methods/" rel="alternate" type="text/html" title="Policy Gradient Methods" /><published>2022-10-04T00:00:00+09:00</published><updated>2022-10-04T00:00:00+09:00</updated><id>http://localhost:4000/rl/policy-gradient-methods</id><content type="html" xml:base="http://localhost:4000/rl/policy-gradient-methods/">&lt;p&gt;이번 장은 드디어 마지막 장인 &lt;span style=&quot;color:red&quot;&gt;Policy Gradient&lt;/span&gt;입니다. 이번 장에서는 지금까지 이 교재에서 다룬 방법들과는 다르게, Policy 자체를 매개변수화하는 방법을 알아보겠습니다. 지금까지의 방법들은 Estimated Action-Value를 기반으로 Action을 선택했기 때문에 Action-Value를 추정하는 것이 중요했습니다. 하지만 이번 장에서 배울 새로운 방법인 Policy Gradient는 Action을 선택하는 데 Value Function을 사용하지 않습니다. 이번 장에서 사용할 새로운 표기는 Policy에 대한 매개변수 벡터인 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$입니다. 따라서 Policy는 이제 매개변수 $\boldsymbol{\theta}$를 포함하여 $\pi (a \mid s, \boldsymbol{\theta}) = Pr \{ A_t = a \mid S_t = s, \boldsymbol{\theta}_t = \boldsymbol{\theta} \}$로 표현합니다. 이것은 시간 $t$에서 State가 $s$이고 매개변수가 $\boldsymbol{\theta}$일 때 Action $a$를 선택할 확률로 정의됩니다. 만약 학습 알고리즘 안에서 Value Function에 대한 추정을 포함하는 경우, 이전과 마찬가지로 여전히 Weight Vector $\mathbf{w} \in \mathbb{R}^d$를 포함하여 $\hat{v}(s, \mathbf{w})$로 표현합니다.&lt;/p&gt;

&lt;p&gt;새로 정의하는 Policy 매개변수 $\boldsymbol{\theta}$를 학습하기 위해서는 스칼라 성능 측정 함수인 $J(\boldsymbol{\theta})$를 기반으로 합니다. 당연히 성능을 최대화하는 것이 목적이기 때문에, $J$의 Gradient를 상승시키기 위해 $\boldsymbol{\theta}$의 값을 조절합니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \widehat{\nabla J (\boldsymbol{\theta}_t)} \tag{13.1}\]

&lt;p&gt;식 (13.1)에서 $\widehat{\nabla J (\boldsymbol{\theta}_t)} \in \mathbb{R}^{d’}$는 매개변수 $\boldsymbol{\theta}_t$에 대해 성능을 나타내는 Gradient에 가까운 확률적 추정치입니다. 이러한 과정을 따르는 모든 방법은 Approximate Value Function을 학습하는지에 대한 여부에 상관 없는 Policy Gradient Method라고 합니다. 만약 Policy와 Value Function에 대한 근사값을 모두 학습한다면 Actor-Critic Method라고 합니다. Actor는 Policy를 학습하는 것을 의미하며, Critic은 Value Function을 학습하는 것을 말합니다. 이번 장은 Section 10.3과 마찬가지로 먼저 매개변수된 Policy 하에서의 State에 대한 Value로 성능이 정의되는 Episodic Task를 다룬 후, 성능이 Average Reward로 정의되는 Continuing Task를 다룰 예정입니다. 결국 마지막에는, 두 경우 모두에 대해 매우 유사한 용어를 사용하여 알고리즘을 표현할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;policy-approximation-and-its-advantages&quot;&gt;Policy Approximation and its Advantages&lt;/h2&gt;

&lt;p&gt;Policy Gradient Method에서 $\pi (a \mid s, \boldsymbol{\theta})$가 매개변수 $\boldsymbol{\theta}$에 대해 미분할 수 있고 모든 State $s \in \mathcal{S}$와 모든 Action $a \in \mathcal{A}(s)$, 그리고 매개변수 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$에 대해 유한하다면 Policy는 어떤 방식으로든 매개변수화할 수 있습니다. 실제로, 탐색을 보장하기 위해서는 Policy가 절대 Deterministic이 아니어야 합니다. (즉, 모든 $s, a, \boldsymbol{\theta}$에 대해 $\pi (a \mid s, \boldsymbol{\theta}) \in (0, 1)$) 이번 Section에서는 이산적인 Action Space에서의 가장 일반적인 매개변수화 방법을 소개하고, 그것이 Action-Value 방법에 비해 어떤 장점이 있는지 논의하겠습니다. Policy에 기반한 방법은 (추후 Section 13.7에서 설명하는 것처럼) 연속적인 Action Space가 주어졌을 때 유용한 방법이기도 합니다.&lt;/p&gt;

&lt;p&gt;만약 Action Space가 이산적이고 너무 크지 않다면, 일반적으로 떠올릴 수 있는 방법은 각 State-Action 쌍에 대해 매개변수화하는 Numerical Preference $h(s, a, \boldsymbol{\theta})$를 생성하는 것입니다. 각 State에서 Preference가 가장 높은 Action을 선택할 확률이 높게 만드는 것인데, 대표적인 방법으로 다음과 같은 &lt;span style=&quot;color:red&quot;&gt;Exponential Soft-max Distribution&lt;/span&gt;이 있습니다.&lt;/p&gt;

\[\pi (a|s, \boldsymbol{\theta}) \doteq \frac{e^{h(s, a, \boldsymbol{\theta})}}{\sum_b e^{h(s, a, \boldsymbol{\theta})}} \tag{13.2}\]

&lt;p&gt;식 (13.2)에서 $e \approx 2.71828$는 자연 로그의 밑입니다. 여기서 분모는 각 State의 Action을 선택할 확률의 합이 1이 되도록 설정한 것입니다. 이러한 Policy 매개변수화를 &lt;span style=&quot;color:red&quot;&gt;Soft-max in Action Preference&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Action Preference 설정 자체는 임의로 매개변수화할 수 있습니다. 예를 들어, Deep Artificial Neural Network (ANN)로 계산할 수도 있습니다. 여기서 $\boldsymbol{\theta}$는 네트워크의 모든 Connection Weight의 벡터로 구성됩니다. 또는, 간단하게 다음과 같이 선형으로 나타낼 수도 있습니다.&lt;/p&gt;

\[h(s, a, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\sf T} \mathbf{x} (s,a) \tag{13.3}\]

&lt;p&gt;식 (13.3)에서 $\mathbf{x}  (s, a) \in \mathbb(R)^{d’}$는 Section 9.5에서 설명했던 Feature Vector입니다.&lt;/p&gt;

&lt;p&gt;Action Preference의 Soft-max에 따라 Policy를 매개변수화하는 것의 장점 중 하나는 Approximate Policy가 Deterministic Policy에 점점 가까워진다는 것입니다. 지금까지 많이 사용했던 $\epsilon$-greedy의 경우 항상 무작위 Action을 선택할 확률이 존재합니다. 물론 Action-Value를 기반으로 Soft-max Distribution에 따라 Action을 선택할 수도 있지만, 이것만으로는 Policy가 Deterministic Policy에 가까워질 수 없습니다. 대신 Action-Value의 추정치는 그에 해당하는 Real Value값으로 수렴할 뿐이며, 이것은 0과 1이 아닌 특정 확률로 수렴합니다. Soft-max Distribution에 Temperature 매개변수가 포함된 경우 Temperature는 Deterministic에 접근하기 위해 시간이 지남에 따라 감소할 수 있지만, 실제 Action-Value에 대한 사전 지식 없이는 어느 정도 감소하게 할지, 또는 초기 Temperature를 어떻게 설정할 것인가에 대한 문제가 있기 때문입니다. 하지만 Action Preference는 이러한 특정한 값에 가까워지지 않기 때문에 다릅니다. 이것은 오직 Optimal Stochastic Policy를 유도하는데 주력할 뿐입니다. Optimal Policy가 Deterministic이라면, Optimal Action에 대한 Action Preference는 모든 다른 Action보다 무한히 높아집니다.&lt;/p&gt;

&lt;p&gt;Action Preference의 Soft-max에 따라 Policy를 매개변수화하는 것의 두 번째 장점으로는 임의의 확률로 Action을 선택할 수 있다는 것입니다. 특정한 Function Approximation이 있는 문제에서 Optimal Approximate Policy는 Stochastic일 수도 있습니다. 예를 들어, 불완전한 정보를 가진 카드 게임에서의 최적의 플레이는 포커와 같이 임의의 확률로 블러핑을 하는 것입니다. Action-Value 방법은 이런 경우 Stochastic Optimal Policy를 찾는 방법이 없지만, Policy Gradient Method는 다음 예제와 같이 이것이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 13.1) Short corridor with switched actions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프에 삽입된 작은 Gridworld 문제가 있습니다. Reward는 각 단계당 -1로 설정되어 있습니다. Episode는 항상 S에서 시작하고, G에 도착하면 종료됩니다. 맨 오른쪽 State를 제외한 나머지 State에서는 각각 오른쪽/왼쪽으로 이동하는 2가지 Action이 있습니다. (단, S에서 왼쪽으로 가는 Action은 움직이지 않는 것으로 대체합니다) 이 문제에서 재밌는 점은 왼쪽에서 두 번째 State의 경우, Action에 따른 결과가 반전된다는 것입니다. 즉, 왼쪽을 선택하면 오른쪽으로, 오른쪽을 선택하면 왼쪽으로 움직입니다.&lt;/p&gt;

&lt;p&gt;Function Approximation의 경우 모든 State가 동일하게 간주되므로 해결하기 어렵습니다. 예를 들어 모든 $s$에 대해 $\mathbf{x}(s, \text{right}) = [1, 0]^{\sf T}$, $\mathbf{x}(s, \text{left}) = [0, 1]^{\sf T}$로 정의하면, $\epsilon$-greedy를 사용한 Action-Value 방법은 크게 2가지 Policy만을 생성할 수 있습니다. 하나는 모든 단계에서 높은 확률로 오른쪽을 선택하고 $1 - \epsilon / 2$의 확률로 왼쪽을 선택하는 것이고, 다른 하나는 그 반대를 선택하는 것입니다. 만약 $\epsilon = 0.1$이라면 이 2개의 Policy는 위의 그래프처럼 시작 State에서 -44와 -82의 기대 Value를 각각 얻습니다. 만약 Stochastic Policy를 사용할 수 있다면 훨씬 더 나은 성능을 보일 수 있습니다. 가장 좋은 확률은 오른쪽을 약 0.59의 확률로 선택하는 것이며, 이 때의 Value는 약 -11.6이 됩니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Policy에 대한 매개변수화가 Action-Value 매개변수화에 비해 가질 수 있는 가장 간단한 장점은 Policy가 더 간단한 함수로 근사화할 수 있다는 것입니다. 하지만 이것은 Policy와 Action-Value Function의 복잡성에 따라 다릅니다. 문제에 따라 Action-Value Function이 더 간단하게 근사화할 수도 있기 때문입니다. 다행히 일반적인 경우에는 Policy 기반 방법이 더 빠르게 학습하고 우수한 Policy를 생성할 수 있다는 것이 증명되었습니다. (참고 : Şimşek, Algorta, Kothiyal, 2016)&lt;/p&gt;

&lt;p&gt;마지막으로, Policy를 매개변수하는 것은 때때로 원하는 Policy의 형태에 대한 사전 지식을 강화학습 시스템에 전달하는 좋은 방법이 될 수도 있습니다. 이것은 Policy 기반 학습 방법을 사용하는 가장 큰 이유 중 하나입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-policy-gradient-theorem&quot;&gt;The Policy Gradient Theorem&lt;/h2&gt;

&lt;p&gt;이전 Section에서 설명한 Policy 매개변수화의 장점 외에도 중요한 이론적인 이점이 있습니다. 지속적인 Policy 매개변수화를 사용한 Action 확률은 학습된 매개변수에 대한 함수로 쉽게 표현되는 반면, $\epsilon$-greedy에서의 Action 확률은 추정된 Action-Value의 작은 변화에도 극적으로 변할 수 있습니다. 이로 인해 Policy Gradient Method는 Action-Value 방법보다 더 강력한 수렴을 보장할 수 있습니다. 특히, 매개변수에 대한 Policy 의존성의 연속성이 식 (13.1)에서 Policy Gradient Method가 Gradient Ascent를 사용할 수 있게 보장합니다.&lt;/p&gt;

&lt;p&gt;Episodic Task와 Continuing Task에서 성능 측정 함수 $J(\boldsymbol{\theta})$를 다르게 정의하므로 어느 정도 다르게 취급해야 합니다. 하지만 일단 여기서는 두 가지 경우를 통합하고, 중요한 이론적 결과를 단일 방정식으로 설명할 수 있도록 표기법을 정리할 것입니다.&lt;/p&gt;

&lt;p&gt;이 Section에서는 Episode의 시작 State에 대한 Value를 성능 측정으로 정의하는 Episodic Task를 다루겠습니다. 모든 Episode가 무작위가 아닌 특정한 State $s_0$에서 시작한다고 가정함으로써, 일반성을 잃지 않고 표기법을 단순화할 수 있습니다. 그러면 Episodic Task의 성능을 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}} (s_0) \tag{13.4}\]

&lt;p&gt;식 (13.4)에서 $v_{\pi_{\boldsymbol{\theta}}}$는 $\boldsymbol{\theta}$로 인해 정의된 Policy인 $\pi_{\boldsymbol{\theta}}$를 따를 때의 Real Value Function입니다. 여기에서는 일단 완전성을 위해 알고리즘에서 Discounting을 포함하지만, Episodic Task에서는 Discounting이 없다고 가정하겠습니다. (즉, $\gamma = 1$)&lt;/p&gt;

&lt;p&gt;Function Approximation을 사용하면 Policy Improvement를 보장하는 방식으로 Policy 매개변수를 업데이트하는 것이 어려울 수 있습니다. 문제는 성능이 Action의 선택과, 그러한 선택을 만든 State Distribution 모두에 의존하는데, 이 두 가지 모두 Policy 매개변수에 의해 영향을 받는다는 것입니다. State가 주어졌을 때 Action에 대한 Policy 매개변수의 효과와 Reward에 대한 영향은 매개변수화에 대한 지식으로부터 간단한 방법으로 계산할 수 있습니다. 그러나 State Distribution에 대한 Policy의 영향은 Environment의 함수이기 때문에 일반적으로 알 수 없습니다. 그렇다면 Gradient가 State Distribution에서 알 수 없는 Policy 변경에 의존할 때 Policy 매개변수에 대한 성능의 Gradient를 어떻게 추정해야 할까요?&lt;/p&gt;

&lt;p&gt;다행히 Policy 매개변수에 대한 성능의 Gradient를 분석할 수 있는 &lt;span style=&quot;color:red&quot;&gt;Policy Gradient Theorem&lt;/span&gt;이라는 이론적 해결 방법이 있습니다. 이로 인해 식 (13.1)의 Gradient Ascent를 추정할 수 있으며, State Distribution에 대한 미분을 포함하지 않습니다. Episodic Task의 경우 Policy Gradient Theorem은 다음과 같이 적용할 수 있습니다.&lt;/p&gt;

\[\nabla J(\boldsymbol{\theta}) \propto \sum_s \mu (s) \sum_a q_{\pi} (s, a) \nabla \pi (a | s, \boldsymbol{\theta}) \tag{13.5}\]

&lt;p&gt;이 식에서의 Gradient는 $\boldsymbol{\theta}$의 각 원소에 대한 편미분의 열 벡터이고, $\pi$는 매개변수 벡터 $\boldsymbol{\theta}$에 해당하는 Policy를 의미합니다. 기호 $\propto$는 비례를 의미합니다. Episodic Task의 경우 비례 상수는 Episode의 평균 길이이고, Continuing Task의 경우 1이 됩니다. (즉, 이 때는 등식이 됩니다) Distribution $\mu$는 9장과 10장에서 다루었던 $\pi$에 대한 On-policy Distribution 입니다. Episodic Task에서 Policy Gradient Theorem은 다음과 같이 증명할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of the Policy Gradient Theorem for Episodic Case&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;간단한 미적분학과 식의 변형으로 Policy Gradient Theorem을 증명할 수 있습니다. 표기법을 단순하게 유지하기 위해, 모든 경우에 $\pi$가 $\boldsymbol{\theta}$에 대한 함수이고, 모든 Gradient도 $\boldsymbol{\theta}$에 대해 표현할 수 있다는 것을 가정하겠습니다. 먼저 State-Value Function의 Gradient는 다음과 같이 Action-Value Function의 관점에서 나타낼 수 있습니다.&lt;/p&gt;

\[\begin{align}
\nabla v_{\pi} (s) &amp;amp;= \nabla \left[ \sum_a \pi (a|s) q_{\pi} (s, a) \right], \quad \text{for all } s \in \mathcal{S} \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi}(s, a) + \pi (a|s) \nabla q_{\pi} (s, a) \right] \tag{product rule of calculus} \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi} (s,a) + \pi(a|s) \nabla \sum_{s&apos;, r} p (s&apos;, r|s, a) (r + v_{\pi} (s&apos;)) \right] \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi} (s,a) + \pi (a|s) \sum_{s&apos;} p(s&apos; |s, a) \nabla v_{\pi} (s&apos;) \right] \tag{Equation 3.4} \\ \\
&amp;amp;= \sum_a \Bigg[ \nabla \pi (a|s) q_{\pi}(s,a) + \pi(a|s) \sum_{s&apos;} p(s&apos; |s, a) \\ \\
&amp;amp; \qquad \sum_{a&apos;} [\nabla \pi (a&apos;|s&apos;) q_{\pi} (s&apos;, a&apos;) + \pi (a&apos;|s&apos;) \sum_{s&apos;&apos;} p (s&apos;&apos; | s&apos;, a&apos;) \nabla v_{\pi} (s&apos;&apos;)] \Bigg] \tag{unrolling} \\ \\
&amp;amp;= \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \text{Pr} (s \to x, k, \pi) \sum_{a} \nabla \pi (a|s) q_{\pi} (x, a)
\end{align}\]

&lt;p&gt;Unrolling 부분은 식이 너무 길어서 2줄로 나누었습니다. 위 식에서 $\text{Pr} \left( s \to x, k, \pi \right)$는 Policy $\pi$에 따라 $k$단계 후에 State $s$에서 State $x$로 전환될 확률입니다. 이것을 이용하여 $\nabla J(\boldsymbol{\theta})$의 식을 변형하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp;= \nabla v_{\pi} (s_0) \\ \\
&amp;amp;= \sum_s \left( \sum_{k=0}^{\infty} \text{Pr} (s_0 \to s, k, \pi) \right) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \\ \\
&amp;amp;= \sum_a \eta (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Equation 9.2} \\ \\
&amp;amp;= \sum_{s&apos;} \eta (s&apos;) \sum_s \frac{\eta (s)}{\sum_{s&apos;} \eta(s&apos;)} \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \\ \\
&amp;amp;= \sum_{s&apos;} \eta (s&apos;) \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Equation 9.3} \\ \\
&amp;amp;\propto \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Q.E.D.}
\end{align}\]

&lt;h2 id=&quot;reinforce-monte-carlo-policy-gradient&quot;&gt;REINFORCE: Monte Carlo Policy Gradient&lt;/h2&gt;

&lt;p&gt;이제 Policy Gradient를 사용한 첫 번째 학습 알고리즘을 만들 준비가 되었습니다. 학습의 기본 전략은 이전에 다룬 식 (13.1)의 Stochastic Gradient Ascent를 기반으로 합니다. 이것을 수행하기 위해서는 Sample Gradient의 기대값이 매개변수의 함수로써 성능 측정의 실제 Gradient에 비례하도록 Sample을 얻는 방법이 필요합니다. Sample의 Gradient는 원래의 Gradient에 비례하기만 하면 되는데, 왜냐하면 비례 상수는 Step-size Parameter인 $\alpha$와 통합하여 취급할 수도 있고, 임의로 설정할 수도 있기 때문입니다. Policy Gradient Theorem은 Gradient에 비례하는 정확한 표현을 제공하므로, 필요한 것은 기대값이 이 표현식과 같거나 가까운 Sampling 방법입니다. Policy Gradient Theorem의 오른쪽 항은 Target Policy $\pi$ 하에서 State가 얼마나 자주 발생하는지에 따라 Weight가 부여된 State의 합입니다. 즉, Policy $\pi$를 따를 경우, 이러한 비율로 State가 발생합니다. 이것을 식으로 표현하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp; \propto \sum_s \mu (s) \sum_a q_{\pi} (s, a) \nabla \pi (a | s, \boldsymbol{\theta}) \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ \sum_a q_{\pi} (S_t, a) \nabla \pi (a|S_t, \boldsymbol{\theta}) \right] \tag{13.6}
\end{align}\]

&lt;p&gt;또한 식 (13.1)의 Stochastic Gradient Ascent 알고리즘을 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha \sum_a \hat{q} (S_t, a, \mathbf{w}) \nabla \pi (a | S_t, \boldsymbol{\theta}) \tag{13.7}\]

&lt;p&gt;식 (13.7)에서 $\hat{q}$는 $q_{\pi}$에 대한 학습된 근사치입니다. 업데이트가 모든 Action을 포함하기 때문에 이 알고리즘은 &lt;span style=&quot;color:red&quot;&gt;All-actions&lt;/span&gt; 방법이라고 불리며, 이 방법 중 대표적인 것으로 시간 $t$에서 실제로 취한 Action인 $A_t$에 대한 업데이트가 포함된 고전적인 &lt;span style=&quot;color:red&quot;&gt;REINFORCE&lt;/span&gt; 알고리즘입니다. (Willams, 1992 참고)&lt;/p&gt;

&lt;p&gt;REINFORCE의 완전한 알고리즘을 유도하기 위해서는 식 (13.6)가 $S_t$를 포함한 것과 마찬가지로 $A_t$를 포함시켜야 합니다. 즉, 확률 변수의 가능한 값에 대한 합을 Policy $\pi$ 하의 기대값으로 대체하여 Sampling합니다. 식 (13.6)은 Action에 대한 적절한 합을 포함하지만, 각 항은 $\pi (a \mid S_t, \boldsymbol{\theta})$에 Weight가 부여되지 않습니다. 그래서 우리는 전체적인 식의 관계가 변경되지 않도록 $\pi (a \mid S_t, \boldsymbol{\theta})$으로 나누어 식을 수정합니다. 이 과정을 식 (13.6)에 이어서 작성하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp; \propto \mathbb{E}_{\pi} \left[ \sum_a \pi (a|S_t, \boldsymbol{\theta}) q_{\pi} (S_t, a) \frac{\nabla \pi (a | S_t, \boldsymbol{\theta})}{\pi (a | S_t, \boldsymbol{\theta})} \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ q_{\pi} (S_t, A_t) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \right] \qquad \text{(replacing } a \text{ by the sample } A_t \sim \pi \text{)} \\ \\
&amp;amp;=\mathbb{E}_{\pi} \left[ G_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \right] \qquad \text{(because } \mathbb{E}_{\pi} [G_t | S_t, A_t] = q_{\pi} (S_t, A_t) \text{)}
\end{align}\]

&lt;p&gt;위 식에서 $G_t$는 일반적인 Return입니다. 마지막 식의 결과는 기대값의 Gradient에 비례하는 각 시간 단계에서의 Sampling할 수 있는 양입니다. 이 Sample을 사용하여 식 (13.1)의 Stochastic Gradient Ascent 알고리즘을 인스턴스화하면 REINFORCE 업데이트 식이 유도됩니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha G_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \tag{13.8}\]

&lt;p&gt;식 (13.8)과 같은 업데이트 식은 직관적입니다. 각 시간별 증가분은 Return $G_t$와 Action을 취할 확률의 Gradient를 그 확률로 나눈 값에 비례합니다. 그 벡터의 방향은 미래에 State $S_t$를 방문할 때 Action $A_t$를 반복할 확률을 가장 많이 증가시키는 매개변수 공간의 방향입니다. 즉, 업데이트는 Return $G_t$에 비례하고, Action 확률에 반비례하는 방향으로 매개변수 벡터를 증가시킵니다. Return에 비례하는 것은 가장 높은 Reward를 얻을 수 있는 Action을 선호하는 방향으로 매개변수를 움직인다는 의미가 있습니다. Action 확률에 비례하는 것은 자주 선택되는 Action을 선호한다는 뜻입니다. 자주 선택되는 Action은 가장 높은 Reward를 내지 못하더라도 유리한 선택이 될 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;REINFORCE는 Episode가 끝날 때까지 미래의 모든 Reward를 포함하는 시간 $t$부터 완전한 Return을 사용합니다. 이런 의미에서 REINFORCE는 Monte Carlo 알고리즘이라고 볼 수 있으며, Episode가 끝난 후 모든 업데이트가 수행되는 Episode Task에만 잘 정의됩니다. REINFORCE 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Pseudocode에서 마지막 줄은 REINFORCE 업데이트인 식 (13.8)과 다른 점이 있습니다. 먼저 $\nabla \ln x = \frac{\nabla x}{x}$라는 것을 이용하여, $\frac{\nabla \pi (A_t \mid S_t, \boldsymbol{\theta}_t)}{\pi (A_t \mid S_t, \boldsymbol{\theta}_t)}$를  $\nabla \ln \pi (A_t \mid S_t, \boldsymbol{\theta}_t)$로 바꾸었습니다. 이 벡터의 이름은 문헌에 따라 다르지만, 여기서는 간단하게 &lt;span style=&quot;color:red&quot;&gt;Eligibility Vector&lt;/span&gt;라고 명칭하겠습니다. 이 부분이 알고리즘에서 Policy 매개변수화가 나타나는 유일한 부분입니다.&lt;/p&gt;

&lt;p&gt;마지막 줄에서의 또 다른 차이점으로는 $\gamma^t$의 유무입니다. 식 (13.8)을 논할 때는 Discounting을 가정하지 않았기 때문에(즉, $\gamma = 1$) 이것을 생략하였지만, 위의 Pseudocode는 일반적인 경우에 대한 알고리즘이기 때문에 포함되었습니다.&lt;/p&gt;

&lt;p&gt;다음의 그래프는 Example 13.1에서 REINFORCE의 성능을 나타내고 있습니다. $\alpha$에 값에 따라 성능의 차이가 크기 때문에, 좋은 $\alpha$를 정하는 것이 중요하다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;REINFORCE는 Stochastic Gradient Method로 인해 좋은 이론적 수렴 특성을 가지고 있습니다. Episode에 대한 예상 업데이트는 성능에 대한 Gradient와 같은 방향입니다. 이것은 충분히 작은 $\alpha$에 대해 예상되는 성능의 개선과, $\alpha$가 감소하는 일반적인 확률적 근사 조건 하에 Local Optimum에 수렴하는 것을 보장합니다. 그러나 Monte Carlo Method인 REINFORCE는 Variance가 커서 학습이 느려질 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;reinforce-with-baseline&quot;&gt;REINFORCE with Baseline&lt;/h2&gt;

&lt;p&gt;Policy Gradient Theorem을 나타내는 식 (13.5)는 상대적인 Action-Value를 비교하는데 사용되는 임의의 baseline $b(s)$를 포함하여 일반화할 수 있습니다.&lt;/p&gt;

\[\nabla J(\boldsymbol{\theta}) \propto \sum_s \mu (s) \sum_a \left( q_{\pi} (s, a) - b(s) \right) \nabla (a | s, \boldsymbol{\theta}) \tag{13.10}\]

&lt;p&gt;Baseline은 $a$에 의해 변하지 않는 한, 확률 변수를 포함한 어떤 함수든 될 수 있습니다. 다음과 같이 Baseline은 Gradient를 취했을 때 0이 되기 때문입니다.&lt;/p&gt;

\[\sum_a b(s) \nabla \pi (a|s, \boldsymbol{\theta}) = b(s) \nabla \sum_a \pi (a|s, \boldsymbol{\theta}) = b(s) \nabla 1 = 0\]

&lt;p&gt;식 (13.10)과 같이 Baseline이 있는 Policy Gradient Theorem을 사용하여 이전 Section에서와 유사하게 업데이트 규칙을 유도할 수 있습니다. 다음은 Baseline을 포함한 새로운 버전의 REINFORCE 업데이트 식입니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha \left( G_t - b(S_t) \right) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \tag{13.11}\]

&lt;p&gt;Baseline은 0으로 균일할 수 있으므로 위의 식 (13.11)은 REINFORCE의 엄격한 일반화입니다. 일반적으로 Baseline은 업데이트의 Expected Value를 변경하지 않지만, Variance에 큰 영향을 줄 수 있습니다. 예를 들어, Section 2.8에서와 유사한 Baseline은 Gradient Bandit Algorithm의 Variance를 크게 줄일 수 있습니다. Variance가 줄어든다는 것은 그만큼 학습 속도가 빨라진다는 의미입니다.&lt;/p&gt;

&lt;p&gt;Bandit Algorithm에서 Baseline은 숫자(=평균 Reward)에 불과했지만, MDP의 경우 Baseline은 State에 따라 달라져야 합니다. 어떤 State에서는 모든 Action이 높은 Value를 가질 수 있기 때문에, 더 높은 Value와 덜 높은 Value의 Action을 구별하기 위해 높은 Baseline이 필요합니다. 물론 반대로 모든 Action이 낮은 Action에서는 낮은 Baseline을 통해 Action의 상대적 Value를 구별해야 합니다.&lt;/p&gt;

&lt;p&gt;Baseline에 대한 자연스러운 선택 중 하나는 State-Value의 추정값인 $\hat{v} (S_t, \mathbf{w})$ 입니다. 여기서 $\mathbf{w} \in \mathbb{R}^d$는 이전 장에서 제시된 방법 중 하나로 학습된 Weight Vector입니다. REINFORCE는 Policy 매개변수 $\boldsymbol{\theta}$를 학습하기 위한 Monte Carlo Method이기 때문에 Monte Carlo Method를 사용하여 State-Value의 Weight인 $\mathbf{w}$를 학습할 수도 있습니다. 학습된 State-Value Function을 Baseline으로 사용하는 REINFORCE의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 알고리즘에는 두 개의 Step-size Parameter인 $\alpha^{\boldsymbol{\theta}}$와 $\alpha^{\mathbf{w}}$가 있습니다. 이 중 $\alpha^{\mathbf{w}}$를 선택하는 것은 비교적 쉽습니다. 예를 들어, Section 9.6에서와 같이 Linear의 경우 $\alpha^{\mathbf{w}} = 0.1 / \mathbb{E} [ \lVert \nabla \hat{v} (S_t, \mathbf{w}) \rVert^2_{\mu}]$라는 경험적인 법칙이 있었습니다. 하지만 $\alpha^{\boldsymbol{\theta}}$는 Reward의 범위와 Policy 매개변수화에 따라 결정해야하기 때문에 명확한 방법이 존재하지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 예제 13.1에서 Baseline이 있는 경우와 없는 경우 REINFORCE의 성능을 비교합니다. 이 비교에 사용된 State-Value Function의 추정값은 $\hat{v} (s, \mathbf{w}) = w$입니다. 즉, $\mathbf{w}$는 단일 요소 $w$로 구성되어 있습니다. 그래프를 보시면, 수렴되는 값은 두 방법이 차이가 없지만, Baseline을 사용하는 방법이 더 빠르게 수렴함을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;actorcritic-methods&quot;&gt;Actor–Critic Methods&lt;/h2&gt;

&lt;p&gt;Baseline이 있는 REINFORCE에서 학습된 State-Value Function은 각 State Transition에서 첫 번째 State의 Value를 추정합니다. 이 추정값은 후속 Return에 대한 Baseline을 설정하지만, Action이 Tranistion되기 전에 이루어지므로 해당 Action을 평가하는데 사용할 수 없습니다.&lt;/p&gt;

&lt;p&gt;반면, Actor-Critic Method에서는 State-Value Function이 두 번째 State에도 적용됩니다. 두 번째 State의 Estimated Value는 Discount되어 Reward에 추가될 때, 1-step Return $G_{t:t+1}$을 포함하며, 이것은 실제 Return에 대한 올바른 추정이므로 Action을 평가할 수 있습니다. 이전에 배운 TD Learning에서도 보았듯이, 1-step Return은 Bias를 감안하더라도 Variance 및 계산 적합성 측면에서 실제 Return보다 우수합니다. 또한 7장 및 12장에서와 같이 $n$-step Return 및 Eligibility Trace를 사용하여 Bias의 범위를 유연하게 조정할 수도 있습니다. 이와 같이 State-Value Function을 사용하여 Action을 평가할 때, 이를 &lt;span style=&quot;color:red&quot;&gt;Critic&lt;/span&gt;이라고 하며, 전체 Policy Gradient Method를 &lt;span style=&quot;color:red&quot;&gt;Actor-Critic Method&lt;/span&gt;라고 합니다. Gradient 추정값의 Bias는 Bootstrapping으로 인한 것이 아니기 때문에 Critic이 Monte Carlo Method로 학습하더라도 Actor는 Bias될 것입니다.&lt;/p&gt;

&lt;p&gt;먼저 1-step Actor-Critic Method를 소개하겠습니다. 1-step 방법의 장점은 완전한 On-line 및 Incremental 방식임에도 Eligibility Trace의 복잡함을 피할 수 있다는 것입니다. 1-step Actor-Critic Method는 REINFORCE 방법인 식 (13.11)의 전체 Return을 다음과 같이 1-step Return으로 대체합니다. (단, 이때 Baseline은 학습된 State-Value Function을 사용합니다)&lt;/p&gt;

\[\begin{align}
\boldsymbol{\theta}_{t+1} &amp;amp; \doteq \boldsymbol{\theta}_t + \alpha \Big( G_{t:t+1} - \hat{v} (S_t, \mathbf{w}) \Big) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.12} \\ \\
&amp;amp;= \boldsymbol{\theta}_t + \alpha \Big( R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w}) \Big) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.13} \\ \\
&amp;amp;= \boldsymbol{\theta}_t + \alpha \delta_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.14}
\end{align}\]

&lt;p&gt;이 업데이트와 짝을 이루는 State-Value Function의 학습 방법은 Semi-gradient TD(0)입니다. 이것을 반영한 전체 알고리즘의 Pseudocode는 다음과 같습니다. 이 알고리즘은 State, Action, 그리고 Reward가 발생하는 즉시 처리되는 완전한 On-line Incremental 알고리즘이라는 점에 유의하시기 바랍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 1-step 방법의 Forward View에 대한 일반화 및 $n$-step으로의 일반화는 간단합니다. 이 때, 식 (13.12)의 1-step Return은 각각 $G_{t:t+n}$ 또는 $G^{\lambda}_t$로 대체됩니다. $\lambda$-return 알고리즘의 Backward View도 12장의 패턴을 따라 Actor와 Critic에 대해 별도의 Eligibility Trace를 사용하여 처리합니다. 이것을 반영한 전체 알고리즘의 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;policy-gradient-for-continuing-problems&quot;&gt;Policy Gradient for Continuing Problems&lt;/h2&gt;

&lt;p&gt;Section 10.3에서와 같이, Continuing Task의 성능은 다음과 같이 시간 단계에서 Average Reward로 정의해야 합니다.&lt;/p&gt;

\[\begin{align}
J(\boldsymbol{\theta}) &amp;amp; \doteq r(\pi) \doteq \lim_{h \to \infty} \frac{1}{h} \sum_{t=1}^h \mathbb{E} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \tag{13.15} \\ \\
&amp;amp;= \lim_{t \to \infty} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \\ \\
&amp;amp;\sum_s \mu (s) \sum_a \pi (a | s) \sum_{s&apos;, r} p (s&apos;, r | s, a) r
\end{align}\]

&lt;p&gt;위 식에서 $\mu$는 Policy $\pi$ 하에서의 Steady-state Distribution, $\mu (s) \doteq \underset{t \to \infty}{\operatorname{lim}} \text{Pr} \{ S_t = s \mid A_{0:t} \sim \pi \}$는 State $S_0$와 독립적으로 존재한다고 가정합니다. 이것은 $\pi$에 따라 Action을 선택하는 경우, 동일한 Distribution를 유지하는 특별한 Distribution임을 유의하시기 바랍니다.&lt;/p&gt;

\[\sum_s \mu (s) \sum_a \pi (a|s, \boldsymbol{\theta}) p(s&apos;|s,a) = \mu (s&apos;), \quad \text{for all } s&apos; \in \mathcal{S} \tag{13.16}\]

&lt;p&gt;(Backward View) Continuing Task에서 Actor-Critic 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 Continuing Task에서 Value Function은 각각 $v_{\pi} (s) \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right]$ (State-Value)와 $q_{\pi} (s, a) \doteq \mathbb{E} \left[ G_t \mid S_t = s, A_t = a \right]$ (Action-Value)로 정의됩니다. 이 때, Return $G_t$는 다음과 같습니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \cdots \tag{13.17}\]

&lt;p&gt;이렇게 정의가 바꾸더라도 식 (13.5) Policy Gradient Theorem은 유효합니다. Continuing Task에서 Policy Gradient Theorem에 대한 증명은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of the Policy Gradient Theorem for Continuing Case&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Episodic Task에서와 마찬가지로 먼저 모든 경우에 $\pi$가 $\boldsymbol{\theta}$에 대한 함수이고, Gradient가 $\boldsymbol{\theta}$로 표현될 수 있다는 가정이 필요합니다. 식 (13.15)에 의하여 Continuing Task에서 $J(\boldsymbol{\theta}) = r(\pi)$이고, $v_{\pi}$와 $q_{\pi}$는 식 (13.17)의 Return으로 구성되어 있습니다. 따라서 State-Value Function의 Gradient는 임의의 State $s \in \mathcal{S}$에 대해 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\nabla v_{\pi} (s) &amp;amp;= \nabla \left[ \sum_a \pi (a | s) q_{\pi} (s, a) \right], \quad \text{for all } s \in \mathcal{S} \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \nabla q_{\pi} (s, a) \Big] \tag{product rule of calculus} \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \nabla \sum_{s&apos; ,r} p(s&apos;, r|s, a) \big(r - r(\boldsymbol{\theta}) + v_{\pi} (s&apos;) \big) \Big] \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \big[ - \nabla r(\boldsymbol{\theta}) + \sum_{s&apos;} p (s&apos; |s, a) \nabla v_{\pi} (s&apos;) \big] \Big] \end{align}\]

&lt;p&gt;이 식을 정리하면 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\nabla r(\boldsymbol{\theta}) = \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \sum_{s&apos;} p(s&apos; |s, a) \nabla v_{\pi} (s&apos;) \Big] - \nabla v_{\pi} (s)\]

&lt;p&gt;위 식의 좌변은 $J(\boldsymbol{\theta})$로 표기할 수 있으며, State $s$에 의존하지 않습니다. 따라서 우변 역시 State $s$에 의존하지 않으므로 식의 변경 없이 Weight $\mu (s)$를 붙여 합산할 수 있습니다. ($\because \sum_s \mu (s) = 1$)&lt;/p&gt;

\[\begin{align}
J(\boldsymbol{\theta}) &amp;amp;= \sum_s \mu (s) \Bigg( \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \sum_{s&apos;} p(s&apos; | s, a) \nabla v_{\pi} (s&apos;) \Big] - \nabla v_{\pi} (s) \Bigg) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_s \mu (s) \sum_a \pi (a|s) \sum_{s&apos;} p(s&apos; | s, a) \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_{s&apos;} \underbrace{\sum_s \mu (s) \sum_a \pi (a|s) p (s&apos; | s, a)}_{\mu (s&apos;) \text{( 13.16)}} \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_{s&apos;} \mu (s&apos;) \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \tag{Q.E.D.}
\end{align}\]

&lt;h2 id=&quot;policy-parameterization-for-continuous-actions&quot;&gt;Policy Parameterization for Continuous Actions&lt;/h2&gt;

&lt;p&gt;Policy에 기반한 방법은 Action Space가 매우 큰 경우(=Action의 수가 매우 많은 경우)나 Action의 수가 무한한 연속적인 Space에서도 효과적인 해법을 제공합니다. 이런 경우, Action 각각에 대해 학습된 확률을 계산하는 대신 확률 분포의 통계를 학습합니다. 예를 들어, Action 집합은 Normal Distribution(=Gaussian Distribution)에서 선택된 Action을 포함하는 실수 집합일 수 있습니다.&lt;/p&gt;

&lt;p&gt;아시다시피, Normal Distribution에 대한 Probability Density Function은 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[p(x) \doteq \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(x - \mu)^2}{2 \sigma^2} \right) \tag{13.18}\]

&lt;p&gt;Probability Density Function에서 $\mu$는 Normal Distribution의 평균이고, $\sigma$는 표준편차입니다. 이 때 $p(x)$는 $x$가 일어날 확률이 아니라 $x$에서의 Probability Density입니다. 즉, 1보다 클 수도 있습니다. 합이 1이 되어야 하는 부분은 $p(x)$와 $x$축 사이의 총 넓이입니다. 일반적으로 $x$의 값을 범위로 정하여 적분을 취하면 해당 범위 내에 $x$가 존재할 확률을 구할 수 있습니다. Probability Density Function에서 평균과 표준편차의 값에 따라 나타낸 그래프는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Probability Density Function을 기반으로 Policy 매개변수화를 생성하기 위해서는, Policy를 State에 따라 달라지는 매개변수 함수 근사가 제공하는 평균 및 표준편차와 함께 실수 값 Scalar Action을 Normal Probability Density로 정의하면 됩니다. 즉,&lt;/p&gt;

\[\pi (a|s, \boldsymbol{\theta}) \doteq \frac{1}{\sigma(s, \boldsymbol{\theta}) \sqrt{2 \pi}} \exp \left( - \frac{(a - \mu (s, \boldsymbol{\theta}))^2}{2 \sigma (s, \boldsymbol{\theta})^2} \right) \tag{13.19}\]

&lt;p&gt;식 (13.19)에서 $\mu : \mathcal{S} \times \mathbb{R}^{d’} \to \mathbb{R}$과 $\sigma : \mathcal{S} \times \mathbb{R}^{d’} \to \mathbb{R}^+$는 매개변수화된 함수 근사입니다.&lt;/p&gt;

&lt;p&gt;위 식을 완성하기 위해서는 두 개의 함수 근사에 대한 형태를 정의해야 합니다. 이를 위해 Policy의 매개변수 벡터를 $\boldsymbol{\theta} = [\boldsymbol{\theta}_{\mu}, \boldsymbol{\theta}_{\sigma}]^{\sf T}$와 같이 두 부분으로 나눕니다. 이 두 부분은 각각 평균과 표준편차에 대한 근사를 의미합니다. 이 중 평균은 Linear Function으로 근사할 수 있습니다. 표준편차의 경우에는 항상 양수여야하는 조건이 있기 때문에 Linear Function의 지수 형태로 근사할 수 있습니다. 이를 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\mu (s, \boldsymbol{\theta}) \doteq \boldsymbol{\theta}_{\mu}^{\sf T} \mathbf{x}_{\mu} (s) \quad \text{and} \quad \sigma (s, \boldsymbol{\theta}) \doteq \exp \left( \boldsymbol{\theta}_{\sigma}^{\sf T} \mathbf{x}_{\sigma} (s) \right) \tag{13.20}\]

&lt;p&gt;식 (13.20)에서 $\mathbf{x}_{\mu} (s)$와 $\mathbf{x}_{\sigma} (s)$는 Section 9.5에서 설명된 방법 중 하나로 구성되는 Feature Vector입니다. 이러한 정의를 사용하면 이번 장에서 배웠던 알고리즘 중 하나를 적용하여 실수값으로 정의된 Action을 선택하는 것을 학습할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장을 배우기 전까지는 Action-Value를 학습한 다음, Action을 선택하는데 사용하는 방법(Action-Value Method)에 초점을 맞췄습니다. 반면에 이번 장에서는 Action-Value의 추정값을 참조하지 않고 Action을 선택할 수 있도록 매개변수화된 Policy를 학습하는 방법을 고려하였습니다. 특히, Policy 매개변수에 대한 성능의 Gradient를 추정하는 방법인 &lt;strong&gt;Policy Gradient Method&lt;/strong&gt;를 고려하였습니다.&lt;/p&gt;

&lt;p&gt;Policy 매개변수를 학습하는 방법에는 많은 이점이 있습니다. 먼저, Action을 취할 특정한 확률을 학습할 수 있습니다. 이 방법들은 적절한 수준의 Exploration을 수행하고 Deterministic Policy에 점근적으로 접근할 수 있습니다. 또한 연속적인 Action Space도 다룰 수 있습니다. 이것은 Policy 기반 방법에서는 쉽지만 $\epsilon$-greedy나 일반적인 Action-Value 방법으로는 어렵거나, 불가능합니다. 또한 일부 문제에서는 Value Function을 매개변수로 표현하는 것보다, Policy를 매개변수로 표현하는 것이 더 간단합니다. 이러한 문제들은 매개변수화된 Policy 방법에 더 적합합니다.&lt;/p&gt;

&lt;p&gt;매개변수화된 Policy 방법은 &lt;strong&gt;Policy Gradient Theorem&lt;/strong&gt;을 기반으로 한 중요한 이론적 장점이 있습니다. 이 정리는 State Distribution에 대한 정보를 포함하지 않는 Policy 매개변수에 의해 성능이 어떻게 영향을 받는지에 대한 정확한 공식을 제공합니다. 이 정리는 모든 Policy Gradient Method에 대한 이론적인 토대를 마련합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;REINFORCE&lt;/strong&gt;는 Policy Gradient Theorem을 직접적으로 따르는 방법입니다. 또한 State-Value Function을 Baseline으로 추가하면 Bias를 피하면서 REINFORCE의 Variance를 줄일 수 있습니다. State-Value Function이 Policy의 Action 선택을 평가하거나 비판하는데 사용되는 경우, Value Function을 &lt;strong&gt;Critic&lt;/strong&gt;이라고 하고 Policy를 &lt;strong&gt;Actor&lt;/strong&gt;라고 합니다. 이 방법은 &lt;strong&gt;Actor-Critic Method&lt;/strong&gt;이라고 합니다. Critic은 Actor의 Gradient 추정에 Bias를 추가하지만, Bootstrapping TD 방법이 종종 Monte Carlo Method보다 Variance가 낮다는 측면에서 우수하기 때문에 때때로 더 우수합니다.&lt;/p&gt;

&lt;p&gt;정리하자면, Policy Gradient Method는 Action-Value 방법과는 상당히 다른 장점과 단점을 지닙니다. 현재 이 분야는 아직 활발하게 연구되는 주제이기 때문에, 앞으로 더 흥미로운 결과가 나오는 것을 기대하고 있습니다.&lt;/p&gt;

&lt;p&gt;이로써 길고 길었던 강화학습 포스팅이 얼추 마무리가 되었습니다. 당분간 새로운 이론적인 포스트를 작성하기 보다는, 쉬면서 작성했던 포스트를 검토하여 부족했던 부분을 수정하거나 추가하도록 하겠습니다. 지금까지 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장은 드디어 마지막 장인 Policy Gradient입니다. 이번 장에서는 지금까지 이 교재에서 다룬 방법들과는 다르게, Policy 자체를 매개변수화하는 방법을 알아보겠습니다. 지금까지의 방법들은 Estimated Action-Value를 기반으로 Action을 선택했기 때문에 Action-Value를 추정하는 것이 중요했습니다. 하지만 이번 장에서 배울 새로운 방법인 Policy Gradient는 Action을 선택하는 데 Value Function을 사용하지 않습니다. 이번 장에서 사용할 새로운 표기는 Policy에 대한 매개변수 벡터인 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$입니다. 따라서 Policy는 이제 매개변수 $\boldsymbol{\theta}$를 포함하여 $\pi (a \mid s, \boldsymbol{\theta}) = Pr \{ A_t = a \mid S_t = s, \boldsymbol{\theta}_t = \boldsymbol{\theta} \}$로 표현합니다. 이것은 시간 $t$에서 State가 $s$이고 매개변수가 $\boldsymbol{\theta}$일 때 Action $a$를 선택할 확률로 정의됩니다. 만약 학습 알고리즘 안에서 Value Function에 대한 추정을 포함하는 경우, 이전과 마찬가지로 여전히 Weight Vector $\mathbf{w} \in \mathbb{R}^d$를 포함하여 $\hat{v}(s, \mathbf{w})$로 표현합니다.</summary></entry><entry><title type="html">Eligibility Trace</title><link href="http://localhost:4000/rl/eligibility-traces/" rel="alternate" type="text/html" title="Eligibility Trace" /><published>2022-09-22T00:00:00+09:00</published><updated>2022-09-22T00:00:00+09:00</updated><id>http://localhost:4000/rl/eligibility-traces</id><content type="html" xml:base="http://localhost:4000/rl/eligibility-traces/">&lt;p&gt;이번 장에서 새로 배우는 &lt;span style=&quot;color:red&quot;&gt;Eligibility Trace&lt;/span&gt;는 강화학습의 기본 메커니즘 중 하나입니다. 예를 들어, TD($\lambda$)에서 $\lambda$는 Eligibility Trace를 사용한다는 것을 의미합니다. Q-learning과 Sarsa를 포함한 대부분의 TD 방법은 Eligibility Trace와 결합하여 보다 효율적으로 학습할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace는 TD와 Monte Carlo Method를 통합하여 일반화하는 방법입니다. TD 방법을 Eligibility Trace를 사용하여 일반화하면 $\lambda = 1$일 때 Monte Carlo Method처럼 동작하며, $\lambda = 0$일 때 1-step TD로 동작합니다. 이로 인해 Eligibility Trace는 온라인으로 Monte Carlo Method을 구현할 수 있고, Episode가 없는 Continuing Problem에 대한 학습 방법을 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace의 메커니즘을 간단히 설명하자면, Eligibility Trace의 Short-term Memory Vector $\mathbf{z}_t \in \mathbb{R}^d$는 Long-term Weight Vector $\mathbf{w}_t \in \mathbb{R}^d$와 평행합니다. $\mathbf{w}_t$를 통해 함수를 추정할 때, $\mathbf{z}_t$의 구성 요소와 충돌한 후, $\mathbf{z}_t$는 사라지기 시작합니다. 이 &lt;strong&gt;Trace&lt;/strong&gt;가 0으로 감소하기 전에 0이 아닌 TD Error가 발생하면, $\mathbf{w}_t$의 해당 구성 요소에서 학습이 일어납니다. 이 때 $\lambda \in \left[ 0, 1 \right]$는 Trace가 얼마나 빨리 0으로 감소하는 지 나타내는 Trace-decay Parameter입니다.&lt;/p&gt;

&lt;p&gt;그런데 우리는 이미 7장에서 Monte Carlo와 1-step TD를 조율한 $n$-step TD를 배웠습니다. 하지만 Eligibility Trace는 $n$-step TD에 비해 계산적으로 이점이 있습니다. $n$-step TD는 마지막 $n$개의 Feature Vector를 저장했지만, Eligibility Trace는 1개의 Trace Vector만 필요합니다. 또한 $n$-step TD에서의 학습은 Episode가 끝나기 전까지 지연되는 방식이지만, Eligibility Trace는 지속적이고 균일하게 학습이 일어납니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 통해 학습 알고리즘은 계산상의 이점을 위해 때때로 다른 방법으로 구현될 수도 있다는 것을 보여줍니다. 기존 방법을 예로 들자면, Monte Carlo Method와 $n$-step TD는 Episode의 마지막부터 Episode의 처음까지 학습하거나, $n$-step만큼 학습하였습니다. 이것을 &lt;strong&gt;Forward View&lt;/strong&gt;라고 하는데, Forward View는 막상 알고리즘을 수행할 때 바로 사용할 수 없는 미래의 요소에 의존하기 때문에 구현하는 것이 상당히 복잡합니다. 그러나 Eligibility Trace는 알고리즘이 수행하는 순서와 거의 동일한 Update를 구현할 수 있습니다. 이것을 Backward View라고 합니다. 이번 장에서 이것에 대해 조금 더 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;이번 장 역시 이전 장들과 마찬가지로, State-Value의 Prediction에 대한 개념을 먼저 다룬 다음에, Action-Value 및 Control 문제로 확장합니다. 또한 마찬가지로 On-policy 학습을 먼저, Off-policy 학습을 나중에 다룰 예정입니다. Function Approximation은 Linear Function Approximation에 중점을 둘 것이며, Tabular 방법과 State Aggregation 경우에도 적용할 수 있다는 것을 보일 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-lambda-return&quot;&gt;The $\lambda$-Return&lt;/h2&gt;

&lt;p&gt;먼저 7장에서 배운 $n$-step Return을 복습해봅시다. 식 (7.1)에서 $n$-step Return은 처음 $n$개의 Discounted Reward와 방문한 State의 Estimated Value의 합으로 정의했습니다. 이 식을 매개변수를 사용한 Function Approximation 식으로 수정하면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{v} \left( S_{t+n}, \mathbf{w}_{t+n-1} \right), \quad 0 \le t \le T-n \tag{12.1}\]

&lt;p&gt;식 (12.1)에서 $\hat{v} \left( s, \mathbf{w} \right)$는 Weight Vector $\mathbf{w}$가 주어졌을 때 State $s$의 근사값이고, $T$는 Episode가 종료되는 시간입니다.&lt;/p&gt;

&lt;p&gt;또한 이러한 Update는 $n$-step Return 뿐만 아니라 다른 모든 $n$에 대한 $n$-step Return의 평균에 대해서도 유효합니다. 예를 들자면, 2-step Return의 절반과 4-step Return의 절반의 합으로 구성된 $\frac{1}{2} G_{t:t+2} + \frac{1}{2} G_{t:t+4}$와 같은 식에 대해서도 Update를 수행할 수 있다는 것입니다. 이렇게 각 Return의 Weight가 양수이면서 합이 1인 조건 하에서는 모든 $n$-step Return이 이런 방식으로 평균을 낼 수 있습니다. 심지어 항의 개수가 무한해도 말입니다. 이러한 평균화 기법을 이용하면 새로운 알고리즘을 개발할 수 있습니다. 예를 들어, TD나 Monte Carlo Method를 연결하기 위해 1-step 및 무한 단계 Return을 평균화하는 식으로 말입니다. 이와 같이 간단하게 구성 요소를 Update 하는 평균화 기법을 &lt;span style=&quot;color:red&quot;&gt;Compound Update&lt;/span&gt; 라고 합니다. 이에 대한 Backup Diagram은 Update 식에 따라 달라지는데, 방금 다룬 2-step Return의 절반과 4-step Return의 절반을 합친 식에 대한 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Backup Diagram에서 볼 수 있듯이, Compound Update의 Update를 수행하기 위해서는 가장 긴 구성 요소의 Update가 완료되어야 수행할 수 있습니다. 예를 들어, 위의 Backup Diagram에서 가장 긴 구성 요소는 4-step Return이므로, 시간 $t$에서의 추정치는 시간 $t+4$에 도달해야만 추정이 가능합니다. 이러한 문제로 인해 Update에 지연이 발생할 수 있으므로, 일반적으로는 가장 긴 구성 요소의 길이를 제한하는 방식으로 해결합니다.&lt;/p&gt;

&lt;p&gt;$n$-step update를 평균화하는 방법 중 대표적으로 &lt;span style=&quot;color:red&quot;&gt;TD($\lambda$)&lt;/span&gt;가 있습니다. 이 방법은 모든 $n$-step update에 대해 $\lambda^{n-1}$의 Weight를 부여한 평균화 방법입니다. 그리고 이 Weight의 합을 1로 만들기 위해 맨 앞에 $1 - \lambda$를 곱해줍니다. 이것을 식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+n} \tag{12.2}\]

&lt;p&gt;식 (12.2)의 Update 식을 &lt;span style=&quot;color:red&quot;&gt;$\lambda$-Return&lt;/span&gt; 이라고 합니다. $\lambda$-Return의 Backup Diagram은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\lambda$-Return에서는 각 항의 계수가 다르기 때문에 Weight 또한 항 마다 다릅니다. 예를 들어, 1-step Return의 계수는 $(1 - \lambda)$이지만, 2-step Return은 $(1 - \lambda) \lambda$의 계수를 갖습니다. 이렇게 시간 $t$를 기준으로 멀어질수록 $\lambda$를 곱하기 때문에 Weight가 낮아집니다. 이것을 그림으로 표현하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 보시다피시 마지막 항의 계수만 다른 항과 표현 방식이 다르기 때문에, 식 (12.2)를 다음과 같이 마지막 항만 분리하여 표현할 수도 있습니다.&lt;/p&gt;

\[G_t^{\lambda} = (1 - \lambda) \sum_{n=1}^{T-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{T-t-1}G_t \tag{12.3}\]

&lt;p&gt;식 (12.3)에서 $\lambda = 1$인 경우를 따져봅시다. $(1 - \lambda)$ 항이 0이 되므로 2번째 항만 살아남아 Monte Carlo Return이 됩니다. 반대로 $\lambda = 0$인 경우라면 2번째 항이 사라짐은 물론, 1번째 항의 첫 번째 Return을 제외하고 모두 0이 되기 때문에 1-step Return이 됩니다. 따라서 $\lambda = 0$인 경우라면 $\lambda$-Return은 1-step TD 방법이 된다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이제 $\lambda$-Return을 기반으로 만든 첫 번째 학습 알고리즘인 &lt;span style=&quot;color:red&quot;&gt;Off-line $\lambda$-Return Algorithm&lt;/span&gt;을 소개하겠습니다. &lt;strong&gt;Off-line&lt;/strong&gt; Algorithm이므로 Episode가 수행되는 동안에는 Weight Vector가 변경되지 않습니다. 그 후 Episode가 끝날 때 $\lambda$-Return을 Target으로 사용하여 일반적인 Semi-gradient Rule에 따라 Off-line Update의 전체 과정을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G_t^{\lambda} - \hat{v} (S_t, \mathbf{w}) \right] \nabla \hat{v} (S_t, \mathbf{w}), \quad t = 0, \ldots, T - 1 \tag{12.4}\]

&lt;p&gt;$\lambda$-Return은 7장에서 배웠던 $n$-step bootstrapping 방법과 다른 방법으로 Monte Carlo와 1-step TD 사이를 조절할 수 있는 대안을 제시합니다. 이에 대한 예시로 교재에서는 Random Walk Example을 제시하는데, 이 예제를 제가 7장에서 소개하지 않았습니다. 일단 여기에서 비교 내용만 설명하고, 추후 7장에 이 예제를 추가하겠습니다.&lt;/p&gt;

&lt;p&gt;아래 그림은 19개의 State를 가진 Random Walk Example에서의 $\lambda$-Return Algorithm과 $n$-step TD 방법 비교 그래프입니다. 두 방법 모두 처음 10개의 Episode에 대한 평균을 나타내며 그래프의 세로축은 Root Mean Square Error를 의미하기 때문에 낮을 수록 좋습니다. 그래프를 보면 두 방법 모두 성능이 비슷하며, 중간 정도의 $\lambda$와 $n$일 때 최적의 성능을 보임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 지금까지 취한 접근 방식은 학습 알고리즘에 대한 Theoretical View, 혹은 Forward View라고 부를 수 있습니다. 방문하는 각 State에 대해 향후 얻을 수 있는 모든 Reward에 대한 기대값과 이를 결합하는 최선의 방법을 결정하기 때문입니다. 아래 그림과 같이 Update를 결정하기 위해 각 State에서 기다리면서 State의 흐름을 타고 있다고 볼 수 있습니다. 한 State를 Update 한 후, 다음 State로 이동한 후에는 이 작업을 반복할 필요가 없습니다. 반면에 미래 State는 이전의 유리한 지점에서 한 번씩 반복적으로 처리됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tdlambda&quot;&gt;TD($\lambda$)&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;TD($\lambda$)&lt;/span&gt;는 강화학습에서 가장 오래되고 널리 사용되는 알고리즘 중 하나입니다. TD($\lambda$)는 Eligibility Trace를 사용하여 Forward View와 Backward View 사이의 형식적인 관계를 나타내는 최초의 알고리즘입니다. Forward View는 이론적인 면에서, Backward View는 계산적인 면에서 각각 이점이 있습니다. 이번 Section에서는 TD($\lambda$)가 이전 Section에서 배운 Off-line $\lambda$-Return Algorithm에 근접함을 경험적으로 보여줄 것입니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)는 Off-line Return Algorithm을 세 가지 방식으로 개선합니다. 첫째, Episode가 끝날 때 뿐만 아니라 Episode의 모든 단계에서 Weight Vector를 Update함으로써 추정치를 더 빠르게 계산합니다. 둘째, Episode가 끝낼 때 한번에 계산하지 않고, 시간에 따라 계산이 균등하게 분산됩니다. 셋째, Episodic Task 뿐만 아니라 Continuing Task에도 적용할 수 있습니다. 이번 Section에서는 Function Approximation을 사용하여 TD($\lambda$)의 Semi-gradient 버전을 제시합니다.&lt;/p&gt;

&lt;p&gt;Function Approximation에서 Eligibility Trace는 Weight Vector $\mathbf{w}_t$와 동일한 수의 구성 요소를 갖는 Vector $\mathbf{z}_t \in \mathbb{R}^d$입니다. Weight Vector는 시스템의 전체 수명 동안 누적되는 Long-term Memory이지만, Eligibility Trace는 일반적으로 Episode의 길이보다 짧은 시간 동안만 지속되는 Short-term Memory입니다. Eligibility Trace의 결과는 Weight Vector에 영향을 미치고 Weight Vector가 추정한 값을 결정함으로써 학습 과정에 도움이 됩니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)에서 Eligibility Trace의 Vector는 Episode 시작 시 Zero Vector로 초기화되고, 각 시간 단계에서 이전 Vector의 $\gamma \lambda$만큼 감소한 후, Value Function의 Gradient만큼 증가합니다. 이것을 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_{-1} &amp;amp; \doteq \mathbf{0} \\ \\
\mathbf{z}_t &amp;amp; \doteq  \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{v} (S_t, \mathbf{w}_t), \quad 0 \le t \le T \tag{12.5}
\end{align}\]

&lt;p&gt;식 (12.5)에서 $\gamma$는 Discount Factor이고, $\lambda$는 이전 Section에서 소개한 매개변수인데, 앞으로 이것을 Trace-decay 매개변수라고 부르겠습니다. Eligibility Trace는 Weight Vector의 어떤 구성 요소가 최근 State에 대한 평가에 긍정적으로/부정적으로 기여했는지 &lt;strong&gt;Trace&lt;/strong&gt;합니다. 여기서 &lt;strong&gt;최근&lt;/strong&gt;은 $\gamma \lambda$로 정의됩니다. Trace는 학습 이벤트가 발생할 경우 그것에 의해 변경이 일어날 수 있는 Weight Vector에서 각 구성 요소들의 &lt;strong&gt;Eligibility&lt;/strong&gt;를 나타냅니다. 여기서 우리가 우려할 수 있는 학습 이벤트는 매 순간의 1-step TD Error입니다. State-Value Prediction에 대한 TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t \doteq R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v} (S_t, \mathbf{w}_t) \tag{12.6}\]

&lt;p&gt;TD($\lambda$)에서 Weight Vector는 Scalar TD Error 및 Vector에 대한 Eligibility Trace에 비례하여 각 단계에서 다음과 같이 Update됩니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t \tag{12.7}\]

&lt;p&gt;Semi-gradient TD($\lambda$)의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TD($\lambda$)는 시간적으로 Backward View라고 볼 수 있습니다. 매 순간 현재의 TD Error를 확인하고, 그 State가 당시 Eligibility Trace에 얼마나 기여했는지에 따라 각각의 이전 State에 거꾸로 반영합니다. State가 미래에 다시 발생할 때를 대비하여 아래 그림과 같이 State의 흐름과 TD Error를 계산하고 식 (12.7)에 의해 얻은 Update를 이용하여 과거의 Value를 변경합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것을 조금 더 잘 이해하기 위해서는 $\lambda$에 값에 따라 어떻게 달라지는지 생각해보면 됩니다. 만약 $\lambda = 0$인 경우라면 식 (12.5)에 의해 시간 $t$ 에서의 Trace는 정확히 State $S_t$에서 Value의 Gradient와 같습니다. 따라서 이 때의 TD($\lambda$) Update인 식 (12.7)은 9장에서 배운 1-step TD Update와 동일합니다. 이것이 그 당시 1-step TD Update를 TD(0)로도 불렀던 이유입니다. 위의 그림을 토대로 설명하자면, TD(0)는 현재 State를 기준으로 한 단계 이전의 State에 대한 TD Error로만 Update하는 경우입니다. 하지만 만약 $\lambda &amp;lt; 1$ 조건 하에 $\lambda$의 값이 증가한다면 더 많은 이전 State들이 Update되는데, 그림에서 볼 수 있듯이 시간적으로 멀리 떨어진 State일수록 Eligibility Trace가 더 작기 때문에 덜 Update됩니다. 이것을 &lt;strong&gt;초기 State는 TD Error에 대해 더 적은 Credit을 받았다&lt;/strong&gt;라고 표현하기도 합니다.&lt;/p&gt;

&lt;p&gt;만약 $\lambda = 1$인 경우라면, 이전 State에 부여된 Credit은 단계당 $\gamma$만큼 떨어집니다. 예를 들어, TD Error $\delta_t$는 Discount 되지 않은 $R_{t+1}$를 포함합니다. 그리고 이전의 $k$ 단계에 대한 Return을 계산할 때는 Reward에 $\gamma^k$ 만큼의 Discount가 곱해지는데, 이것은 점점 감소하는 Eligibility Trace가 됩니다. 만약 $\lambda = 1$이고 $\gamma = 1$일 때는 시간적으로 아무리 떨어져 있더라도 Eligibility Trace가 소멸되지 않습니다. 이 경우에는 Discount가 없는 Episodic Task에 대한 Monte Carlo Method처럼 작동합니다. $\lambda = 1$인 경우 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;TD(1)&lt;/span&gt;으로도 부릅니다.&lt;/p&gt;

&lt;p&gt;TD(1)은 기존의 Monte Carlo Method를 더 일반적으로 구현한 방법입니다. 기존의 Monte Carlo Method는 Episodic Task에 한정되었지만, TD(1)은 Discounted Continuing Task에도 적용할 수 있습니다. 또한 TD(1)은 점진적으로, 온라인으로 수행할 수도 있습니다. Monte Carlo Method는 Episode가 끝날 때까지 아무것도 학습하지 못한다는 단점이 있지만, TD(1)는 Episode가 끝나지 않은 상황에서도 그 일부분을 $n$-step TD 방식으로 학습할 수 있다는 장점이 있습니다. 예를 들어, 만약 Episode 중 비정상적으로 좋거나 나쁜 일이 발생하면 TD(1)에 기반한 Control은 즉시 이전까지의 내용을 학습하고 Episode를 변경할 수 있습니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)가 Off-line $\lambda$-Return Algorithm을 근사하는데 얼마나 성능이 좋은지 알아보기 위해 또 다시 19개의 State를 가진 Random Walk Example을 놓고 비교해보겠습니다. 아래 그림을 보시면 그래프의 모양 자체는 차이가 있지만, $\lambda$의 값이 최적인 State에서는 거의 동일한 성능을 보임을 알 수 있습니다. 다만 $\lambda$가 최적보다 크게 선택되는 상황을 보면 TD($\lambda$)는 Off-line $\lambda$-Return Algorithm보다 성능이 더 나쁘다는 것을 알 수 있습니다. 일반적으로 최적의 State를 제외하고는 $\lambda$를 사용하지 않기 때문에 큰 문제는 아닙니다만, TD($\lambda$)가 &lt;strong&gt;더 불안정하다&lt;/strong&gt;라고는 말할 수 있을 정도의 단점은 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Linear TD($\lambda$)는 On-policy인 경우 조건 식 (2.7)에 따라 Step-size Parameter가 시간에 따라 감소한다면 수렴합니다. Section 9.4에서 다룬 바와 같이 수렴한다는 것은 Weight Vector의 최소 오차가 $\lambda$에 따른다는 것을 의미합니다. 식 (9.14)에서 배운 오차 한계식은 $\lambda$에 의해 일반화될 수 있습니다. 만약 Discounted Continuing Task라면, 다음과 같습니다.&lt;/p&gt;

\[\overline{\text{VE}}(\mathbf{w}_{\text{TD}}) \le \frac{1 - \gamma \lambda}{1 - \gamma} \min_{\mathbf{w}} \overline{\text{VE}}(\mathbf{w}) \tag{12.8}\]

&lt;p&gt;즉, 점근적인 오차는 가능한 최소 오차의 $\frac{1 - \gamma \lambda}{1 - \gamma}$배를 넘지 않는 다는 뜻입니다. $\lambda$가 1에 가까워질수록 최소 오차에 가까워집니다. 이렇게 보면 $\lambda$를 1에 가깝게 잡는 것이 좋아보이지만, 실제로는 가장 좋지 않은 선택이 될 가능성이 높은데, 그 이유는 나중에 밝혀집니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-truncated-lambda-return-methods&quot;&gt;$n$-step Truncated $\lambda$-Return Methods&lt;/h2&gt;

&lt;p&gt;Off-line $\lambda$-Return Algorithm은 중요하지만, Episode가 끝날 때까지 알 수 없는 $\lambda$-Return을 이용하기 때문에 효용이 제한적입니다. (식 12.2 참고) Continuing Task의 경우, $\lambda$-Return은 기술적으로 계산할 수 없기 때문에 임의적으로 큰 $n$에 대해 $n$-step Return에 의존합니다. 하지만 시간적으로 멀리 떨어진 Reward일수록 $\gamma \lambda$만큼의 비율로 계속 비중이 줄어들기 때문에, 이 경우 근사를 하기 위해서는 일정 Step 마다 구간을 나누는 것이 좋습니다. $n$-step Return은 누락된 Reward를 추정된 값으로 대체하는 개념을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;Episode를 일정 구간인 $h$만큼 자르는 경우, 시간 $t$에 대한 Truncated $\lambda$-Return 식은 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[G_{t:h}^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{h-t-1}G_{t:h}, \quad 0 \le t &amp;lt; h \le T \tag{12.9}\]

&lt;p&gt;이 식은 $h$의 역할이 식 (12.3)에서 $T$의 역할과 동일함을 알 수 있습니다. 또 다른 차이점을 굳이 찾자면 두 번째 항의 $G_t$ 대신 $G_{t:h}$로 변경된 것 정도가 있고, 그 외에는 식 (12.3)과 동일합니다.&lt;/p&gt;

&lt;p&gt;Truncated $\lambda$-Return은 7장에서의 $n$-step 방법과 유사한 $n$-step Return Algorithm을 즉시 생성하는 방식으로 구성됩니다. 이 때의 Update는 $n$-step 만큼 지연되고 처음 $n$-step만 고려되었지만, 이제는 모든 $k$-step ($1 \le k \le n$) Return이 포함됩니다. State-Value의 경우 이 알고리즘과 같은 종류를 &lt;span style=&quot;color:red&quot;&gt;Truncated TD($\lambda$)&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;TTD($\lambda$)&lt;/span&gt;라고 부릅니다. 아래 그림의 복합적인 Backup Diagram은 가장 긴 구성 요소에 대한 Update가 항상 Episode의 끝까지 진행되는 것이 아니라 최대 $n$-step이라는 점을 명시하고 있습니다. 그 점을 제외한다면 이전에 보여드린 $\lambda$-Return의 Backup Diagram과 유사합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TTD($\lambda$)에 대한 식은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \left[ G_{t:t+n}^{\lambda} - \hat{v} (S_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{v} (S_t, \mathbf{w}_{t+n-1}), \quad 0 \le t &amp;lt; T\]

&lt;p&gt;이 알고리즘은 각 단계별 계산이 $n$으로 확장되지 않도록 효율적으로 구현할 수 있습니다. (즉, 시간 복잡도가 $n$에 비례하지 않도록) $n$-step TD 방법과 마찬가지로 각 Episode의 처음 $n-1$ 시간 단계에서는 Update가 수행되지 않으며, Episode가 종료 후 $n-1$에 대한 추가적인 Update가 수행됩니다. 효율적인 구현을 위해 $k$-step $\lambda$-Return은 다음과 같이 표현으로 수정할 수 있습니다.&lt;/p&gt;

\[G_{t:t+k}^{\lambda} = \hat{v} (S_t, \mathbf{w}_{t-1}) + \sum_{i=t}^{t+k-1} (\gamma \lambda)^{i-t} \delta_i &apos; \tag{12.10}\]

&lt;p&gt;이 때, $\delta_i^{\prime} \doteq R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v} (S_t, \mathbf{w}_{t-1})$입니다.&lt;/p&gt;

&lt;h2 id=&quot;redoing-updates-online-lambda-return-algorithm&quot;&gt;Redoing Updates: Online $\lambda$-Return Algorithm&lt;/h2&gt;

&lt;p&gt;Truncated TD($\lambda$)에서 Truncation Parameter $n$을 선택할 때는 Trade-off가 있습니다. Truncated TD($\lambda$)가 Off-line $\lambda$-Return Algorithm에 근접하기 위해서 $n$이 커야 하지만, Update가 더 빨리 이루어지기 위해서는 $n$이 작아야 합니다. 이런 상황에서 둘 다 포기하지 않는 방법은 있지만, 그만큼 계산 복잡도가 증가하는 문제가 있습니다. 이번 Section에서는 이 방법을 소개하겠습니다.&lt;/p&gt;

&lt;p&gt;기본적인 아이디어는 새로운 데이터의 Increment를 얻을 때마다 현재 Episode의 시작 부분으로 되돌아가 모든 Update를 다시 실행하는 것입니다. 그러면 각 시간 단계에서 새로운 데이터를 고려할 수 있기 때문에 새 Update는 이전에 계산한 결과보다 더 나을 것이기 때문입니다. 즉, Update는 항상 최신 Horizon $h$를 사용하여 $n$-step Truncated $\lambda$-Return의 Target을 계산하는 것입니다. 각 Episode가 끝날 때마다 약간 더 긴 Horizon $h$를 사용하면 약간 더 나은 결과를 얻을 수 있습니다. 먼저, 식 (12.9)에서의 Truncated $\lambda$-Return은 다음과 같이 정의했었습니다.&lt;/p&gt;

\[G_{t:h}^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{h-t-1}G_{t:h}\]

&lt;p&gt;계산 복잡도가 문제가 되지 않는 상황에서 이 Target을 이상적으로 사용할 수 있는 방법을 단계별로 살펴보겠습니다. 각각의 Episode는 이전 Episode의 끝에서 $\mathbf{w}_0$을 사용하여 시간 0에서의 추정으로 시작합니다. 데이터 Horizon이 시간 단계 1로 확장될 때 학습이 시작됩니다. Horizon 1까지의 데이터가 주어지면, 시간 단계 0에서의 추정 Target은 $R_1$과 추정치 $\hat{v}(S_1, \mathbf{w}_0)$의 Bootstrap을 포함한 1-step Return $G_{0:1}$입니다. 이것은 정확하게 $G_{0:1}^{\lambda}$이며, 위 식의 첫 번째 항의 합은 0으로 감소합니다. 그 후 이 Target Update를 사용하여 $\mathbf{w}_1$을 만듭니다. 그 두 데이터 Horizon을 시간 단계 2로 진행한 후, $R_2$, $S_2$를 얻을 수 있으므로 $S_0$의 더 나은 Update인 $G_{0:2}^{\lambda}$와 $S_1$의 Update인 $G_{1:2}^{\lambda}$를 계산할 수 있습니다. 이렇게 더 나아진 Target을 사용하여 $S_1$과 $S_2$를 다시 Update하고, Weight Update를 $\mathbf{w}_0$부터 시작하여 $\mathbf{w}_2$를 계산합니다. 데이터 Horizon이 시간 단계 3으로 넘어가면 이 과정을 또다시 반복하는 것입니다. 이렇게 새로운 데이터 Horizon을 얻을 때마다 Weight Update를 $\mathbf{w}_0$부터 다시 계산하여 Update를 수행합니다.&lt;/p&gt;

&lt;p&gt;이러한 개념적인 알고리즘은 각각의 Horizon $h$에서 동일한 Episode에 대한 서로 다른 Weight Vector를 생성합니다. 이것을 명확하게 설명하기 위해서는 다른 Horizon에서 계산된 Weight Vector를 구별할 수 있어야 합니다. Horizon $h$까지의 과정에서 시간 $t$의 Value를 추정하는데 사용한 Weight를 $\mathbf{w}_t^h$라 합시다. 각 과정에서의 첫 번째 Weight Vector $\mathbf{w}_0^h$는 이전 Episode로부터 상속된 것이고 (모든 $h$에 대해 마찬가지), 각 과정의 마지막 Weight Vector $\mathbf{w}_h^h$는 알고리즘의 궁극적인 Weight Vector를 정의합니다. 마지막 Horizon $h = T$에서는 다음 Episode의 초기 Weight를 생성하기 위해 전달할 최종 Weight $\mathbf{w}_T^T$를 얻습니다. 이러한 과정을 $h = 3$까지 수식으로 표현하자면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
h = 1 : \mathbf{w}_1^1 \doteq \mathbf{w}_0^1 + \alpha \left[ G_{0:1}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^1) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^1) \\ \\
h = 2 : \mathbf{w}_1^2 \doteq \mathbf{w}_0^2 + \alpha \left[ G_{0:2}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^2) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^2) \\ \\
\quad \mathbf{w}_2^2 \doteq \mathbf{w}_1^2 + \alpha \left[ G_{1:2}^{\lambda} - \hat{v} (S_1, \mathbf{w}_1^2) \right] \nabla \hat{v} (S_1, \mathbf{w}_1^2) \\ \\
h = 3 : \mathbf{w}_1^3 \doteq \mathbf{w}_0^3 + \alpha \left[ G_{0:3}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^3) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^3) \\ \\
\quad \mathbf{w}_2^3 \doteq \mathbf{w}_1^3 + \alpha \left[ G_{1:3}^{\lambda} - \hat{v} (S_1, \mathbf{w}_1^3) \right] \nabla \hat{v} (S_1, \mathbf{w}_1^3) \\ \\
\quad \mathbf{w}_3^3 \doteq \mathbf{w}_2^3 + \alpha \left[ G_{2:3}^{\lambda} - \hat{v} (S_2, \mathbf{w}_2^3) \right] \nabla \hat{v} (S_2, \mathbf{w}_2^3)
\end{align}\]

&lt;p&gt;이것을 일반화하면 다음과 같은 수식을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1}^h \doteq \mathbf{w}_t^h + \alpha \left[ G_{t:h}^{\lambda} - \hat{v} (S_t, \mathbf{w}_t^h) \right] \nabla \hat{v} (S_t, \mathbf{w}_t^h), \quad 0 \le t &amp;lt; h \le T\]

&lt;p&gt;만약 $\mathbf{w}_t \doteq \mathbf{w}_t^t$로 정의된다면 &lt;span style=&quot;color:red&quot;&gt;On-line $\lambda$-Return Algorithm&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;On-line $\lambda$-Return Algorithm은 완전하게 On-line으로 동작하며, 시간 $t$에서 사용할 수 있는 정보만 사용하여 새로운 Weight Vector $\mathbf{w}_t$를 계산합니다. 이 때의 단점은 매 시간 단계마다 경험한 Episode의 일부를 사용하여 계산하는 것이 계산 복잡도가 높다는 것입니다. Off-line $\lambda$-Return Algorithm은 Episode를 수행하는 동안 Update를 수행하지 않고, Episode를 종료하는 시점에서 모든 시간 단계에 대한 Update를 수행했기 때문에 계산 복잡도가 높지 않았습니다. On-line $\lambda$-Return Algorithm은 그 계산 복잡도를 대가로 Episode가 진행되는 도중 뿐만 아니라 Episode가 끝날 때도 더 나은 성능을 기대할 수 있습니다. Bootrstrapping에 사용되는 Weight Vector에 반영되는 정보가 더 많기 때문입니다. 아래 그림은 지금까지 보았던 Random Walk Example에서 On-line과 Off-line 알고리즘을 비교한 그래프입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;true-online-tdlambda&quot;&gt;True Online TD($\lambda$)&lt;/h2&gt;

&lt;p&gt;이전 Section에서 제시한 On-line $\lambda$-Return Algorithm은 현재 가장 성능이 좋은 시분할 알고리즘입니다. 즉, On-line TD($\lambda$)를 근사화하는 이상적인 알고리즘입니다. On-line $\lambda$-Return Algorithm은 Forward View Algorithm이지만, 효율적으로 구현하기 위해서 Backward View Algorithm으로 변형시킬 방법이 있을까요? Linear Function Approximation의 경우라면 그 대답은 Yes입니다. 이 구현은 TD($\lambda$) Algorithm보다 On-line $\lambda$-Return Algorithm에 이상적으로 가깝기 때문에 &lt;span style=&quot;color:red&quot;&gt;True On-line TD($\lambda$) Algorithm&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)를 유도하는 과정을 여기에서 보이기에는 너무 복잡하기 때문에 여기에서는 생략하겠습니다. (다음 Section 및 van Seijen et al., 2016) 대략적인 아이디어를 소개하자면, 먼저 On-line $\lambda$-Return Algorithm은 다음과 같이 삼각형으로 나열할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 삼각형에서 하나의 행은 각 시간 단계에서 생성됩니다. 삼각형을 구성하는 요소는 많지만, 이전 Section에서 보았듯이 우리에게 필요한 것은 대각선 요소인 $\mathbf{w}_t^t$뿐입니다. 첫 번째 요소인 $\mathbf{w}_0^0$는 Episode의 초기 Weight Vector이고, 마지막 요소인 $\mathbf{w}_T^T$는 최종 Weight Vector이며, 그 중간 요소인 $\mathbf{w}_t^t$는 Update에 필요한 $n$-step Return을 얻기 위한 Bootstrapping 역할을 합니다.&lt;/p&gt;

&lt;p&gt;이제 삼각형의 대각선 구성 요소(가장 오른쪽 요소)는 표기의 편의를 위해 $\mathbf{w}_t \doteq \mathbf{w}_t^t$로 재정의하겠습니다. 이제 해야할 것은 대각선 구성 요소인 $\mathbf{w}_t$를 간결하고 효율적으로 계산하는 방법을 찾는 것입니다. 그렇게 하면 $\hat{v}(\mathbf{s}, \mathbf{w}) = \mathbf{w}^{\sf T} \mathbf{x} (\mathbf{s})$와 같은 Linear에 대해 다음과 같은 True On-line TD($\lambda$) Algorithm을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t + \alpha \left( \mathbf{w}_t^{\sf T} \mathbf{w}_t - \mathbf{w}_{t-1}^{\sf T} \mathbf{x}_t \right) \left( \mathbf{z}_t - \mathbf{x}_t \right),\]

&lt;p&gt;위 식에서 $\mathbf{x}_t \doteq \mathbf{x} (S_t)$이며, $\delta_t$는 TD($\lambda$)인 식 (12.6)을 의미합니다. 또한 $\mathbf{z}_t$는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma \lambda \mathbf{z}_{t-1} + \left( 1 - \alpha \gamma \lambda \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t \tag{12.11}\]

&lt;p&gt;이 알고리즘은 On-line $\lambda$-Return Algorithm과 정확하게 동일한 Weight Vector $\mathbf{w}_t (0 \le t \le T)$를 생성하는 것으로 증명되었습니다. (van Seijen et al., 2016) 이전 Section에서의 마지막 그림인 Random Walk의 On-line $\lambda$-Return Algorithm도 이것을 사용한 결과입니다. 이제 지금까지 단점으로 남아있던 높은 계산 복잡도가 해결되었습니다. 공간 복잡도 측면에서 보면 On-line TD($\lambda$)의 메모리 요구량은 기존 TD($\lambda$)의 메모리 요구량과 동일하고, 시간 복잡도 측면에서 보면 각 단계별 계산량은 약 50% 증가했지만, 전체적으로 보았을 때 각 단계별 시간 복잡도는 TD($\lambda$)와 동일하게 $O(d)$로 유지됩니다.&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)에 사용된 Eligibility Trace 식 (12.11)은 기존의 TD($\lambda$)에서 사용한 Eligibility Trace 식 (12.5)와 구분하기 위해 &lt;span style=&quot;color:red&quot;&gt;Dutch Trace&lt;/span&gt;라고 부릅니다. 참고로 식 (12.5)와 같은 Eligibility Trace은 Accumulating Trace라고 부르기도 합니다.&lt;/p&gt;

&lt;p&gt;이전에는 Tabular 방법이나 Tile Coding과 같은 Binary Feature Vector에 대해서는 &lt;span style=&quot;color:red&quot;&gt;Replacing Trace&lt;/span&gt;라고 하는 또 다른 방법을 사용했었습니다. Replacing Trace는 Feature Vector의 구성 요소가 1인지, 0인지에 따라 다르게 정의됩니다.&lt;/p&gt;

\[z_{i, t} \doteq \begin{cases} 1, &amp;amp; \text{if } x_{i, t} = 1 \\ \gamma \lambda z_{i, t-1}, &amp;amp; \text{otherwise} \end{cases} \tag{12.12}\]

&lt;p&gt;요즘에는 Replacing Trace를 Dutch Trace의 조잡한 근사치 정도로 간주합니다. Replacing Trace는 일반적으로 Dutch Trace보다 성능이 낮기 때문에 Dutch Trace로 이를 대체하는 경우가 많습니다. 물론, 이에 대한 이론적인 근거 또한 있습니다. Accumulating Trace는 Dutch Trace를 사용할 수 없는 non-Linear Function Approximation에서 사용하기 때문에 중요한 Trace로 간주합니다.&lt;/p&gt;

&lt;h2 id=&quot;dutch-traces-in-monte-carlo-learning&quot;&gt;Dutch Traces in Monte Carlo Learning&lt;/h2&gt;

&lt;p&gt;Eligibility Trace는 TD 학습과 밀접한 관련이 있는 것 같지만 실제로는 별로 관련이 없습니다. 이번 Section에서 보이겠지만, Monte Carlo 학습도 Eligibility Trace가 발생합니다. 9장에서 다루었던 Forward View에서 본 Linear Monte Carlo 알고리즘에 Dutch Trace를 사용하여 더 계산적으로 효율적인 Backward View 알고리즘을 유도할 수 있다는 것을 보일 예정입니다. 이점은 이 책의 저자가 인정하는 Forward View와 Backward View의 유일한 동치 부분입니다. 또한 이것은 True On-line TD($\lambda$)와 On-line $\lambda$-Return Algorithm의 동등성 증명 방향을 제공하지만, 훨씬 더 간단합니다.&lt;/p&gt;

&lt;p&gt;먼저 Gradient Monte Carlo 예측 알고리즘의 Linear 버전은 Episode의 각 시간 단계에서 하나씩 다음과 같은 Update가 발생합니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G - \mathbf{w}_t^{\sf T} \mathbf{x}_t \right] \mathbf{x}_t , \quad 0 \le t &amp;lt; T \tag{12.13}\]

&lt;p&gt;예시를 단순화하기 위해 Return $G$는 Episode가 끝날 때 받은 단일 Reward이고 Discount가 없다고 가정합니다. (Return $G$는 &lt;strong&gt;단일&lt;/strong&gt; 보상이기 때문에 $G_t$와 같이 시간에 대한 첨자가 붙지 않습니다.) 이 경우 Update는 Least Mean Square (LMS) 규칙이라고도 합니다. Monte Carlo 알고리즘에서의 모든 Update는 최종 Reward/Return에 따라 달라지므로 Episode가 끝날 때까지 아무것도 할 수 없기 때문에 Monte Carlo 알고리즘은 Off-line 알고리즘입니다. 그래서 여기서는 계산상의 이점이 있는 새로운 알고리즘의 구현을 목적으로 합니다. 새로운 알고리즘에서도 기존과 마찬가지로 Episode가 끝날 때만 Weight Vector를 Update하겠지만, Episode의 각 단계에서 약간의 계산을 수행함으로써 전체적으로 계산량을 고르게 분배할 계획입니다. 이것을 통해 단계당 $O(d)$의 시간 복잡도가 소요되지만, 각 단계에서 Feature Vector를 저장할 필요가 없습니다. 대신 Eligibility Trace를 도입하여 지금까지 경험한 모든 Feature Vector의 요점만을 저장합니다. 이 방법은 Episode가 끝날 때까지 식 (12.13)의 Update 과정과 정확히 동일한 Update를 효율적으로 재생성합니다. 식에 대한 전개 과정은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{T} &amp;amp;= \mathbf{w}_{T-1} + \alpha \left( G - \mathbf{w}_{T-1}^{\sf T} \mathbf{x}_{T-1} \right) \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{w}_{T-1} + \alpha \mathbf{x}_{T-1} \left( - \mathbf{x}_{T-1}^{\sf T} \mathbf{w}_{T-1} \right) + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \left( \mathbf{I} - \alpha \mathbf{x}_{T-1} \mathbf{x}_{T-1}^{\sf T} \right) \mathbf{w}_{T-1} + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{w}_{T-1} + \alpha G \mathbf{x}_{T-1}
\end{align}\]

&lt;p&gt;이 때 $\mathbf{F}_{t} \doteq \mathbf{I} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T}$는 &lt;span style=&quot;color:red&quot;&gt;Forgetting&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;Fading&lt;/span&gt;이라고 부르는 행렬입니다. 이제 위 식을 재귀적으로 전개해보면,&lt;/p&gt;

\[\begin{align}
&amp;amp;= \mathbf{F}_{T-1} \left( \mathbf{F}_{T-2} \mathbf{w}_{T-2} + \alpha G \mathbf{x}_{T-2} \right) + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{w}_{T-2} + \alpha G \left( \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \left( \mathbf{F}_{T-3} \mathbf{w}_{T-3} + \alpha G \mathbf{x}_{T-3} \right) + \alpha G \left( \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{F}_{T-3} \mathbf{w}_{T-3} + \alpha \left( \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{x}_{T-3} + \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp; \qquad \qquad \vdots \\ \\
&amp;amp;= \underbrace{\mathbf{F}_{T-1} \mathbf{F}_{T-2} \cdots \mathbf{F}_0 \mathbf{w}_0}_{\mathbf{a}_{T-1}} + \alpha G \underbrace{\sum_{k=1}^{T-1} \mathbf{F}_{T-1} \mathbf{F}_{T-2} \cdots \mathbf{F}_{k+1} \mathbf{x}_k}_{\mathbf{z}_{T-1}} \\ \\
&amp;amp;= \mathbf{a}_{T-1} + \alpha G \mathbf{z}_{T-1} \tag{12.14}
\end{align}\]

&lt;p&gt;여기서 $\mathbf{a}_{T-1}$과 $\mathbf{z}_{T-1}$는 시간 $T-1$에서의 메모리 Vector로써, $G$에 대한 정보 없이 각 시간 단계에서 $O(d)$의 시간 복잡도로 Update할 수 있습니다. 이 중 Vector $\mathbf{z}_{t}$는 Dutch-style Eligibility Trace입니다. 이 Vector는 시간 단계 0에서 $\mathbf{z}_0 = \mathbf{x}_0$으로 초기화된 후 다음과 같이 Update됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_t &amp;amp; \doteq \sum_{k=1}^t \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_{k+1} \mathbf{x}_k, \quad 1 \le t &amp;lt; T \\ \\
&amp;amp;= \sum_{k=0}^{t-1} \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_{k+1} \mathbf{x}_k + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{F}_t \sum_{k=1}^{t-1} \mathbf{F}_{t-1} \mathbf{F}_{t-2} \cdots \mathbf{F}_{k+1} \mathbf{x}_k + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{F}_t \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \left( I - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \right) \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} - \alpha \left( \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} + \left( 1 - \alpha \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t
\end{align}\]

&lt;p&gt;이것은 식 (12.11)에서 $\gamma \lambda = 1$인 경우에 대한 Dutch Trace입니다. Auxiliary Vector $\mathbf{a}_t$는 시간 단계 0에서 $\mathbf{a}_0 = \mathbf{w}_0$로 초기화 된 후, 다음과 같이 Update됩니다.&lt;/p&gt;

\[\mathbf{a}_t \doteq \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_0 \mathbf{w}_0 = \mathbf{F}_t \mathbf{a}_{t-1} = \mathbf{a}_{t-1} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \mathbf{a}_{t-1}, \quad 1 \le t &amp;lt; T\]

&lt;p&gt;Auxiliary Vector $\mathbf{a}_t$와 Dutch Trace $\mathbf{z}_{t}$는 각 시간 단계 $t &amp;lt; T$에서 Update되고 $G$를 알 수 있는 시간 단계 $T$에서는 $\mathbf{w}_T$를 계산하기 위해 식 (12.14)에서 사용됩니다. 이런 방법으로 계산 복잡도가 높은 식 (12.13)과 같은 MC/LMS 알고리즘과 정확히 동일한 최종 결과를 얻었습니다. 새로 구한 방법은 각 시간 단계별 시간 복잡도 및 공간 복잡도가 $O(d)$인 증분 알고리즘을 사용합니다. 이것은 TD Learning이 아닌 환경에서 Eligibility Trace의 개념이 발생했기 때문에 흥미로운 결과입니다. 그러므로 Eligibility Trace는 TD Learning에만 국한되지 않는다는 것을 알 수 있습니다. Eligibility Trace의 필요성은 효율적인 방식으로 장기적인 예상치를 학습하고자 할 때 나타나는 것으로 보면 될 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;sarsalambda&quot;&gt;Sarsa($\lambda$)&lt;/h2&gt;

&lt;p&gt;Eligibility Trace를 Action-Value로 확장하기 위해서는 다행스럽게도 이미 제시된 아이디어에서 거의 변경할 필요가 없습니다. Estimated Value인 $\hat{q} (s, a, \mathbf{w})$를 학습하기 위해서는 아래와 같이 10장에서 배운 $n$-step Return의 Action-Value 형태를 사용해야 합니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{q} \left( S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1} \right), \quad t+n &amp;lt; T\]

&lt;p&gt;이 때 만약 $t + n \ge T$인 경우라면 $G_{t:t+n} \doteq G_t$입니다. 이 방법을 통해 $\lambda$-Return의 Action-Value 형태를 만들 수 있습니다. Off-line $\lambda$-Return Algorithm의 Action-Value 형식은 단순히 $\hat{v}$를 $\hat{q}$로 대체하면 됩니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G_t^{\lambda} - \hat{q} \left( S_t, A_t, \mathbf{w}_t \right) \right] \nabla \hat{q} \left( S_t, A_t, \mathbf{w}_t \right), \quad t = 0, \ldots, T - 1 \tag{12.15}\]

&lt;p&gt;이 때 $G_t^{\lambda} \doteq G_{t:\infty}^{\lambda}$입니다. 이 Forward View 에 대한 복합적인 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Backup Diagram과 TD($\lambda$)의 Backup Diagram을 비교해보면 굉장히 유사하다는 것을 알 수 있습니다. 또한 $\lambda$-Return에서 각 $n$-step Update의 Weight는 TD($\lambda$) 및 $\lambda$-Return 알고리즘에서와 같습니다.&lt;/p&gt;

&lt;p&gt;Action Value에 대한 TD 방법은 &lt;span style=&quot;color:red&quot;&gt;Sarsa($\lambda$)&lt;/span&gt;로 알려져 있는데, 이것은 이 Forward View를 추정합니다. 이 방법의 Update 규칙은 아래와 같이 TD($\lambda$)와 동일합니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t\]

&lt;p&gt;단, TD Error의 Action-Value 형태는 예외적입니다.&lt;/p&gt;

\[\delta_t \doteq R_{t+1} + \gamma \hat{q} \left( S_{t+1}, A_{t+1}, \mathbf{w}_t \right) - \hat{q} \left( S_t, A_t, \mathbf{w}_t \right) \tag{12.16}\]

&lt;p&gt;또한 Eligibility Trace의 Action-Value 형태는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_{-1} &amp;amp; \doteq \mathbf{0} \\ \\
\mathbf{z} &amp;amp; \doteq \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{q} \left( S_t, A_t, \mathbf{w} \right), \quad 0 \le t \le T
\end{align}\]

&lt;p&gt;Sarsa($\lambda$)의 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 12.1) Traces in Gridworld&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 사용하면 1-step 방법이나 $n$-step 방법보다 Control 알고리즘의 효율성을 크게 높일 수 있습니다. Gridworld 예제를 이용하여 이것을 설명하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 그림은 단일 Episode에서 Agent가 이동한 경로를 나타냅니다. 초기 Estimated Value는 0이고, G로 표시된 Target 지점을 제외하면 모든 Reward는 0입니다. 나머지 그림에 나타난 화살표는 각각의 알고리즘에 대해 어떤 Action-Value가 얼마나 증가하는지를 나타냅니다. 1-step Sarsa는 Target에 도달했을 때 마지막 Action에 대한 Value만 증가시키지만, $n$-step 방법은 마지막 $n$개의 Action에 대한 Value를 동일하게 증가시킵니다. ($\gamma = 1$이라고 가정) 가장 오른쪽에 있는 Sarsa($\lambda$) 방법은 Episode에서의 모든 Action에 대한 Value를 Update 하지만, Target 지점에서 (시간적으로) 멀어질수록 더 적게 반영됩니다. 이러한 Update 방법을 &lt;span style=&quot;color:red&quot;&gt;Fading&lt;/span&gt;이라고 하는데, 일반적으로 Fading 방법이 제일 좋은 경우가 많습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 12.2) Sarsa($\lambda$) on Mountain Car&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 10장에서 다루었던 Mountain Car 예제에 Sarsa($\lambda$)를 적용해 보겠습니다. 기본적인 예제의 세팅은 10장에서와 동일합니다. 위의 그림은 Mountain Car 문제에 대해  Sarsa($\lambda$)와 $n$-step Sarsa의 성능을 비교한 그래프입니다. $n$-step Sarsa에서는 변수로써 $n$의 값을 변경하며 비교했지만, Sarsa($\lambda$)에서는 $\lambda$의 값을 변경하며 비교합니다. 두 그래프를 비교해보면 Sarsa($\lambda$)의 Fading-trace bootstrapping 전략이 이 문제에 대해 더 효율적인 학습 방법이라는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;또한 이상적인 TD 방법의 Action-Value 버전을 On-line $\lambda$-Return 알고리즘 및 True On-line TD($\lambda$)으로 구현할 수도 있습니다. Section 12.4에서 다룬 On-line $\lambda$-Return 알고리즘의 Action-Value 버전은 $n$-step Return을 Action-Value 형식으로 바꾸는 것 외에는 변경할 부분이 없습니다. 또한 Section 12.5와 12.6에서의 분석은 Action-Value에 대해서도 동일하며, 유일한 차이점은 State에 대한 Feature Vector를 $\mathbf{x}_t = \mathbf{x}(S_t)$ 대신 $\mathbf{x}_t = \mathbf{x}(S_t, A_t)$로 사용한다는 것입니다. True On-line Sarsa($\lambda$)에 대한 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 그림은 Mountain Car 예제에서 Sarsa($\lambda$)의 여러 버전에 대해 성능을 비교하는 그래프입니다. True On-line Sarsa($\lambda$)는 일반 Sarsa($\lambda$)보다 더 나은 성능을 보여줌을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;variable-lambda-and-gamma&quot;&gt;Variable $\lambda$ and $\gamma$&lt;/h2&gt;

&lt;p&gt;이제 기본적인 TD Learning 알고리즘에 대한 개발은 끝을 향해 달려가고 있습니다. 최종 알고리즘을 일반적인 형태로 나타내기 위해서는 State와 Action에 잠재적으로 의존하는 함수에 대해 일정한 매개변수는 물론, Bootstrapping 및 Discounting에 대한 정도를 일반화하는 것이 좋습니다. 즉, 각 시간 단계에 대해 서로 다른 $\lambda$및 $\gamma$를 설정하여, 이것을 각각 $\lambda_t$와 $\gamma_t$로 표현하는 것입니다. 또한 표기법을 변경하여 기존의 $\lambda$는 $\lambda : \mathcal{S} \times \mathcal{A} \to [0, 1]$와 같이 State와 Action에 대한 함수로 정의하고, 시간에 따라 변하는 $\lambda_t$는 $\lambda_t \doteq \lambda (S_t, A_t)$와 같이 함수 $\lambda$를 사용하여 표현합니다. 비슷하게, $\gamma$ 또한 $\gamma : \mathcal{S} \to [0, 1]$로 정의하고, $\gamma_t$를 $\gamma_t \doteq \gamma (S_t)$로 표현합니다.&lt;/p&gt;

&lt;p&gt;함수 $\gamma$는 특히 더 중요한데, 우리가 추정하고자 하는 기본 확률 변수인 Return을 변경하기 때문입니다. 이 함수 $\gamma$를 앞으로 &lt;span style=&quot;color:red&quot;&gt;Termination Function&lt;/span&gt;이라고 부르겠습니다. 이제 Return을 다음과 같이 더 일반적으로 정의하겠습니다.&lt;/p&gt;

\[\begin{align}
G_t &amp;amp; \doteq R_{t+1} + \gamma_{t+1} G_{t+1} \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} R_{t+2} + \gamma_{t+1} \gamma_{t+2} R_{t+3} + \gamma_{t+1} \gamma_{t+2} \gamma_{t+3} R_{t+4} + \cdots \\ \\
&amp;amp;= \sum_{k=t}^{\infty} \left( \prod_{i=t+1}^{k} \gamma_i \right) R_{k+1} \tag{12.17}
\end{align}\]

&lt;p&gt;식 (12.17)에서 합이 유한함을 보장하기 위해서는 모든 $t$에 대해 1의 확률로 $\prod_{k=t}^{\infty} \gamma_k = 0$를 만족해야 합니다.&lt;/p&gt;

&lt;p&gt;위와 같은 정의의 편리한 점은 Episode의 설정이나 알고리즘이 특정한 Terminal State나 Start Distribution, 종료 시간과 같은 특별한 설정 없이 단일 경험의 관점에서 표시될 수 있다는 것입니다. 특별한 경우로, 이전의 Terminal State는 $\gamma (s) = 0$인 State가 되어 Start Distribution으로 전환됩니다. 그리고 다른 모든 State에서 상수로 $\gamma ( \cdot )$를 선택함으로써 고전적인 Episode Task로 설정할 수 있습니다. State에 의존적인 종료에는 Markov Process의 흐름을 변경하지 않고 수량을 예측하는 Pseudo Termination과 같은 다른 예측 사례가 포함됩니다. Discounted Return은 그러한 수량으로 생각할 수 있으며, 이 경우 State에 의존적인 종료는 Episodic Task과 Discounted-Continuing Task 모두를 통합합니다. 물론 unDiscounted-Continuing Task의 경우 여전히 특별한 해결 방법이 필요합니다. (이 문단의 번역이 굉장히 어렵네요. 최대한 노력했습니다만 이해가 어려우실 것 같아 원문을 함께 봐주시기 바랍니다)&lt;/p&gt;

&lt;p&gt;가변 Bootsrtapping에 대한 일반화는 Discounting과 같은 문제의 변경이 아니라 해결 방법의 변경입니다. 일반화하는 State와 Action을 위한 $\lambda$-Return에 영향을 미칩니다. 새로운 State 기반 $\lambda$-Return은 다음과 같이 재귀적으로 작성할 수 있습니다.&lt;/p&gt;

\[G_t^{\lambda s} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \hat{v} (S_{t+1}, \mathbf{w}_t) + \lambda_{t+1} G_{t+1}^{\lambda s} \right) \tag{12.18}\]

&lt;p&gt;식 (12.18)은 위 첨자 $s$를 추가하여 이것이 State-Value에서 Bootstrapping 하는 Return임을 나타내고 있습니다. 이와 반대로 아래 식 (12.19)는 위 첨자로 $a$를 추가하여 Action-Value에서 Bootstrapping 하는 Return임을 나타냅니다. 이 식의 첫 번째 항은 $\lambda$-Return에서 Bootstrapping에 영향받지 않고 unDiscounted인 첫 번째 Reward를 의미합니다. 두 번째 항은 만약 다음 State가 Terminal State라면 0이 됩니다.&lt;/p&gt;

&lt;p&gt;만약 다음 State가 Terminal State가 아니라면, 두 번째 항은 State의 Bootstrapping 정도에 따라 두 가지 경우로 구분됩니다. Bootstrapping하는 범위 내에서 이 항은 State에서 추정된 값이지만, Bootstrapping 하지 않는 범위에서 이 항은 다음 시간 단계에서의 $\lambda$-Return입니다.&lt;/p&gt;

&lt;p&gt;Action 기반의 $\lambda$-Return Sarsa 형태는 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda a} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t) + \lambda_{t+1} G_{t+1}^{\lambda a} \right) \tag{12.19}\]

&lt;p&gt;위 식을 Expected Sarsa 형태로 수정하면 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda a} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \bar{V}_t (S_{t+1}) + \lambda_{t+1} G_{t+1}^{\lambda a} \right) \tag{12.20}\]

&lt;p&gt;식 (12.20)에서 $\bar{V}_t (s)$는 다음과 같이 Function Approximation으로 정의됩니다.&lt;/p&gt;

\[\bar{V}_t (s) \doteq \sum_a \pi (a | s) \hat{q} (s, a, \mathbf{w}_t) \tag{12.21}\]

&lt;h2 id=&quot;off-policy-traces-with-control-variates&quot;&gt;Off-policy Traces with Control Variates&lt;/h2&gt;

&lt;p&gt;Eligibility Trace의 마지막 단계는 Importance Sampling을 통합하는 것입니다. non-Truncated $\lambda$-Return을 사용하는 방법의 경우 Importance Sampling의 Weight가 Target Return에 적용할 수 있는 옵션이 없습니다. (ex. Section 7.3의 $n$-step 방법) 그래서 대신 Section 7.4에서와 같이 Control Variate가 있는 Per-decision Importance Sampling의 Bootstrapping 일반화로 해결하고자 합니다.&lt;/p&gt;

&lt;p&gt;State의 경우, 식 (12.18)의 $\lambda$-Return 일반화에 대한 최종 정의는 식 (7.13)과 결합하여 다음과 같이 정의됩니다.&lt;/p&gt;

\[G_t^{\lambda s} \doteq \rho_t \Big( R_{t+1} + \gamma_{t+1} \big( (1 - \lambda_{t+1}) \hat{v} (S_{t+1}, \mathbf{w}) + \lambda_{t+1} G_{t+1}^{\lambda s} \big) \Big) + (1 - \rho_t) \hat{v} (S_t, \mathbf{w}_t) \tag{12.22}\]

&lt;p&gt;여기서 $\rho_t = \frac{\pi (A_t \mid S_t)}{b (A_t \mid S_t)}$는 단일 단계 Importance Sampling Ratio입니다. 이 교재에서 다루었던 다른 Return과 마찬가지로 이 최종 $\lambda$-Return은 단순히 State 기반 TD Error의 합계로 근사할 수 있습니다. 먼저 State 기반 TD Error는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\delta_t^s \doteq R_{t+1} + \gamma_{t+1} \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{12.23}\]

&lt;p&gt;이것을 통해 $G_t^{\lambda s}$를 근사하면,&lt;/p&gt;

\[G_t^{\lambda s} \approx \hat{v}(S_t, \mathbf{w}_t) + \rho_t \sum_{k=t}^{\infty} \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \tag{12.24}\]

&lt;p&gt;이 때, Approximate Value Function $\hat{v}$가 변하지 않으면 식 (12.24)의 근사는 정확해집니다.&lt;/p&gt;

&lt;p&gt;식 (12.24)와 같은 $\lambda$-Return의 형태는 Forward-View Update에서 사용하기 편리해 보입니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t + \alpha \big( G_t^{\lambda s} - \hat{v} (S_t, \mathbf{w}_t) \big) \nabla \hat{v} (S_t, \mathbf{w}_t) \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \rho_t \left( \sum_{k=t}^{\infty} \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \right) \nabla \hat{v} (S_t, \mathbf{w}_t)
\end{align}\]

&lt;p&gt;위 식은 Eligibility에 기반한 TD Update처럼 보입니다. $\prod$ 연산 부분은 Eligibility Trace와 같으며, 여기에 TD Error가 곱해집니다. 그러다 이것은 Forward View의 한 단계일 뿐입니다. 우리가 찾고 있는 관계는 시간이 지남에 따라 합산되는 Forward View Update가 역시 시간이 지남에 따라 합산되는 Backward View Update와 거의 같다는 것입니다. (다만 이 관계는 Value Function의 변경을 무시하기 때문에 대략적인 것으로만 성립합니다) 시간 경과에 따른 Forward View Update의 합계는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\sum_{t=0}^{\infty} (\mathbf{w}_{t+1} - \mathbf{w}_t) &amp;amp; \approx \sum_{t=0}^{\infty} \sum_{k=t}^{\infty} \alpha \rho_t \delta_k^s \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;= \sum_{k=0}^{\infty} \sum_{t=0}^k \alpha \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;(\text{using the summation rule : } \sum_{t=x}^y \sum_{k=t}^y = \sum_{k=x}^y \sum_{t=x}^k) \\ \\
&amp;amp;= \sum_{k=0}^{\infty} \alpha \delta_k^s \sum_{t=0}^k \rho_t \nabla (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i
\end{align}\]

&lt;p&gt;위 식의 두 번째 합계부터는 전체 표현식이 Eligibility Trace으로 작성되고 점진적으로 Update 될 수 있는 경우 Backward View TD Update의 합계 형태가 될 것입니다. 즉, 이 표현식이 시간 $k$에서의 trace이면, 시간 $k-1$의 Value에서 이것을 Update할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_k &amp;amp;= \sum_{t=0}^k \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;= \sum_{t=0}^{k-1} \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i + \rho_k \nabla \hat{v}(S_k, \mathbf{w}_k) \\ \\
&amp;amp;= \gamma_k \lambda_k \rho_k \underbrace{\sum_{t=0}^{k-1} \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^{k-1} \gamma_i \lambda_i, \rho_i}_{\mathbf{z}_{k-1}} + \rho_k \nabla \hat{v} (S_k, \mathbf{w}_k) \\ \\
&amp;amp;= \rho_k \big( \gamma_k \lambda_k \mathbf{z}_{k-1} + \nabla \hat{v} (S_k, \mathbf{w}_k) \big)
\end{align}\]

&lt;p&gt;위 전개식에서는 아래 첨자가 $k$로 나와있지만, 시간에 대해 일반화하는 것을 명시하기 위해 아래 첨자를 $t$로 수정하여 최종 결과를 정리하겠습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \rho_t \big( \gamma_t \lambda_t \mathbf{z}_{t-1} + \nabla \hat{v} (S_t, \mathbf{w}_t) \big) \tag{12.25}\]

&lt;p&gt;이 Eligibility Trace는 식 (12.7)과 같은 TD($\lambda$)에 대한 일반적인 Semi-gradient 매개변수 Update 규칙과 함께 On-policy 또는 Off-policy 데이터에 적용할 수 있는 일반적인 TD($\lambda$) 알고리즘을 형성합니다. On-policy의 경우 $\rho_t$가 항상 1이므로 식 (12.25)가 식 (12.5)와 동일해지기 때문에 알고리즘은 정확히 TD($\lambda$)입니다. Off-policy의 경우 알고리즘이 잘 작동하는 경우가 많지만 Semi-gradient 방법으로는 안정성이 보장되지 않습니다. 이어지는 다음 여러 Section을 통해 안정성을 보장할 수 있는 방법을 고려할 것입니다.&lt;/p&gt;

&lt;p&gt;위와 유사한 과정을 거쳐 Action-Value에 대한 방법과 이에 해당하는 일반적인 Sarsa($\lambda$) 알고리즘에 대한 Off-policy Eligibility Trace를 얻을 수 있습니다. 전자는 식 (12.19)나 (12.20)과 같은 일반적인 Action 기반 $\lambda$-Return에 대한 재귀 방법으로 시작해야 하지만, 후자(Expected Sarsa 형식)는 더 간단합니다. 식 (12.20)에 식 (7.14)를 결합하여 다음과 같이 Off-policy로 확장하면 되기 때문입니다.&lt;/p&gt;

\[\begin{align}
G_t^{\lambda a} &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \Big( (1 - \lambda_{t+1}) \bar{V}_{t} (S_{t+1}) + \lambda_{t+1} [ \rho_{t+1} G_{t+1}^{\lambda a} + \hat{V}_t (S_{t+1}) - \rho_{t+1} \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t) ] \Big) \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} \Big( \bar{V}_t (S_{t+1}) + \lambda_{t+1} \rho_{t+1} \left[ G_{t+1}^{\lambda a} - \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t ) \right] \Big) \tag{12.26}
\end{align}\]

&lt;p&gt;식 (12.26)에서 $\bar{V}_t (S_{t+1})$은 식 (12.21)과 동일합니다. 그리고 또 다시  $\lambda$-Return은 TD Error의 합으로 표현할 수 있습니다.&lt;/p&gt;

\[G_t^{\lambda a} \approx \hat{q}(S_t, A_t, \mathbf{w}_t) + \sum_{k=t}^{\infty} \delta_k^a \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \tag{12.27}\]

&lt;p&gt;식 (12.27)의 $\delta_t^a$는 다음과 같이 Action 기반 TD Error의 기대 형식으로 정의됩니다.&lt;/p&gt;

\[\delta_t^a = R_{t+1} + \gamma_{t+1} \bar{V}_t (S_{t+1}) - \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{12.28}\]

&lt;p&gt;식 (12.24)와 마찬가지로, Approximate Value Function $\hat{q}$가 변하지 않으면 식 (12.27)의 근사값은 정확해집니다.&lt;/p&gt;

&lt;p&gt;State의 경우에 대한 과정과 유사한 과정을 사용하여 식 (12.27)에 기반한 Forward View Update를 유도하고, Summation Rule을 사용하여 Update의 합계를 변환한 후, 마지막으로 Action-Value에 대한 Eligibility Trace를 다음과 같이 유도할 수 있습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma_t \lambda_t \rho_t \mathbf{z}_{t-1} + \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{12.29}\]

&lt;p&gt;이 Eligibility Trace는 식 (12.28)과 같은 TD Error 및 식 (12.7)의 일반적인 Semi-gradient Update 규칙과 함께 On-policy, 또는 Off-policy에 적용할 수 있는 효율적인 Expected Sarsa($\lambda$) 알고리즘을 생성합니다. 이것은 아마도 현 시점에서 가장 좋은 알고리즘일 것입니다. (물론, 위에서 언급했듯이 아직까지는 안정성이 보장되지 않습니다.) On-policy의 경우 상수 $\lambda$와 $\gamma$, 그리고 식 (12.16)과 같은 State-Action TD Error를 사용한 알고리즘은 Section 12.7에서 제시된 Sarsa($\lambda$)와 동일합니다.&lt;/p&gt;

&lt;p&gt;$\lambda = 1$에서 이러한 알고리즘은 Monte Carlo 알고리즘과 밀접한 관련이 있습니다. Episodic Task와 Off-line Update에 대해 정확하게 동일할 것으로 생각할 수도 있지만, 실제 관계는 그것보단 약합니다. 가장 간단한 조건은 Episode별로 Update가 없으며 기대치만 있는 경우입니다. 이 방법은 Trajectory가 이어짐에 따라 (철회할 수 없는) Update를 만들지만, True Monte Carlo Method는 Trajectory가 있는 경우 Target Policy 하에 0의 확률을 가진 Action이 있을 경우 Trajectory를 Update하지 않습니다. 특히 이 모든 방법들은 $\lambda = 1$일지라도 Target이 현재 Value에 대한 추정치에 의존하기 때문에 여전히 Bootstrap합니다. 이것이 실제로 좋은지 나쁜지는 또 다른 문제입니다.&lt;/p&gt;

&lt;p&gt;관련 연구를 하나 소개하자면 (Sutton, Mahmood, Precup and van Hasselt, 2014)의 논문에서 정확한 동등성을 달성하는 방법이 제안되었습니다. 이 방법은 Update를 추적하지만, 나중에 취한 Action에 따라 철회할 수 있는 &lt;strong&gt;Provisional Weight&lt;/strong&gt;라는 추가적인 Vector를 이용합니다. 이 방법의 State 및 State-Action 방법 버전을 각각 &lt;span style=&quot;color:red&quot;&gt;PTD($\lambda$)&lt;/span&gt;와 &lt;span style=&quot;color:red&quot;&gt;PQ($\lambda$)&lt;/span&gt;라고 합니다. 여기서 ‘P’는 &lt;strong&gt;Provisional&lt;/strong&gt;의 약자입니다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 새로운 Off-policy 방법의 실질적인 결과는 아직 확립되지 않았습니다. 확실한 것은, Importance Sampling을 사용하는 모든 Off-policy 방법과 마찬가지로 높은 Variance 문제가 발생할 것입니다.&lt;/p&gt;

&lt;p&gt;만약 $\lambda &amp;lt; 1$이면 모든 Off-policy 알고리즘은 Bootstrapping을 포함하고 Section 11.3에서 언급한 &lt;strong&gt;Deadly Traid&lt;/strong&gt;가 적용됩니다. 이것은 Tabular, State Aggregation 및 기타 제한된 형태의 Function Approximation에 대해서만 안정성이 보장될 수 있다는 것을 의미합니다. Linear과 같은 보다 일반적인 형태의 Function Approximation의 경우, 매개변수 Vector는 11장의 예제에서와 같이 무한대로 발산할 수 있습니다. 11장에서 논의한 바와 같이 Off-policy 학습의 과제는 크게 두 부분으로 나눌 수 있습니다. Off-policy Eligibility Trace는 문제의 첫 번째 부분은 효과적으로 처리하여 Target의 Expected Value를 추정하지만, Update의 Distribution과 관련된 두 번째 문제는 전혀 처리하지 못합니다. Eligibility Trace를 포함한 Off-policy 학습에서 두 번째 문제를 해결하기 위한 알고리즘 전략은 Section 12.11에서 보일 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;watkinss-qlambda-to-tree-backuplambda&quot;&gt;Watkins’s Q($\lambda$) to Tree-Backup($\lambda$)&lt;/h2&gt;

&lt;p&gt;Q-learning을 Eligibility Trace로 확장하기 위해 여러 방법이 제안되었습니다. 가장 처음 제안된 방법은 Watkins의 &lt;span style=&quot;color:red&quot;&gt;Q($\lambda$)&lt;/span&gt;로, Greedy Action이 수행되는 한 일반적인 방식으로 Eligibility Trace을 감소시킨 다음, 첫 번째 non-Greedy Action 후에 Trace를 0으로 줄입니다. Q($\lambda$)의 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;6장에서 Q-learning과 Expected Sarsa를 통합하여 임의의 Target Policy로 일반화했으며, 이 장의 이전 Section에서 Expected Sarsa를 Off-policy Eligibility Trace로 일반화하였습니다. 그러나 7장에서는 Importance Sampling을 사용하지 않는 속성을 유지한 $n$-step Tree Backup과 $n$-step Expected Sarsa를 구분했습니다. 이제 우리는 Tree Backup의 Eligibility Trace 버전인 &lt;span style=&quot;color:red&quot;&gt;Tree Backup($\lambda$)&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;TB($\lambda$)&lt;/span&gt;를 제시해야 합니다. 이는 Off-policy 데이터에 적용할 수 있음에도 불구하고 Importance Sampling이 없다는 장점이 있기 때문에 Q-learning의 진정한 확장이라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;TB($\lambda$)의 개념은 간단합니다. Section 7.5에서와 같이 Tree Backup의 Update는 Bootstrapping Parameter $\lambda$에 따라 일반적인 방식으로 Weight가 부여됩니다. TB($\lambda$)의 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반적인 Bootstrapping 및 Discounting Parameter에 대한 올바른 Index를 사용하여 구체적인 방정식을 얻으려면, 다음과 같이 식 (12.20) $\lambda$-Return의 재귀 형식으로 시작한 다음, 식 (7.16) Target의 Bootstrapping 경우로 확장하는 것이 가장 좋습니다.&lt;/p&gt;

\[\begin{align}
G_t^{\lambda a} &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \Big( (1 - \lambda_{t+1}) \bar{V}_t (S_{t+1}) + \lambda_{t+1} \big[ \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) \hat{q} (S_{t+1}, a, \mathbf{w}_t) + \pi (A_{t+1} | S_{t+1}) G_{t+1}^{\lambda a} \big] \Big) \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} \Big( \bar{V}_t (S_{t+1}) + \lambda_{t+1} \pi (A_{t+1} | S_{t+1}) \big( G_{t+1}^{\lambda a} - \hat{v} (S_{t+1}, A_{t+1}, \mathbf{w}_t) \big) \Big)
\end{align}\]

&lt;p&gt;이전과 마찬가지로, $G_t^{\lambda a}$를 TD Error의 합으로 근사할 수도 있습니다. 이 때 TD Error는 식 (12.28)과 같은 형태를 사용합니다.&lt;/p&gt;

\[G_t^{\lambda a} \approx \hat{q} (S_t, A_t, \mathbf{w}_t) + \sum_{k=t}^{\infty} \delta_k^a \prod_{i=t+1}^k \gamma_i \lambda_i \pi (A_i | S_i)\]

&lt;p&gt;이전 Section과 동일한 단계에 따라, 선택한 Action의 Target Policy 확률과 관련된 특별한 Eligibility Trace Update를 유도할 수 있습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma_t \lambda_t \pi (A_t | S_t) \mathbf{z}_{t+1} + \nabla \hat{q}(S_t, A_t, \mathbf{w}_t)\]

&lt;p&gt;이것은 식 (12.7)과 같은 일반적인 매개변수 Update 규칙과 함께 TB($\lambda$) 알고리즘을 정의합니다. 모든 Semi-gradient 알고리즘과 마찬가지로 TB($\lambda$)는 Off-policy 데이터와 강력한 Function Approximation 방법과 함께 사용했을 때 안정성이 보장되지 않습니다. 안정성을 보장받기 위해서는 TB($\lambda$)를 다음 Section에 나오는 방법 중 하나와 결합해야 합니다.&lt;/p&gt;

&lt;h2 id=&quot;stable-off-policy-methods-with-traces&quot;&gt;Stable Off-policy Methods with Traces&lt;/h2&gt;

&lt;p&gt;Eligibility Trace를 사용한 Off-policy 학습에서 안정성을 보장하기 위한 여러 방법이 제안되었습니다. 여기에서는 일반적인 Bootstrapping 및 Discount Function을 포함한 네 가지 방법을 제시합니다. 이 방법들은 모두 Section 11.7과 11.8에 제시한 Gradient-TD 또는 Emphatic-TD의 아이디어에 기반하고 있습니다. 모든 알고리즘은 Linear Function Approximation를 사용한다고 가정하지만, non-Linear Function Approximation에 대한 확장도 여러 논문에서 찾을 수 있습니다.&lt;/p&gt;

&lt;p&gt;첫 번째 방법으로 &lt;span style=&quot;color:red&quot;&gt;GTD($\lambda$)&lt;/span&gt;는 TDC와 유사한 Eligibility Trace 알고리즘으로, Section 11.7에서 제시한 두 가지 State-Value Gradient TD 예측 알고리즘보다 우수합니다. 이 알고리즘의 목표는 Behavior Policy $b$를 따르는 데이터를 위해 $\hat{v} (s, \mathbf{w}) \doteq \mathbf{w}_t^{\sf T} \mathbf{x} (s) \approx v_{\pi}(s)$와 같은 식에서 매개변수 $\mathbf{w}_t$를 학습하는 것입니다. 이 방법의 Update 식은 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t^s \mathbf{z}_t - \alpha \gamma_{t+1} (1 - \lambda_{t+1}) (\mathbf{z}_t^{\sf T} \mathbf{v}_t) \mathbf{x}_{t+1}\]

&lt;p&gt;위 식에서 $\delta_t^s$는 식 (12.23), $\mathbf{z}$는 식 (12.25), $\rho_t$는 식 (11.1)과 같습니다. 그리고 $\mathbf{v}_t$는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\mathbf{v}_{t+1} \doteq \mathbf{v}_t + \beta \delta_t^s \mathbf{z}_t - \beta (\mathbf{v}_t^{\sf T} \mathbf{x}_t) \mathbf{x}_t \tag{12.30}\]

&lt;p&gt;Section 11.7에서와 같이 $\mathbf{v} \in \mathbb{R}^d$는 $\mathbf{w}$와 같은 차원의 Vector이고, $\mathbf{v} = \mathbf{0}$으로 초기화됩니다. 그리고 $\beta &amp;gt; 0$은 두 번째 Step-size Parameter입니다.&lt;/p&gt;

&lt;p&gt;두 번째 방법인 &lt;span style=&quot;color:red&quot;&gt;GQ($\lambda$)&lt;/span&gt;는 Eligibility Trace가 포함된 Action-Value에 대한 Gradient-TD 알고리즘입니다. 이 알고리즘의 목표는 Off-policy 데이터에서 $\hat{q} (s, a, \mathbf{w}_t) \doteq \mathbf{w}_t^{\sf T} \mathbf{x}(s, a) \approx q_{\pi} (s, a)$와 같은 식의 매개변수 $\mathbf{w}_t$를 학습하는 것입니다. 만약 Target Policy가 $\epsilon$-greedy인 경우, 또는 $\hat{q}$에 대한 Greedy Policy로 Bias되는 경우 GQ($\lambda$)를 Control 알고리즘으로 사용할 수 있습니다. 이 방법의 Update는 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w} + \alpha \delta_t^a \mathbf{z}_t - \alpha \gamma_{t+1} (1 - \lambda_{t+1}) (\mathbf{z}_t^{\sf T} \mathbf{v}_t) \bar{\mathbf{x}}_{t+1}\]

&lt;p&gt;위 식에서 $\bar{\mathbf{x}}_t$는 Target Policy를 따르는 $S_t$에 대한 평균 Feature Vector로 정의됩니다.&lt;/p&gt;

\[\bar{\mathbf{x}}_t \doteq \sum_a \pi (a | S_t) \mathbf{x} (S_t, a)\]

&lt;p&gt;또한 $\delta_t^a$는 다음과 같은 TD Error로 정의됩니다.&lt;/p&gt;

\[\delta_t^a \doteq R_{t+1} + \gamma_{t+1} \mathbf{w}_t^{\sf T} \bar{\mathbf{x}}_{t+1} - \mathbf{w}_t^{\sf T} \mathbf{x}_t\]

&lt;p&gt;$\mathbf{z}_t$는 식 (12.29)와 동일하게 정의되고, $\mathbf{v}_t$의 Update를 포함한 나머지는 GTD($\lambda$)와 동일합니다.&lt;/p&gt;

&lt;p&gt;세 번째로 &lt;span style=&quot;color:red&quot;&gt;HTD($\lambda$)&lt;/span&gt;는 GTD($\lambda$)와 TD($\lambda$)를 결합한 State-Value 알고리즘입니다. 이 알고리즘의 가장 큰 장점은 TD($\lambda$)를 Off-policy 학습으로 엄격하게 일반화한다는 것입니다. &lt;strong&gt;엄격하게&lt;/strong&gt; 라는 의미는 Behavior Policy가 Target Policy와 같게 되면 HTD($\lambda$)가 TD($\lambda$)와 동일하게 된다는 뜻입니다. (GTD($\lambda$)는 그렇게 되지 않습니다.) 보통 TD($\lambda$)가 GTD($\lambda$)보다 빠르게 수렴하기 때문에 이 장점은 매력적입니다. 또한 TD($\lambda$)는 단일 Step-size Parameter만 필요하다는 장점도 있습니다. HTD($\lambda$)는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_t + \alpha \delta_t^s \mathbf{z}_t + \alpha \big( (\mathbf{z}_t - \mathbf{z}_t^b)^{\sf T} \mathbf{v}_t \big) (\mathbf{x}_t - \gamma_{t+1} \mathbf{x}_{t+1}) \\ \\
\mathbf{v}_{t+1} &amp;amp; \doteq \mathbf{v}_t \beta \delta_t^s \mathbf{z}_t - \beta \Big( \mathbf{z}_t^{b^{\sf T}} \mathbf{v}_t \Big) (\mathbf{x}_t - \gamma_{t+1} \mathbf{x}_{t+1}) \quad \text{with} \quad \mathbf{v}_0 \doteq \mathbf{0} \\ \\
\mathbf{z}_t &amp;amp; \doteq \rho_t \big( \gamma_t \lambda_t \mathbf{z}_{t-1} + \mathbf{x}_t \big) \quad \text{with} \quad \mathbf{z}_{-1} \doteq \mathbf{0} \\ \\
\mathbf{z}_t^b &amp;amp; \doteq \gamma_t \lambda_t \mathbf{z}_{t-1}^b + \mathbf{x}_t \quad \text{with} \quad \mathbf{z}_{-1}^b \doteq \mathbf{0}
\end{align}\]

&lt;p&gt;위 식에서 $\beta &amp;gt; 0$은 두 번째 Step-size Parameter입니다. 또한 두 번째 Weight 집합인 $\mathbf{v}_t$ 외에도 HTD($\lambda$)는 두 번째 Eligibility Trace 집합인 $\mathbf{z}_t^b$가 있습니다. 이것들은 Behavior Policy에 대한 누적된 Eligibility Trace이며, 모든 $\rho_t$가 1이면 $\mathbf{w}_t$ Update의 마지막 항이 0이 되면서 $\mathbf{z}_t$와 같아집니다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;span style=&quot;color:red&quot;&gt;Emphatic TD($\lambda$)&lt;/span&gt;는 1-step Emphatic-TD 알고리즘을 Eligibility Trace로 확장한 것입니다. (Section 9.11과 11.8 참고) 결과적으로 이 알고리즘은 강력한 Off-policy 수렴을 보장하면서 어느 정도 Bootstrapping도 가능하게 하지만, 높은 Variance를 가지고 수렴 속도가 느리다는 단점이 있습니다. Emphatic TD($\lambda$)는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t \\ \\
\delta_t &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \mathbf{w}_t^{\sf T} \mathbf{x}_{t+1} - \mathbf{w}_t^{\sf T} \mathbf{x}_t \\ \\
\mathbf{z}_t &amp;amp; \doteq \rho_t \left( \gamma_t \lambda_t \mathbf{z}_{t-1} + M_t \mathbf{x}_t \right) \quad \text{with} \quad \mathbf{z}_{-1} \doteq \mathbf{0} \\ \\
M_t &amp;amp; \doteq \lambda_t I_t + \left( 1 - \lambda_t \right) F_t \\ \\
F_t &amp;amp; \doteq \rho_{t-1} \gamma_t F_{t-1} + I_t \quad \text{with} \quad F_0 \doteq i (S_0)
\end{align}\]

&lt;p&gt;위 식에서 $M_t \ge 0$은 &lt;strong&gt;Emphasis&lt;/strong&gt;의 일반적인 형태이고, $F_t \ge 0$은 &lt;span style=&quot;color:red&quot;&gt;Followon Trace&lt;/span&gt;라고 하며, $I_t \ge 0$은 Section 11.8에서 설명한 &lt;strong&gt;Interest&lt;/strong&gt;입니다. 이 알고리즘의 중요한 점은 $\delta_t$와 같이 $M_t$ 또한 공간 복잡도를 높이지 않는다는 것입니다. 이 식의 정의를 Eligibility Trace 식에 대입하여 처리가 가능합니다. Emphatic TD($\lambda$)의 True On-line 버전에 대한 Pseudocode 및 프로그램은 (Sutton, 2015b) 논문에서 확인이 가능합니다.&lt;/p&gt;

&lt;p&gt;On-policy의 경우 (즉, 모든 $t$에 대해 $\rho_t = 1$) Emphatic TD($\lambda$)는 기존 TD($\lambda$)와 유사하지만 차이점도 많습니다. 예를 들어, Emphatic TD($\lambda$)는 모든 State에 종속적인 $\lambda$ 함수에 대해 수렴이 보장되지만, TD($\lambda$)는 그렇지 않습니다. TD($\lambda$)는 모든 상수 $\lambda$에 대해서만 수렴이 보장됩니다. 이에 대한 반례는 (Ghiassian, Rafiee, and Sutton, 2016) 논문을 참고해주시기 바랍니다.&lt;/p&gt;

&lt;h2 id=&quot;implementation-issues&quot;&gt;Implementation Issues&lt;/h2&gt;

&lt;p&gt;처음에는 Eligibility Trace를 사용하는 Tabular 방법이 1-step 방법보다 복잡해 보일 수도 있습니다. 단순한 구현은 모든 State(또는 State-Action 쌍)가 모든 시간 단계에서 Estimated Value와 Eligibility Trace를 모두 Update해야 합니다. 이것은 단일 명령, 다중 데이터, 병렬 컴퓨터 또는 Artificial Neural Network에서 구현하는 경우 문제가 되지 않지만, 기존의 직렬 컴퓨터에서 구현하는 경우에는 문제가 될 수 있습니다. 다행히도 일반적인 $\lambda$와 $\gamma$에 대해 거의 모든 State에서의 Eligibility Trace는 항상 0에 가깝습니다. 최근에 방문한 State에서만 0보다 훨신 큰 Trace가 있을 것이기 때문에 이러한 몇 개의 State만 Update하여 구현함으로써 간단하게 알고리즘을 근사적으로 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;실제로 이러한 방법을 사용하면 기존 컴퓨터로도 0보다 훨씬 큰 일부 Trace만 Update함으로써 구현할 수 있습니다. 이러한 꼼수를 사용하면 Tabular 방법에서 Trace를 사용하는 계산 비용이 일반적인 1-step 방법의 몇 배에 불과합니다. 물론 정확한 배수는 $\lambda$ 및 $\gamma$의 값과 다른 계산 비용에 따라 달라집니다. Tabular 방법의 경우는 어떤 의미에서 Eligibility Trace의 최악의 계산 복잡도를 가지고 있습니다. Function Approximation을 사용할 때는 Function Approximation 방법 자체의 계산 복잡도가 높기 때문에 Eligibility Trace를 사용하지 않는 것과 크게 차이가 나지 않기 때문입니다. 예를 들어, Artificial Neural Network 및 Backpropagation Algorithm을 사용하는 경우 Eligibility Trace를 추가해도 각 단계별로 필요한 메모리나 계산량이 두 배 정도만 늘어납니다. Section 12.3의 Truncated $\lambda$-Return 방법은 항상 추가적인 메모리 용량이 필요하지만, 기존 컴퓨터에서 계산적으로 효율적인 구현이 가능하기도 합니다.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;TD Error를 사용하는 Eligibility Trace는 Monte Carlo와 TD 방법의 중간 지점을 선택할 수 있는 효율적이고 점진적인 방법을 제공합니다. 7장의 $n$-step TD 방법도 이것을 가능하게 했지만 Eligibility Trace 방법은 더 일반적이고 더 빨리 학습할 수도 있으며 Trade-off를 통해 다른 계산 복잡성을 가질 수도 있습니다. 이번 장에서는 On-policy와 Off-policy 학습에서 Variable Bootstrapping 및 Discounting을 위해 Eligibility Trace에 대한 새로운 이론을 제시했습니다. 이 이론의 하나로써 기존 TD 방법의 계산 복잡도를 유지한 채 이상적인 방법의 Action을 정확하게 재현하는 True On-line 방법이 있습니다. 또 다른 것으로는 직관적인 Forward View 방법에서 보다 계산적으로 효율적인 Backward View로 전환할 수 있는 가능성입니다. 이제 고전적이고 계산량이 많은 Monte Carlo 알고리즘으로 시작하여 True On-line TD 방법에 사용한 것과 동일한 Eligibility Trace를 사용하여 계산량이 적은 Incremental non-TD를 구현함으로써 이 일반적인 아이디어를 설명했습니다.&lt;/p&gt;

&lt;p&gt;5장에서 언급했듯이 Monte Carlo Method은 Bootstrap하지 않기 때문에 non-Markov Process에서 이점이 있을 수 있습니다. Eligibility Trace는 TD 방법을 Monte Carlo Method와 유사하게 만들기 때문에 이런 경우에도 이점을 가질 수 있습니다. 예를 들어, TD 방법의 장점으로 인해 이것을 사용하고 싶지만, 일부 작업이 non-Markov인 경우 Eligibility Trace를 도입함으로써 이 문제를 해결할 수 있습니다. Eligibility Trace는 장기간의 지연된 Reward와 non-Markov Process 모두에 대한 해결 방법을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;$\lambda$의 값을 조절하여 Monte Carlo에서 1-step TD 방법에 이르기까지 Eligibility Trace를 어디에나 사용할 수 있습니다. 그렇다면 어느 단계에서 사용하는 것이 가장 좋을까요? 안타깝게도 이 질문에 대한 명확한 이론적인 답이 없습니다. 대신 경험적인 답으로써, Episode당 단계가 많거나 Discounting이 반감기 내에 단계가 많은 작업에서 Eligibility Trace를 사용하는 것이 더 좋다고 판단됩니다. 아래의 그래프는 $\lambda$에 따른 강화학습의 성능을 나타내고 있는데, 이것을 통해 대략적인 답을 낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;반면에 순수한 Monte Carlo Method에 가까워지면 성능이 급격히 저하됩니다. 그렇기 때문에 적당히 중간 정도의 Step이 최선의 선택이라고 볼 수 있습니다. 미래에는 $\lambda$를 사용하여 TD와 Monte Carlo Method 간 Trade-off를 더 미세하게 조절하는 것이 가능할 수도 있겠지만, 현재로서는 이것을 어떻게 안정적이고 유용하게 사용할 수 있을지 명확한 결론을 내릴 수가 없습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 사용하게 되면 1-step 방법보다 더 많은 계산이 필요하지만, 그 대가로 Reward가 여러 단계로 지연되는 경우 훨씬 더 빠른 학습 속도를 제공합니다. 따라서 On-line과 같이 데이터가 부족하고 반복적으로 처리할 수 없는 경우에는 Eligibility Trace를 사용하는 것이 좋습니다. 반면에, 시뮬레이션을 통해 데이터를 쉽게 생성할 수 있는 Off-line의 경우에는 Eligibility Trace를 사용하는데 큰 이점이 없는 경우가 많습니다. 이 때의 목표는 제한된 데이터에서 더 많은 것을 얻는 것이 아니라 가능한 한 빠르게 많은 데이터를 처리하는 것인데, Eligibility Trace로 인한 데이터의 속도 향상은 그만한 계산 비용를 소모할 가치가 없기 때문에 1-step 방법이 선호됩니다.&lt;/p&gt;

&lt;p&gt;이번 장은 내용이 길고 어려워서 그런지 깔끔하게 포스트를 작성하지 못한 것 같네요. 포스트를 먼저 게시한 다음 나중에 다시 읽어보며 조금씩 매끄럽게 수정하겠습니다.&lt;/p&gt;

&lt;p&gt;다음 장은 강화학습의 마지막 장인 Policy Gradient Methods입니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장에서 새로 배우는 Eligibility Trace는 강화학습의 기본 메커니즘 중 하나입니다. 예를 들어, TD($\lambda$)에서 $\lambda$는 Eligibility Trace를 사용한다는 것을 의미합니다. Q-learning과 Sarsa를 포함한 대부분의 TD 방법은 Eligibility Trace와 결합하여 보다 효율적으로 학습할 수 있습니다.</summary></entry><entry><title type="html">JBL Quantum 810</title><link href="http://localhost:4000/unboxing/jbl-quantum-810/" rel="alternate" type="text/html" title="JBL Quantum 810" /><published>2022-07-26T00:00:00+09:00</published><updated>2022-07-26T00:00:00+09:00</updated><id>http://localhost:4000/unboxing/jbl-quantum-810</id><content type="html" xml:base="http://localhost:4000/unboxing/jbl-quantum-810/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;안녕하세요, 어쩌다보니 지난번에 이어서 또 헤드셋을 개봉하게 되었네요. (정확하게 말하면 지난번에는 헤드폰이었고 이번에는 헤드셋이라는 차이가 있습니다만)&lt;/p&gt;

&lt;p&gt;원래 저는 커세어 보이드 무선 제품을 쭉 사용해오고 있었습니다. 착용감이 편하기도 했고, 제가 막귀라 음질에 크게 구애받지 않았기 때문에 나름 만족하며 사용했습니다. 다만 구매한지 5년이나 지나다보니 쿠션 부분이 다 떨어지고, 가죽 부분이 벗겨지는 바람에 어쩔 수 없이 헤드셋을 새로 구매했습니다. 마이크를 많이 사용하는 것은 아니었지만 마이크 소리가 너무 작아지는 문제도 있었습니다.&lt;/p&gt;

&lt;p&gt;이번에 새로 헤드셋을 구매하면서 가장 염두에 둔 것은 노이즈 캔슬링 기능이었습니다. 이 헤드셋을 구매할 당시 컴퓨터 본체의 소음이 너무 심해서 헤드셋을 껴도 차단이 안될 정도였거든요. (그런데 결국 컴퓨터 본체도 지난 주에 바꾸고 말았습니다) 다만 게이밍 헤드셋 중에 노이즈 캔슬링 기능이 있는 헤드셋은 많지 않았습니다. 처음에는 그냥 에어팟 맥스를 살까 하다가 가격이 너무 비싸기도 했고, 게이밍에 쓰기에는 지연시간이 너무 크다는 소리가 많아 그냥 게이밍 제품 중에서 찾아보기로 했습니다.&lt;/p&gt;

&lt;p&gt;그러다가 마침 찾은 것이 JBL 사의 제품이었습니다. 원하던 노이즈 캔슬링 기능도 들어가 있었고, JBL은 하만의 자회사이기 때문에 삼성의 AS 서비스를 받을 수 있기 때문입니다. 삼성 서비스 센터를 몇 번 방문해보았는데, AS 만큼은 굉장히 만족스러웠습니다.&lt;/p&gt;

&lt;p&gt;JBL의 게이밍 헤드셋 브랜드는 JBL Quantum인데, 구매할 당시 가장 최신 제품은 810 이었습니다. 문제는 불행히도 한국에 810은 아직 정식 발매가 안된 상황이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아무래도 JBL 사는 전통적인 음향 회사의 이미지가 있다보니 게이밍 기기의 수요가 많지 않아 정발이 늦는 것 같았습니다. JBL Quantum 810이 2022년 2월에 출시된 것으로 아는데 아직까지 감감무소식이거든요. 하는 수 없이 810 제품을 구하기 위해 백방으로 알아본 결과 JBL 영국 지사에서 판매하고 있는 것을 보았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한국으로 직배송이 안된다는 단점은 있었으나 가격적인 면을 비교해봐도 전 버전인 800과 크게 차이나지 않았습니다. 당장 급한 물건은 아니었으니 배대지를 이용해 구매하기로 결정했습니다. 굳이 이렇게 귀찮은 짓을 하면서까지 구매한 이유는 810이 800과 음질 자체는 큰 차이가 없으나, 배터리 지속 시간이 2배 이상 늘어나고(14시간 -&amp;gt; 30시간) 무게가 50g 정도 더 가벼워졌기 때문입니다. 이 정도면 충분히 직구를 할만한 가치가 있다고 생각했습니다.&lt;/p&gt;

&lt;p&gt;배송 기간은 약 열흘 정도 걸렸습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;패키지 디자인은 게이밍 제품 답게 상당히 요란합니다. 전면에는 오디오 포멧이 주르륵 나열되어 있고, 액티브 노이즈 캔슬링과 같은 기능이 나열되어 있습니다. 또한 하단에는 어떤 제품과 연결이 가능한지 나와있는데, PC와 플레이스테이션은 무선으로 연결 가능하고 XBOX와 스위치는 유선으로 연결 가능하다고 나와 있습니다. 이왕 무선을 지원해 줄거면 다 지원해주지… 라는 생각이 드네요.&lt;/p&gt;

&lt;p&gt;후면에도 호환되는 프로그램이나 제품 특징이 나와 있습니다. RGB 기능을 끄면 최대 43시간까지 가동이 가능하다고 하네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;신기한 것은 한국에 정발된 제품이 아님에도 한국어로 설명이 나와있다는 것입니다. 아무래도 JBL의 모기업이 삼성인 것과 관련이 있어 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래쪽에 보시면 “재생과 충전을 동시에” 라는 문구가 있습니다. 이 기능이 당연한 것임에도 불구하고 나온 이유는 위 댓글에서 보이듯이 전작인 800에서 재생과 충전이 동시에 되지 않았기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;옆면 디자인은 딱히 눈에 띄는 점은 없습니다. 3D 서라운드 어쩌고 하는 문구가 있는데 저 기능이 뭔지 아직은 잘 모르겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하단에는 하만의 로고가 나와있습니다. JBL이 하만 소속이기 때문에 당연합니다. 다만 삼성의 로고는 제품 어디에서도 찾을 수 없었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;겉 포장을 벗기고 나면 이렇게 검은색 종이 박스가 나옵니다. 포장 자체는 딱히 다른 제품에 비해 좋다거나 특이하다는 생각이 들지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;열어보니 헤드셋이 이렇게 천으로 된 주머니에 담겨있었습니다. 이것 자체에 딱히 불만이 있지는 않았습니다만 헤드셋이 박스에 고정되어 있지 않다는 점이 더 불만스러웠습니다. 이렇게 넣어줄 거면 쿠션이라도 넣어주던가요. 그래도 나름 가격이 있는 물건인데 포장을 좀 정성들여 해줄 수 있는거 아닌가라는 생각이 드네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/12.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;헤드셋 본체 아래에는 이렇게 USB 케이블이나 수신기 같은 것이 담겨 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/13.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;USB 케이블은 Type-C to Type-C 입니다. 요즘 대부분 Type-C를 쓰다 보니 이해가 되네요. 그리고 2.4Ghz 수신기는 가운데에 고정되어 담겨 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/14.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/15.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;헤드셋을 보관할 때는 이런 식으로 쿠션 부분이 돌아갑니다. 문제는 저게 고정되지 않아서 자기 혼자 막 빙글빙글 도는 문제가 있네요. 이 점은 좀 실망스럽습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/16.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽에는 이렇게 마이크 설정과 헤드셋 음량을 조절할 수 있는 기능이 달려 있습니다. 여기에서 가장 이해가 안가는 기능은 마이크 음소거 기능입니다. 마이크를 그냥 위로 올리면 자동으로 음소거가 되는데 뭐하러 저 기능을 또 만들어놨을까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/17.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오른쪽에는 전원을 켜는 부분과 블루투스 페어링 버튼이 있습니다. 저 부분을 테이프로 고정한 이유는 박스 안에서 켜질 수도 있기 때문이 아닐까로 생각하고 있습니다. 헤드셋이 박스에 고정되어 있지 않아서 그럴 가능이 있겠다 싶거든요.&lt;/p&gt;

&lt;h2 id=&quot;간단-후기&quot;&gt;간단 후기&lt;/h2&gt;

&lt;p&gt;구매하고 약 한 달 정도 사용해봤습니다만, 결론적으로 썩 마음에 들지 않았습니다. 돈 값을 못한다고 해야할까요? 기본적으로 불편한 점이 너무 많았습니다.&lt;/p&gt;

&lt;p&gt;첫 번째는 전원을 켜는 기능은 있으나 끄는 기능이 없다는 것입니다. 진짜입니다. 전원을 끄는 기능이 없기 때문에 사용 후 알아서 꺼지기를 기다려야 합니다.&lt;/p&gt;

&lt;p&gt;두 번째로는 착용감이 생각보다 불편했습니다. 아니, 이전에 사용한 커세어 보이드가 너무 편했던 것일까요? 제품 후기에 착용감이 편하다는 의견이 많았는데, 이게 헤드셋 중에서는 그나마 편한 편일지도 모르겠습니다. 저는 비교군이 많지 않기 때문에 객관적인 의견은 아닙니다. 다만 커세어 보이드나 에어팟 맥스와 비교해보면 확실히 불편합니다.&lt;/p&gt;

&lt;p&gt;세 번째로는 그렇게 자랑하던 노이즈 캔슬링 기능입니다. 물론 제가 써본 노이즈 캔슬링 제품은 에어팟 프로/맥스 밖에 없기 때문에 감안하고 봐주시기 바랍니다. 에어팟 시리즈에 비해면 이 제품은 노이즈 캔슬링 기능이 있나 싶을 정도로 잘 체감이 되지 않습니다. 지난번 에어팟 맥스 개봉기에서 에어팟 맥스의 노이즈 캔슬링이 단순히 헤드 쿠션 때문이 아닌가? 라고 후기를 남겼는데, 이 말을 취소해야겠습니다. 에어팟 맥스의 노이즈 캔슬링은 정말 대단한 것이었습니다. 물론 가격이 두 배 넘게 차이나는 제품이긴 하지만 그래도 음향 전문 회사다보니 이 기능에 대해 기대를 상당히 많이 했었는데 실망스럽기만 하네요.&lt;/p&gt;

&lt;p&gt;차라리 가격이라도 저렴했으면 싼 맛에 쓴다고 자기위안이라도 할 수 있었겠지만 그것도 아니라 기분이 좋지 않네요. 차라리 커세어 보이드 신형이나 살껄 그랬습니다.&lt;/p&gt;

&lt;p&gt;물론 이 의견은 주관적이며, 제가 모르는 이 헤드셋의 장점이 있을 수도 있습니다. 특히 타 제품과 비교하여 JBL 헤드셋의 음향 차이를 체감하실 수 있는 분이라면 그 돈에 맞는 가치를 할 수 있을 것 같습니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">초속 5센티미터 감상평</title><link href="http://localhost:4000/talk/5-centimeters-per-second/" rel="alternate" type="text/html" title="초속 5센티미터 감상평" /><published>2022-06-17T00:00:00+09:00</published><updated>2022-06-17T00:00:00+09:00</updated><id>http://localhost:4000/talk/5-centimeters-per-second</id><content type="html" xml:base="http://localhost:4000/talk/5-centimeters-per-second/">&lt;p&gt;지난 포스트에 이어 제출한 두 번째 과제입니다. 역시 기록을 위해 포스팅합니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;(주의) 마법소녀 마도카 마기카의 결말에 대한 스포일러가 포함되어 있습니다.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;과제-내용&quot;&gt;과제 내용&lt;/h2&gt;

&lt;p&gt;교재 ‘애니메이션에 빠진 인문학’의 내용과 7주차~11주차 수업 내용을 참고하여 작성하기 바랍니다.&lt;/p&gt;

&lt;p&gt;현대인의 개념, 속성, 가치관, 상황 등을 이해하기 쉽다고 판단되는 애니메이션 작품을 하나 선택하고(교재와 수업에 소개된 작품으로 한정함),&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;해당 작품에서 현대인의 어떤 점이 잘 드러났는지 구체적인 예를 들어 설명하고,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;작품에 드러난 그런 현대인의 일면이 실제 현대사회에서 어떤 의미(영향)를 가지는지 서술한 뒤,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그런 의미나 영향에 대하여 자신은 어떻게 생각하는지 명확하고 구체적으로 논하시오.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;과제-애니메이션-목록&quot;&gt;과제 애니메이션 목록&lt;/h2&gt;

&lt;p&gt;7주차 : 현대인의 특징을 나타낸 애니메이션 - 천원돌파 그렌라간, 원피스&lt;/p&gt;

&lt;p&gt;8주차 : 현대사회의 문제점을 나타내는 애니메이션 - 강철의 연금술사, 충사, 진격의 거인&lt;/p&gt;

&lt;p&gt;9주차 : 미야자키 하야오 애니메이션 - 센과 치히로의 행방불명, 벼랑 위의 포뇨, 마루 밑 아리에티&lt;/p&gt;

&lt;p&gt;10주차 : 신카이 마코토 애니메이션 - 그녀와 그녀의 고양이, 누군가의 시선, 초속 5cm, 너의 이름은&lt;/p&gt;

&lt;p&gt;11주차 : 호쇼다 마모루 애니메이션 - 시간을 달리는 소녀, 늑대 아이, 괴물의 아이, 미래의 미라이&lt;/p&gt;

&lt;p&gt;저는 이 중 10주차의 초속 5cm를 선택하여 작성하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;제출한-과제&quot;&gt;제출한 과제&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;제목 : 추억은 정말 아름다운가&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“옛날이 좋았다”라는 말은 전 세계적으로, 세대에 상관없이 자주하는 말 중 하나이다. 이것은 심리학 용어로 Good-Old-Days bias (좋았던 옛날 편향)으로 부르며, 실제로 이 현상에 대해 발표한 논문이 존재한다 [1]. 하지만 “편향”이라는 단어에서부터 알 수 있듯이, 실제로 과거가 현재보다 좋은 것이 아니라 단지 추억에 그리움과 같은 보정이 들어가 있을 뿐이다.&lt;/p&gt;

&lt;p&gt;신카이 마코토의 《초속 5cm》는 바로 이러한 현대인의 향수를 자극한 애니메이션이다. 《초속 5cm》는 토오노 타카키라는 남자의 입장에서 바라본 추억을 나타내고 있다. 《초속 5cm》는 총 3부로 나뉘어져 있는데, 각각 유년기, 소년기, 그리고 청년기의 토오노 타카키를 보여준다. 이 영화 내내 토오노 타카키에게 중요하게 묘사되는 인물은 유년기에 만난 시노하라 아카리라는 여자아이다. 그러나 1부에서 두 사람은 이사로 인해 헤어지게 되고, 편지로 안부를 전하는 사이가 된다. 소년기를 다룬 2부에서도 토오노 타카키는 시노하라 아카리를 잊지 못하고 계속 연락하며, 3부에서는 다른 여자와 연애를 했다는 것이 묘사되지만 가슴 한켠에는 아직도 시노하라 아카리를 잊지 못하는 모습을 보여준다.&lt;/p&gt;

&lt;p&gt;그러나 이 애니메이션에서 묘사되는 토오노 타카키와 시노하라 아카리의 관계는 따지고 보면 그렇게 대단한 관계가 아니었음을 알 수 있다. 어린 시절 친하게 지낸 관계이기는 하나, 두 사람은 거리 문제로 인해 같이 지낸 시간은 잠깐뿐이기 때문이다. 나를 포함한 많은 현대인에게도 이렇게 누구나 한번쯤은 곱씹어볼만한 유소년기의 추억이 하나쯤은 있지만, 결국 나중에 생각해보면 사실 그것이 엄청 대단한 것은 아니었음을 알 수 있다. 그러나 많은 사람들이 이렇게 보정받은 추억에 매몰되어 정작 중요한 현재에 집중하지 못한다 [2]. 또는 현재의 힘든 상황에 무기력함을 느끼고 도피하려고 한다. 《초속 5cm》의 토오노 타카키 또한 마음속에 남아있는 시노하라 아카리의 존재로 인해 현재의 애인에게 집중하지 못하고, 무력감에 빠져 회사마저 퇴직하고 만다.&lt;/p&gt;

&lt;p&gt;물론 이러한 것들을 알고 있다고 해도 의식적으로 극복하는 것은 쉽지 않다. 머릿속으로는 현재 하고 있는 일에 집중해야 한다고 생각하지만, 정작 몸은 힘든 현실에 쉽게 무력감에 빠지고 만다. 나는 이럴 때 옛날이 정말 좋았는지를 진지하게 따져보곤 한다. 예를 들어, 스마트폰으로 인해 청소년, 청년들이 고립되어 외부와 단절된다는 뉴스 기사가 자주 보인다 [3]. 이것을 보고 스마트폰이 없던 옛날이 좋았다라고 말하는 사람도 종종 보인다. 하지만 스마트폰이 없던 시절과 현재의 소통은 방식만 달라졌을 뿐 큰 차이가 없다. 크게 외향적인 몇몇 사람은 눈에 띄겠지만, 그렇지 않은 대부분의 사람은 그 때도 조용히 몇몇 사람들과만 소통하였고, 지금은 그 수단이 SNS나 인터넷 커뮤니티와 같은 전자적인 수단으로 바뀌었을 뿐이다. 오히려 스마트폰이 없다면 간편 결제나 정보 검색 등을 할 수 없어 불편한 점만 생길 뿐이다.&lt;/p&gt;

&lt;p&gt;최근 각지에서 레트로 아이템이 유행하고 있다 [4] [5]. 이런 마케팅 열풍이 분다는 것은 그만큼 과거의 추억을 그리워하는 사람들이 많다는 뜻일 것이다. 적절한 향수는 미래로 나아가는 동력이 될 수 있지만, 그것에 얶매이게 되면 오히려 장애물이 될 수 있음을 상기하며 글을 마친다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고문헌&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] Eibach, R. P., &amp;amp; Libby, L. K. (2009). Ideology of the good old days: Exaggerated perceptions of moral decline and conservative politics. Social and psychological bases of ideology and system justification, pp. 402-423.&lt;/p&gt;

&lt;p&gt;[2] 박병률, “[영화 속 경제]좋은 기억만 떠올리는 ‘므두셀라 증후군’”, 경향신문, 2020.03.02.&lt;/p&gt;

&lt;p&gt;[3] 고혜지, 송수연, “하루 10시간, 스마트폰 세상에 갇혀 어느새 ‘학포자’… 게임 캐릭터 친구뿐”, 서울신문, 2021.02.17.&lt;/p&gt;

&lt;p&gt;[4] 신미진, “”내가 산 건 ‘추억’이야”…레트로 아이템에 신난 어른들”, 서울경제, 2022.05.28.&lt;/p&gt;

&lt;p&gt;[5] 신원선, “MZ세대는 왜 레트로에 열광할까… 韓 사회 현상 진단”, 메트로신문, 2022.05.29&lt;/p&gt;

&lt;h2 id=&quot;교수님의-피드백&quot;&gt;교수님의 피드백&lt;/h2&gt;

&lt;p&gt;우선 전근대나 근대인과 다른 현대인의 특징 관련해서 특화하여 서술하지 않은 것이 다음의 논의가 현대인에 대한 것이라기보다는 특정 작품과 그에 대한 느낌 정도로 받아들여질 우려가 있습니다. 현대인이 아니라 다른 시대를 살았던 사람들에게도 받아들여질 수 있는 과거 지향 혹은 향수 등은 현대인을 키워드로 하는 과제의 성격이나 내용으로 볼 때 다소 거리가 있게 느껴집니다. 만일 그것이 현대인의 특질이나 현상, 문제와 연결시키려고 한다면 다른 논거를 예로 들어 현대사회에서 그러한 과거지향이 다른 시대에 비해 두드러진다던가 혹은 사회적 이슈가 되어 문제의 소지가 있다던가 하는 연결 고리를 만들어줄 필요가 있겠습니다.&lt;/p&gt;

&lt;p&gt;사실 신카이 마코토 애니메이션에 보이는 다소 감성적인 측면과 과거의 소환 등은 해석에 있어 이론의 여지가 있습니다. 단순히 과거를 회상하고 그 시절에 비교하여 현재 잃어버린 것을 안타까워하는 느낌으로 해석하거나 서술하신 것처럼 과거 지향적인 시각으로 평가하기도 합니다. 그런데 신카이 마코토의 전체 작품을 보면 과거를 소환하되 언제나 현재를 중심에 놓고 있다고 생각되기도 합니다. 즉 과거를 추억거리나 감상의 소재가 아니라 현재를 파악하고 현재의 소중함을 얘기하고자 하는 것은 아닐까 생각되기도 하는 것입니다.&lt;/p&gt;

&lt;p&gt;과제는 요구하는 것이 어느 정도 틀에 규정되기 쉽습니다. 평가 때문에 그렇습니다. 그런 틀 속에서 보았을 때의 과제에 대한 코멘트였다는 점을 말씀 드립니다. 제출하신 과제는 그런 틀이 아니라면 정돈되고 깔끔하게 정리된 서술이라고 생각됩니다. 한 학기 열심히 참여해 주시고 과제도 제출해 주신 점 고맙고, 응원합니다. 수고하셨습니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="talk" /><summary type="html">지난 포스트에 이어 제출한 두 번째 과제입니다. 역시 기록을 위해 포스팅합니다.</summary></entry><entry><title type="html">마법소녀 마도카☆마기카 감상평</title><link href="http://localhost:4000/talk/puella-magi-madoka-magica/" rel="alternate" type="text/html" title="마법소녀 마도카☆마기카 감상평" /><published>2022-06-13T00:00:00+09:00</published><updated>2022-06-13T00:00:00+09:00</updated><id>http://localhost:4000/talk/puella-magi-madoka-magica</id><content type="html" xml:base="http://localhost:4000/talk/puella-magi-madoka-magica/">&lt;p&gt;이번 학기에 일본 애니메이션과 관련한 강의를 청강하면서 제출한 과제입니다. 대단한 글은 아니지만, 기록을 위해 포스트로 남깁니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;(주의) 마법소녀 마도카 마기카의 결말에 대한 스포일러가 포함되어 있습니다.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;과제-내용&quot;&gt;과제 내용&lt;/h2&gt;

&lt;p&gt;2주차~6주차까지 수업에서 소개된 일본 애니메이션 장르 중 하나를 선택하고, 해당 장르가 현대인 및 현대사회를 잘 반영하고 있다고 생각하는 근거를 구체적인 작품(교재와 수업에 소개된 작품에 한정함)을 예로 들면서 서술하고, 그렇게 반영되어 있다고 생각하는 현대인 혹은 현대사회의 일면에 대한 자신의 생각((애니메이션에 대한 생각이 아님)을 명확하고 구체적으로 논하시오.&lt;/p&gt;

&lt;h2 id=&quot;과제-애니메이션-목록&quot;&gt;과제 애니메이션 목록&lt;/h2&gt;

&lt;p&gt;2주차 : 로봇 애니메이션 - 철완 아톰, 기동전사 건담, 신세기 에반게리온, 공각기동대&lt;/p&gt;

&lt;p&gt;3주차 : 스포츠 애니메이션 - 더 파이팅, 뱀부 블레이드, 원아웃, 자이언트 킬링&lt;/p&gt;

&lt;p&gt;4주차 : 마법소녀 애니메이션 - 마법소녀 마도카☆마기카, 마녀 배달부 키키, 메리와 마녀의 꽃&lt;/p&gt;

&lt;p&gt;5주차 : 영어덜트 애니메이션 - 공의 경계, 카이트 리버레이터, 성인 여성의 아니메 타임, 아인&lt;/p&gt;

&lt;p&gt;6주차 : 일상물 애니메이션 - 럭키스타, 역시 내 청춘 러브코미디는 잘못됐다, 남자 고교생의 일상&lt;/p&gt;

&lt;p&gt;저는 이 중 4주차의 마법소녀 마도카☆마기카를 선택하여 작성했습니다.&lt;/p&gt;

&lt;h2 id=&quot;제출한-과제&quot;&gt;제출한 과제&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;제목 : 현대인의 희생에 대한 인식의 변화&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;희생(犧牲)은 과거부터 현재까지 숭고한 것으로 여겨왔다. 나라를 위해 목숨을 바친 희생, 사랑하는 사람을 지키기 위한 희생, 또는 위험한 상황에 빠진 모르는 사람을 구하기 위한 희생 등 각각 이유는 다르지만, 희생에 대한 기록은 어디에서나 쉽게 찾아볼 수 있다. 게다가 그러한 희생을 영웅시하는 것은 현실에서도 물론이고, 창작물에서도 찾아보기 어렵지 않다.&lt;/p&gt;

&lt;p&gt;하지만 현대에 들어서 희생에 대한 인식이 변화하기 시작했다. 책 《애니메이션에 빠진 인문학》에 따르면, 현대인은 중세인이나 근대인에 비해 “개인”에 대한 인식이 확고하기 때문에 국가나 인류를 위해 자신을 희생하는 것을 원치 않는다고 한다. 이를 확연하게 보여줄 수 있는 현실의 예시가 바로 군대에 대한 인식의 변화이다. 불과 20년 전만 해도 한국에서 군대는 남자로 태어났으면 당연히 가야 하는 것으로 생각하였지만, 현대를 살아가는 젊은 세대들은 “군대는 뺄 수 있으면 무조건 빼는 것이 좋다”라는 생각으로 바뀌어버렸다. 심지어는 “전쟁이 나도 나라를 위해 싸우다가 죽는 것은 바보 같은 짓이고, 나는 해외로 도망칠 것이다”라는 의견을 공개적으로 표현해도 그를 비난하는 사람이 많지 않다. 이로 인한 원인은 복합적이지만, 타인을 위해 자신을 희생하는 것이 회의적으로 변하고 있는 것은 사실이다.&lt;/p&gt;

&lt;p&gt;이러한 희생에 대한 의식의 변화는 애니메이션에서도 드러난다. 1988년에 개봉한 &amp;lt;기동전사 건담 : 역습의 샤아&amp;gt;에서는 지구로 낙하하는 엑시즈를 막기 위해 아무로 레이가 홀로 모빌슈트에 탑승하여 손으로 막는다. 그 모습을 보고 다른 사람들도 모빌슈트에 탑승해 아무로 레이에게 동참하는데, 수많은 사람들의 의지가 합쳐 사이코 필드가 펼쳐침으로써 엑시즈의 추락을 저지하게 된다. 이 과정에서 아무로 레이는 결국 사망하고, 후속작 &amp;lt;기동전사 건담 : UC&amp;gt;에서 아무로 레이는 영웅으로 추서되었다고 표현된다. 이는 희생에 대한 숭고함이 남아있던 근대인의 생각이 반영되었다고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;하지만 2011년에 방영한 &amp;lt;마법소녀 마도카☆마기카&amp;gt;에서는 희생의 대해 매우 부정적으로 묘사한다. &amp;lt;마법소녀 마도카☆마기카&amp;gt;에서 묘사되는 마법소녀는 마녀와 싸울 수 있는 특별한 힘을 얻는 대신 본체라고 할 수 있는 소울 젬을 계속 정화해주어야 하는데, 소울 젬을 정화할 수 있는 그리프 시드를 얻기 위해서는 마녀를 죽여야 한다. 소울 젬을 정화하지 못하면 마법소녀는 마녀가 된다. 즉, 마법소녀의 미래는 마녀와 싸우다 죽거나, 마녀로 타락하는 것 밖에 없다. 이 가련한 운명을 대가로 마법소녀는 원하는 소원을 한 가지 이룰 수 있다.&lt;/p&gt;

&lt;p&gt;&amp;lt;마법소녀 마도카☆마기카&amp;gt;에 등장하는 5명의 마법소녀는 모두 각자의 이유로 희생을 하지만, 작품 내에서 그 희생이 보답 받지 못한다.&lt;/p&gt;

&lt;p&gt;미키 사야카는 좋아하는 남학생의 팔을 치료해달라는 소원을 빌었으나, 그 남학생은 사야카에게 호감이 있다는 묘사가 나오지 않는다. 오히려 사야카의 친한 친구가 그 남학생과 이어지는 듯한 묘사가 나오며, 결국 미키 사야카는 이로 인해 절망을 느끼고 마녀로 변하고 만다.&lt;/p&gt;

&lt;p&gt;사쿠라 쿄코는 성직자인 아버지의 목소리를 다른 사람들이 귀기울여달라는 소원을 빌었으나, 아버지는 자신의 목소리를 듣는 이유가 마법 때문임을 알고 오히려 절망하고, 가족들을 모두 죽인 후 자살하고 만다. 나중에 마녀가 된 미키 사야카를 다시 마법소녀로 되돌리려 하다 실패하고 본인마저 희생하였으나, 그 후 큐베로부터 사쿠라 쿄코의 시도는 애초에 무의미하였다는 확실한 묘사가 나온다.&lt;/p&gt;

&lt;p&gt;토모에 마미는 작중에서 나온 사쿠라 쿄코처럼 이기적으로 행동할 수도 있었으나, 유일하게 자신이 살기 위해 소원을 사용한 후, 마법소녀로써의 의무를 자각하고 미타키하라 시의 시민들을 지키기 위해 싸웠다. 그러나 이를 알고 있는 사람도, 같이 싸우는 동료조차 없었고, 마법소녀 후보인 마도카와 사야카를 만났지만 결국에는 혼자 싸우다 외롭게 죽고 만다.&lt;/p&gt;

&lt;p&gt;아케미 호무라는 유일한 친구인 마도카의 운명을 바꾸기 위해 몇 번이고 시간을 되돌렸다. 시간을 되돌려도 호무라 자신은 계속 기억이 남기 때문에 계속 마도카가 죽는 운명을 보면서 마음이 마모되었으나, 마도카를 구하겠다는 일념 하나로 버텼다. 하지만 호무라가 아무리 시간을 되돌려도 마도카를 구할 수는 없었고, 그저 마도카의 인과율만 늘리는 결과가 되어버리고 만다.&lt;/p&gt;

&lt;p&gt;카나메 마도카의 경우에는 조금 다르게 보일 수도 있다. 마지막화에서 카나마 마도카의 희생으로써 마녀의 존재를 없애는 장면은 유일하게 긍정적으로 보일 수 있는 희생이었지만, 마도카의 희생은 애초부터 선택의 여지가 없었다. 마도카가 희생하지 않는다면 발푸르기스의 밤을 막지 못해 마도카와 호무라는 물론이고 가족들을 포함해 수많은 사람들이 죽었을 가능성이 높기 때문이다. 게다가 후속작인 &lt;반역의 이야기=&quot;&quot;&gt;를 토대로 마도카가 정말로 원했던 것은 가족, 친구, 그리고 소중한 사람들과 헤어지고 싶지 않다는 대사를 통해 마도카가 정말로 원하던 희생이 아니었다는 결론을 도출할 수 있다. 즉, 본인이 의지대로 희생을 한 것이 아니라 희생을 당한 것에 가깝다.&lt;/반역의&gt;&lt;/p&gt;

&lt;p&gt;제작자가 &amp;lt;마법소녀 마도카☆마기카&amp;gt;를 통해 의도하고 싶은 것은 한 가지가 아니겠지만, 마법소녀들의 이러한 작중 행적을 볼 때 희생에 대한 기존의 인식을 부정했다는 것만은 분명하다.&lt;/p&gt;

&lt;p&gt;국어사전에 따르면, 희생의 사전적 의미는 다른 사람이나 어떤 목적을 위하여 자신의 목숨, 재산, 명예, 이익 따위를 바치거나 버리는 것이다. 그러나 또 다른 의미로 사고나 자연재해 따위로 애석하게 목숨을 잃는다는 뜻도 있다. 현대인이 희생에 대해 점점 부정적으로 생각하는 것을 볼 때, 과거의 사람들이 희생을 첫 번째 의미로 인식했다면, 현대인은 점점 두 번째 의미로 인식하는 단계라고 나는 생각한다.&lt;/p&gt;

&lt;h2 id=&quot;교수님의-피드백&quot;&gt;교수님의 피드백&lt;/h2&gt;

&lt;p&gt;희생을 키워드로 &lt;마법소녀 마도카=&quot;&quot; 마기카=&quot;&quot;&gt;를 분석한 글입니다. 장르로서의 마법소녀물이 현대사회와 가지는 연결성을 먼저 서술하는 것이 좋습니다. 키워드 설명 후에 일반적인 혹은 전체적인 마법소녀물에 희생이라는 키워드가 통하는 것이라는 것을 설명해야 이후 글이 설득력이 있습니다. 이 글은 장르에 대한 것보다는 특정 작품 분석에 가깝습니다. 또한 현대사회에서 희생에 대한 인식이나 가치관이 바뀌었다는 것을 주장할 수 있는 근거 제시가 다소 불충분합니다. 여론조사나 실제 사건 등의 지표가 없이는 설득력을 높일 수 없습니다.  마지막으로 자신의 의견 제시를 보다 분명하게 하고 그 주장의 근거나 논리를 보다 명확하게 할 필요가 있습니다. 대부분 의견을 진술하라고 하면 일반적인 혹은 선언적인 되풀이가 되는 경우가 많습니다. 하지만 그것은 너무 포괄적이면서 추상적인 것으로 구체성이 떨어질 우려가 많습니다. 개인적 차원에서 나의 행동반경과 현실적인 카테고리 내에서 실천할 수 있고 해결 가능한 것을 찾아 생각을 정리하고 의견으로 주장하는 것이 보다 긍정적이라고 생각합니다.&lt;/마법소녀&gt;&lt;/p&gt;

&lt;p&gt;이상 제출하신 과제 글에 대한 코멘트입니다. 전체적으로 정돈된 문장과 구성으로 이해하기 쉬운 논리적 연결성을 가지고 있다고 생각합니다. 위의 코멘트를 참고하시면 보다 설득력 있고 풍부한 내용의 글이 가능하리라 생각됩니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="talk" /><summary type="html">이번 학기에 일본 애니메이션과 관련한 강의를 청강하면서 제출한 과제입니다. 대단한 글은 아니지만, 기록을 위해 포스트로 남깁니다.</summary></entry></feed>