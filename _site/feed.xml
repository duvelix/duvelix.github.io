<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-04-12T11:41:22+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">KEEPMIND</title><subtitle>A place I record so that I don&apos;t forget.</subtitle><author><name>Joonsu Ryu</name></author><entry><title type="html">포켓몬스터 스칼렛/바이올렛 더블팩</title><link href="http://localhost:4000/Unboxing/pokemon-scarlet-violet-double-pack/" rel="alternate" type="text/html" title="포켓몬스터 스칼렛/바이올렛 더블팩" /><published>2022-12-09T00:00:00+09:00</published><updated>2022-12-09T00:00:00+09:00</updated><id>http://localhost:4000/Unboxing/pokemon-scarlet-violet-double-pack</id><content type="html" xml:base="http://localhost:4000/Unboxing/pokemon-scarlet-violet-double-pack/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그동안 바빠서 게임을 못하고 있었는데, 어느새 소드/실드가 발매된지 3년이나 지났네요. 그런데 저는 아직도 소드/실드를 못했습니다. 해야지 해야지 하면서도 계속 미루다보니 결국 이렇게 되었네요. 하지만 그렇다고 포켓몬 신작이 발매되었는데 그냥 넘어갈 수는 없죠. 그래서 이번에도 스칼렛/바이올렛 더블팩을 구매했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/01.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저는 분명히 예약 구매를 했지만, 소드/실드 때처럼 예약 구매 특전 같은 것은 따로 오지 않았습니다. 택배 상자를 열어보니 그냥 딱 이렇게 더블팩 박스만 하나 달랑 들어있네요. 점점 닌텐도가 배짱 장사를 하고 있나 하는 생각이 듭니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/02.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒷면에는 게임에 대한 내용이 간단하게 정리되어 있습니다. 어차피 그 나물에 그 밥일테지만, 그럼에도 불구하고 어느 정도의 재미는 보장한다는 것은 인정할 수밖에 없습니다. 예전부터 포켓몬을 제대로 하려면 기기가 2대 필요했는데, 이거 때문에 스위치 라이트를 하나 살까 고민중입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;옆면에는 큼지막한 글씨로 포켓몬스터 스칼렛 바이올렛 더블팩 이라는 글자가 적혀 있습니다. 소드/실드 버전에서도 있었던 것 같은데, 뭔가 묘하게 싼티가 나는 느낌을 지울 수가 없네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;바닥면에는 한국 국내 전용 패키지라는 말과 함께 스타팅 포켓몬 3마리가 나와 있습니다. 이름은 아직 모릅니다만, 풀 속성은 어차피 하드 모드겠죠? 불 속성과 물 속성 중 취향에 맞는 포켓몬을 고르면 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구성품으로는 스칼렛/바이올렛 게임이 각각 들어있고, 더블팩 특전으로 몬스터볼 100개가 담긴 코드 2개가 들어있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/06.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;특전 코드를 확대해 보았습니다. 게임 초반에는 몬스터볼 한 개 구매하는 것도 벅차기 때문에 유용하긴 합니다만, 일회성 코드라는게 조금 아쉽습니다. 일반적인 콘솔 게임 특전처럼 새 게임을 시작하도 얻을 수 있도록 만들어줬으면 더 좋았을텐데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임 패키지 뒷면입니다. 왼쪽이 스칼렛, 오른쪽이 바이올렛입니다. 전체적인 디자인은 동일하나, 자세히 보시면 주인공의 의상이 게임 이름에 맞게 변하는 것을 알 수 있습니다. 스칼렛은 붉은색 느낌, 바이올렛은 보라색 느낌입니다. 다시 보니 맨 위에도 다른 그림이 삽입되어 있네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;케이스 내부 디자인은 영 그렇습니다. 예전에 올린 소드/실드 개봉기에서 내부 디자인을 혹평했었는데요, 다시 보니 그게 선녀라고 생각될 정도네요. 그 때는 그래도 대충 포켓몬 느낌은 났었는데, 이번에는 영… 스칼렛/바이올렛 모두 내부 디자인은 동일합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/069/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;패키지의 컨셉대로 스칼렛은 붉은색, 바이올렛은 보라색으로 디자인되어 있습니다. 한국용 제품이다보니 당연히 일련번호도 KOR이 붙어 있습니다.&lt;/p&gt;

&lt;p&gt;사실 포켓몬 게임을 살 때마다 느끼는 것이긴 합니다만, 멀쩡한 게임 한 개를 뜯어서 2개로 나눠파는 상술은 참 악질이라는 생각이 듭니다. 각각의 가격은 다른 패키지와 동일하게 받아먹는데 말이죠. 특히 최근들어 포켓몬 시리즈의 완성도에 대해 계속 불만이 나오는 시점에서는 더욱 돈이 아깝게 느껴집니다. 게다가 이번 작품은 출시 첫날부터 버그에 대한 이슈가 터져나왔죠. 반 년 정도 지나면 버그 이슈도 어느 정도 해결될 것 같으니, 그 때가 되면 슬슬 플레이해볼 생각입니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">마법소녀 마도카☆마기카 굿즈</title><link href="http://localhost:4000/Unboxing/puella-magi-madoka-magica-goods/" rel="alternate" type="text/html" title="마법소녀 마도카☆마기카 굿즈" /><published>2022-10-22T00:00:00+09:00</published><updated>2022-10-22T00:00:00+09:00</updated><id>http://localhost:4000/Unboxing/puella-magi-madoka-magica-goods</id><content type="html" xml:base="http://localhost:4000/Unboxing/puella-magi-madoka-magica-goods/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예전에 마법소녀 마도카 마기카를 감명깊게 봤기 때문에 요즘 관련 굿즈를 모으고 있습니다. 피규어도 구매했지만 12월에나 전부 도착할 것 같아서, 먼저 도착한 물품들만 후기를 작성해보도록 하겠습니다. 오늘 소개할 관련 굿즈는 블루레이와 PSP, Vita 게임입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/01.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;굿즈를 모아놓고 찍어본 사진입니다. 오른쪽부터 Vita 게임, PSP 게임, 그리고 블루레입니다. PSP 게임이 유난히 커 보이는데, 그 이유는 나중에 말씀드리겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;블루레이&quot;&gt;블루레이&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/02.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/06.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 블루레이입니다. 저는 감명깊게 본 작품들은 대부분 블루레이로 소장하는데, 마마마는 나온지 오래 된 애니메이션이다보니 구하기 어려웠습니다. 특히나 한국은 블루레이 시장이 작아 초반을 놓치면 구하기 어렵습니다. 저는 다행히 알라딘에서 미개봉 중고로 구매하였습니다. 보통 알라딘에서 파는 미개봉 중고품은 원래 가격보다 웃돈을 주고 구매해야 하지만, 의외로 인기가 별로 없는지 저는 오히려 더 저렴한 가격으로 구매했습니다. 원가는 66,000원이지만, 저는 그보다 약간 싼 58000원으로 구매했습니다. 현재도 비슷한 가격으로 판매되는 것으로 보아 정말 잘 안팔리나 봅니다. 아마 한국어 더빙이 없기 때문이 아닐까 생각하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;블루레이는 총 3개로 구성되어 있습니다. 왼쪽부터 전반부, 중반부, 후반부인데요, 각 에피소드 별 핵심 인물들이 나타나 있는 것 같습니다. 전반부의 핵심 인물인 토모에 마미가 빠진 것이 의외네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 전반부 블루레이입니다. 토모에 마미가 왜 없나 했더니 내부에 있었네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/12.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/13.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/14.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/15.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;중반부 블루레이입니다. 중반부 스토리의 핵심은 미키 사야카와 사쿠라 쿄코였기 때문에 잘 구성된 디자인이라고 생각합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/16.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/17.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/18.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/19.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;후반부 블루레이입니다. 스포일러가 될 수 있으니 간단하게만 말하면 아케미 호무라와 얼티밋 마도카가 나와있습니다. 내부 디자인은 마도카로 인해 구원받은 마법소녀를 나타낸 것 같네요.&lt;/p&gt;

&lt;h2 id=&quot;배틀-펜타그램-vita-게임&quot;&gt;배틀 펜타그램 (VITA 게임)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/20.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/21.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/22.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음으로는 비타 게임입니다. 정식 이름은 The Battle Pentagram인데요, 극장판 애니메이션을 기준으로 만들어진 작품입니다. 극장판을 기준으로 했다지만 어차피 극장편도 총집편이었기 때문에 크게 다를 것은 없습니다. 다만 스토리는 독자적인 노선을 타는 것으로 알고 있습니다.&lt;/p&gt;

&lt;p&gt;이 게임은 일반판과 한정판, 두 종류로 발매되었는데, 저는 한정판을 구매했습니다. 일반판은 아직도 세 재품을 구할 수 있었고, 한정판은 중고밖에 없었지만 가격 차이가 그다지 크지 않았기 때문입니다. 게다가 한정판 특전을 보니 한정판을 사는게 낫겠다는 생각이 들었습니다. 왜 그런지는 구성품을 하나하나 소개하면서 말씀드리겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/23.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저는 일본 아마존에서 구매했는데, 상태는 Like New 라고 나와있었으며, 가격은 6080엔이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/24.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/25.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반판은 게임 본편 하나만 들어있지만, 한정판은 위와 같이 특별한 포장 안에 게임이 들어있는 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/26.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/27.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일본 아마존의 장점은 Like New 라고 분류된 제품이 진짜 새 것이나 마찬가지라는 점입니다.&lt;/p&gt;

&lt;p&gt;게임 본편은 아예 뜯어보지도 않은 상태였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/28.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임 본편을 뜯어보니 한정판 초회 특전으로 토모에 마미와 이야기할 수 있는 전화번호가 들어있었습니다. 전화번호는 혹시 몰라서 가리긴 했는데, 전화해보니 사용할 수 없는 번호라고 하네요. 저것도 기간 제한이 있었나 봅니다. 9년전 게임이라 이해할 만 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/29.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임 칩은 일반적인 비타 게임과 동일합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/30.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 한정판 구성품입니다. 왼쪽에 있는건 캐릭터 카드, 그리고 아케미 호무라와 이야기할 수 있는 전화번호(물론 안되겠죠?), 한정판 특전 코스튬이 있습니다. 아마 중고라 코스튬 코드는 사용 불가능할 것 같습니다. 사용하지 않았더라도 워낙 발매된 지 오래된 게임이라 유효 기간이 지났을 것 같네요. 사실 이런건 별로 중요하지 않고, 한정판의 존재 가치는 바로 오른쪽 물건입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/31.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/32.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오른쪽의 내용은 오리지널 사운드 트랙이 담긴 CD와 일러스트집입니다. 특히 일러스트 집은 게임을 플레이하기 전과 게임을 플레이하고 난 후의 느낌이 다르기 때문에 좋아하는 특전입니다. 하나 아쉬운 것은 페이지 수가 생각보다 적네요.&lt;/p&gt;

&lt;h2 id=&quot;마마마-포터블-psp-게임&quot;&gt;마마마 포터블 (PSP 게임)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/33.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/34.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/35.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 PSP 게임인 마법소녀 마도카☆마기카 포터블입니다. 이 게임은 워낙 인기가 없었는지 2012년에 출시된 게임에 한정판임에도 불구하고 아직도 신제품이 팔리고 있었습니다. 그래서 구하는 것이 크게 어렵지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/36.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;심지어 가격도 출시가와 비슷한 6천엔 정도입니다. (다만 지금은 가격이 조금 오른 것 같습니다)&lt;/p&gt;

&lt;p&gt;이 한정판의 가장 큰 특징은 피규어가 동봉되었다는 것입니다. 그렇기 때문에 포장 크기도 가장 큰데요. 앞면에서는 보이지 않지만 뒷면을 보면 마도카 피규어가 있는 것을 볼 수 있습니다. 피규어는 반프레스토에서 출시한 피그마 시리즈인데, 그래서인지 패키지에도 반프레스토의 로고가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/37.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;옆면을 열어보면 가운데에 피규어가 들어있고, 양 옆으로 두 개의 디스크가 들어있습니다. 사실상 포장 대부분은 피규어라고 생각하시면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/38.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/39.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/40.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 게임 본편입니다. 케이스의 크기는 스위치 게임과 비슷합니다. 저는 PSP를 어렸을 때 친구가 사용하는 것만 보고 지금까지 써본 적은 없는데, 말로만 듣던 UMD를 실물로 본건 이번에 처음입니다. UMD는 작은 CD가 들어있는 모양인데 디스크에 큐베의 얼굴이 박혀 있어서 귀엽습니다. 왼쪽에는 (요즘 게임에서 보기 힘든) 게임 설명서가 동봉되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/41.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;게임 설명서는 위와 같이 간단한 캐릭터 소개가 나와 있습니다. 아직 일본어 실력이 미천하여 어떤 내용인지는 잘 모르겠네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/42.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/43.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 번째로 들어있는 디스크는 스페셜 영상과 오리지널 사운드 트랙, 성우 인터뷰 영상과 같은 부가 요소가 동봉되어 있습니다. 열심히 일본어를 공부해서 언젠가 꼭 보고 싶습니다. 오른쪽 아래에 블루레이 로고가 있는 것으로 보아 블루레이 디스크인 것 같네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/44.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;케이스를 개봉하면 블루레이 디스크가 나오는데, 디스크의 디자인은 게임 UMD 디스크의 디자인과 동일합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/068/45.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;디스크를 제거하고 내부 디자인을 보니 캐릭터 일러스트가 선화로 그려져 있네요.&lt;/p&gt;

&lt;p&gt;피규어는 이미 갖고 있기 때문에 꺼내지 않았습니다. 꺼내봤자 놀 곳도 딱히 없구요. 보관하기에는 그냥 케이스에 넣어두는 것이 좋을 것 같습니다. 요즘 한창 일본어를 공부하고 있는데, N1을 딴 이후에 천천히 플레이해 볼 예정입니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">Policy Gradient Methods</title><link href="http://localhost:4000/rl/policy-gradient-methods/" rel="alternate" type="text/html" title="Policy Gradient Methods" /><published>2022-10-04T00:00:00+09:00</published><updated>2022-10-04T00:00:00+09:00</updated><id>http://localhost:4000/rl/policy-gradient-methods</id><content type="html" xml:base="http://localhost:4000/rl/policy-gradient-methods/">&lt;p&gt;이번 장은 드디어 마지막 장인 &lt;span style=&quot;color:red&quot;&gt;Policy Gradient&lt;/span&gt;입니다. 이번 장에서는 지금까지 이 교재에서 다룬 방법들과는 다르게, Policy 자체를 매개변수화하는 방법을 알아보겠습니다. 지금까지의 방법들은 Estimated Action-Value를 기반으로 Action을 선택했기 때문에 Action-Value를 추정하는 것이 중요했습니다. 하지만 이번 장에서 배울 새로운 방법인 Policy Gradient는 Action을 선택하는 데 Value Function을 사용하지 않습니다. 이번 장에서 사용할 새로운 표기는 Policy에 대한 매개변수 벡터인 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$입니다. 따라서 Policy는 이제 매개변수 $\boldsymbol{\theta}$를 포함하여 $\pi (a \mid s, \boldsymbol{\theta}) = Pr \{ A_t = a \mid S_t = s, \boldsymbol{\theta}_t = \boldsymbol{\theta} \}$로 표현합니다. 이것은 시간 $t$에서 State가 $s$이고 매개변수가 $\boldsymbol{\theta}$일 때 Action $a$를 선택할 확률로 정의됩니다. 만약 학습 알고리즘 안에서 Value Function에 대한 추정을 포함하는 경우, 이전과 마찬가지로 여전히 Weight Vector $\mathbf{w} \in \mathbb{R}^d$를 포함하여 $\hat{v}(s, \mathbf{w})$로 표현합니다.&lt;/p&gt;

&lt;p&gt;새로 정의하는 Policy 매개변수 $\boldsymbol{\theta}$를 학습하기 위해서는 스칼라 성능 측정 함수인 $J(\boldsymbol{\theta})$를 기반으로 합니다. 당연히 성능을 최대화하는 것이 목적이기 때문에, $J$의 Gradient를 상승시키기 위해 $\boldsymbol{\theta}$의 값을 조절합니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \widehat{\nabla J (\boldsymbol{\theta}_t)} \tag{13.1}\]

&lt;p&gt;식 (13.1)에서 $\widehat{\nabla J (\boldsymbol{\theta}_t)} \in \mathbb{R}^{d’}$는 매개변수 $\boldsymbol{\theta}_t$에 대해 성능을 나타내는 Gradient에 가까운 확률적 추정치입니다. 이러한 과정을 따르는 모든 방법은 Approximate Value Function을 학습하는지에 대한 여부에 상관 없는 Policy Gradient Method라고 합니다. 만약 Policy와 Value Function에 대한 근사값을 모두 학습한다면 Actor-Critic Method라고 합니다. Actor는 Policy를 학습하는 것을 의미하며, Critic은 Value Function을 학습하는 것을 말합니다. 이번 장은 Section 10.3과 마찬가지로 먼저 매개변수된 Policy 하에서의 State에 대한 Value로 성능이 정의되는 Episodic Task를 다룬 후, 성능이 Average Reward로 정의되는 Continuing Task를 다룰 예정입니다. 결국 마지막에는, 두 경우 모두에 대해 매우 유사한 용어를 사용하여 알고리즘을 표현할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;policy-approximation-and-its-advantages&quot;&gt;Policy Approximation and its Advantages&lt;/h2&gt;

&lt;p&gt;Policy Gradient Method에서 $\pi (a \mid s, \boldsymbol{\theta})$가 매개변수 $\boldsymbol{\theta}$에 대해 미분할 수 있고 모든 State $s \in \mathcal{S}$와 모든 Action $a \in \mathcal{A}(s)$, 그리고 매개변수 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$에 대해 유한하다면 Policy는 어떤 방식으로든 매개변수화할 수 있습니다. 실제로, 탐색을 보장하기 위해서는 Policy가 절대 Deterministic이 아니어야 합니다. (즉, 모든 $s, a, \boldsymbol{\theta}$에 대해 $\pi (a \mid s, \boldsymbol{\theta}) \in (0, 1)$) 이번 Section에서는 이산적인 Action Space에서의 가장 일반적인 매개변수화 방법을 소개하고, 그것이 Action-Value 방법에 비해 어떤 장점이 있는지 논의하겠습니다. Policy에 기반한 방법은 (추후 Section 13.7에서 설명하는 것처럼) 연속적인 Action Space가 주어졌을 때 유용한 방법이기도 합니다.&lt;/p&gt;

&lt;p&gt;만약 Action Space가 이산적이고 너무 크지 않다면, 일반적으로 떠올릴 수 있는 방법은 각 State-Action 쌍에 대해 매개변수화하는 Numerical Preference $h(s, a, \boldsymbol{\theta})$를 생성하는 것입니다. 각 State에서 Preference가 가장 높은 Action을 선택할 확률이 높게 만드는 것인데, 대표적인 방법으로 다음과 같은 &lt;span style=&quot;color:red&quot;&gt;Exponential Soft-max Distribution&lt;/span&gt;이 있습니다.&lt;/p&gt;

\[\pi (a|s, \boldsymbol{\theta}) \doteq \frac{e^{h(s, a, \boldsymbol{\theta})}}{\sum_b e^{h(s, a, \boldsymbol{\theta})}} \tag{13.2}\]

&lt;p&gt;식 (13.2)에서 $e \approx 2.71828$는 자연 로그의 밑입니다. 여기서 분모는 각 State의 Action을 선택할 확률의 합이 1이 되도록 설정한 것입니다. 이러한 Policy 매개변수화를 &lt;span style=&quot;color:red&quot;&gt;Soft-max in Action Preference&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Action Preference 설정 자체는 임의로 매개변수화할 수 있습니다. 예를 들어, Deep Artificial Neural Network (ANN)로 계산할 수도 있습니다. 여기서 $\boldsymbol{\theta}$는 네트워크의 모든 Connection Weight의 벡터로 구성됩니다. 또는, 간단하게 다음과 같이 선형으로 나타낼 수도 있습니다.&lt;/p&gt;

\[h(s, a, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\sf T} \mathbf{x} (s,a) \tag{13.3}\]

&lt;p&gt;식 (13.3)에서 $\mathbf{x}  (s, a) \in \mathbb(R)^{d’}$는 Section 9.5에서 설명했던 Feature Vector입니다.&lt;/p&gt;

&lt;p&gt;Action Preference의 Soft-max에 따라 Policy를 매개변수화하는 것의 장점 중 하나는 Approximate Policy가 Deterministic Policy에 점점 가까워진다는 것입니다. 지금까지 많이 사용했던 $\epsilon$-greedy의 경우 항상 무작위 Action을 선택할 확률이 존재합니다. 물론 Action-Value를 기반으로 Soft-max Distribution에 따라 Action을 선택할 수도 있지만, 이것만으로는 Policy가 Deterministic Policy에 가까워질 수 없습니다. 대신 Action-Value의 추정치는 그에 해당하는 Real Value값으로 수렴할 뿐이며, 이것은 0과 1이 아닌 특정 확률로 수렴합니다. Soft-max Distribution에 Temperature 매개변수가 포함된 경우 Temperature는 Deterministic에 접근하기 위해 시간이 지남에 따라 감소할 수 있지만, 실제 Action-Value에 대한 사전 지식 없이는 어느 정도 감소하게 할지, 또는 초기 Temperature를 어떻게 설정할 것인가에 대한 문제가 있기 때문입니다. 하지만 Action Preference는 이러한 특정한 값에 가까워지지 않기 때문에 다릅니다. 이것은 오직 Optimal Stochastic Policy를 유도하는데 주력할 뿐입니다. Optimal Policy가 Deterministic이라면, Optimal Action에 대한 Action Preference는 모든 다른 Action보다 무한히 높아집니다.&lt;/p&gt;

&lt;p&gt;Action Preference의 Soft-max에 따라 Policy를 매개변수화하는 것의 두 번째 장점으로는 임의의 확률로 Action을 선택할 수 있다는 것입니다. 특정한 Function Approximation이 있는 문제에서 Optimal Approximate Policy는 Stochastic일 수도 있습니다. 예를 들어, 불완전한 정보를 가진 카드 게임에서의 최적의 플레이는 포커와 같이 임의의 확률로 블러핑을 하는 것입니다. Action-Value 방법은 이런 경우 Stochastic Optimal Policy를 찾는 방법이 없지만, Policy Gradient Method는 다음 예제와 같이 이것이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 13.1) Short corridor with switched actions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프에 삽입된 작은 Gridworld 문제가 있습니다. Reward는 각 단계당 -1로 설정되어 있습니다. Episode는 항상 S에서 시작하고, G에 도착하면 종료됩니다. 맨 오른쪽 State를 제외한 나머지 State에서는 각각 오른쪽/왼쪽으로 이동하는 2가지 Action이 있습니다. (단, S에서 왼쪽으로 가는 Action은 움직이지 않는 것으로 대체합니다) 이 문제에서 재밌는 점은 왼쪽에서 두 번째 State의 경우, Action에 따른 결과가 반전된다는 것입니다. 즉, 왼쪽을 선택하면 오른쪽으로, 오른쪽을 선택하면 왼쪽으로 움직입니다.&lt;/p&gt;

&lt;p&gt;Function Approximation의 경우 모든 State가 동일하게 간주되므로 해결하기 어렵습니다. 예를 들어 모든 $s$에 대해 $\mathbf{x}(s, \text{right}) = [1, 0]^{\sf T}$, $\mathbf{x}(s, \text{left}) = [0, 1]^{\sf T}$로 정의하면, $\epsilon$-greedy를 사용한 Action-Value 방법은 크게 2가지 Policy만을 생성할 수 있습니다. 하나는 모든 단계에서 높은 확률로 오른쪽을 선택하고 $1 - \epsilon / 2$의 확률로 왼쪽을 선택하는 것이고, 다른 하나는 그 반대를 선택하는 것입니다. 만약 $\epsilon = 0.1$이라면 이 2개의 Policy는 위의 그래프처럼 시작 State에서 -44와 -82의 기대 Value를 각각 얻습니다. 만약 Stochastic Policy를 사용할 수 있다면 훨씬 더 나은 성능을 보일 수 있습니다. 가장 좋은 확률은 오른쪽을 약 0.59의 확률로 선택하는 것이며, 이 때의 Value는 약 -11.6이 됩니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Policy에 대한 매개변수화가 Action-Value 매개변수화에 비해 가질 수 있는 가장 간단한 장점은 Policy가 더 간단한 함수로 근사화할 수 있다는 것입니다. 하지만 이것은 Policy와 Action-Value Function의 복잡성에 따라 다릅니다. 문제에 따라 Action-Value Function이 더 간단하게 근사화할 수도 있기 때문입니다. 다행히 일반적인 경우에는 Policy 기반 방법이 더 빠르게 학습하고 우수한 Policy를 생성할 수 있다는 것이 증명되었습니다. (참고 : Şimşek, Algorta, Kothiyal, 2016)&lt;/p&gt;

&lt;p&gt;마지막으로, Policy를 매개변수하는 것은 때때로 원하는 Policy의 형태에 대한 사전 지식을 강화학습 시스템에 전달하는 좋은 방법이 될 수도 있습니다. 이것은 Policy 기반 학습 방법을 사용하는 가장 큰 이유 중 하나입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-policy-gradient-theorem&quot;&gt;The Policy Gradient Theorem&lt;/h2&gt;

&lt;p&gt;이전 Section에서 설명한 Policy 매개변수화의 장점 외에도 중요한 이론적인 이점이 있습니다. 지속적인 Policy 매개변수화를 사용한 Action 확률은 학습된 매개변수에 대한 함수로 쉽게 표현되는 반면, $\epsilon$-greedy에서의 Action 확률은 추정된 Action-Value의 작은 변화에도 극적으로 변할 수 있습니다. 이로 인해 Policy Gradient Method는 Action-Value 방법보다 더 강력한 수렴을 보장할 수 있습니다. 특히, 매개변수에 대한 Policy 의존성의 연속성이 식 (13.1)에서 Policy Gradient Method가 Gradient Ascent를 사용할 수 있게 보장합니다.&lt;/p&gt;

&lt;p&gt;Episodic Task와 Continuing Task에서 성능 측정 함수 $J(\boldsymbol{\theta})$를 다르게 정의하므로 어느 정도 다르게 취급해야 합니다. 하지만 일단 여기서는 두 가지 경우를 통합하고, 중요한 이론적 결과를 단일 방정식으로 설명할 수 있도록 표기법을 정리할 것입니다.&lt;/p&gt;

&lt;p&gt;이 Section에서는 Episode의 시작 State에 대한 Value를 성능 측정으로 정의하는 Episodic Task를 다루겠습니다. 모든 Episode가 무작위가 아닌 특정한 State $s_0$에서 시작한다고 가정함으로써, 일반성을 잃지 않고 표기법을 단순화할 수 있습니다. 그러면 Episodic Task의 성능을 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}} (s_0) \tag{13.4}\]

&lt;p&gt;식 (13.4)에서 $v_{\pi_{\boldsymbol{\theta}}}$는 $\boldsymbol{\theta}$로 인해 정의된 Policy인 $\pi_{\boldsymbol{\theta}}$를 따를 때의 Real Value Function입니다. 여기에서는 일단 완전성을 위해 알고리즘에서 Discounting을 포함하지만, Episodic Task에서는 Discounting이 없다고 가정하겠습니다. (즉, $\gamma = 1$)&lt;/p&gt;

&lt;p&gt;Function Approximation을 사용하면 Policy Improvement를 보장하는 방식으로 Policy 매개변수를 업데이트하는 것이 어려울 수 있습니다. 문제는 성능이 Action의 선택과, 그러한 선택을 만든 State Distribution 모두에 의존하는데, 이 두 가지 모두 Policy 매개변수에 의해 영향을 받는다는 것입니다. State가 주어졌을 때 Action에 대한 Policy 매개변수의 효과와 Reward에 대한 영향은 매개변수화에 대한 지식으로부터 간단한 방법으로 계산할 수 있습니다. 그러나 State Distribution에 대한 Policy의 영향은 Environment의 함수이기 때문에 일반적으로 알 수 없습니다. 그렇다면 Gradient가 State Distribution에서 알 수 없는 Policy 변경에 의존할 때 Policy 매개변수에 대한 성능의 Gradient를 어떻게 추정해야 할까요?&lt;/p&gt;

&lt;p&gt;다행히 Policy 매개변수에 대한 성능의 Gradient를 분석할 수 있는 &lt;span style=&quot;color:red&quot;&gt;Policy Gradient Theorem&lt;/span&gt;이라는 이론적 해결 방법이 있습니다. 이로 인해 식 (13.1)의 Gradient Ascent를 추정할 수 있으며, State Distribution에 대한 미분을 포함하지 않습니다. Episodic Task의 경우 Policy Gradient Theorem은 다음과 같이 적용할 수 있습니다.&lt;/p&gt;

\[\nabla J(\boldsymbol{\theta}) \propto \sum_s \mu (s) \sum_a q_{\pi} (s, a) \nabla \pi (a | s, \boldsymbol{\theta}) \tag{13.5}\]

&lt;p&gt;이 식에서의 Gradient는 $\boldsymbol{\theta}$의 각 원소에 대한 편미분의 열 벡터이고, $\pi$는 매개변수 벡터 $\boldsymbol{\theta}$에 해당하는 Policy를 의미합니다. 기호 $\propto$는 비례를 의미합니다. Episodic Task의 경우 비례 상수는 Episode의 평균 길이이고, Continuing Task의 경우 1이 됩니다. (즉, 이 때는 등식이 됩니다) Distribution $\mu$는 9장과 10장에서 다루었던 $\pi$에 대한 On-policy Distribution 입니다. Episodic Task에서 Policy Gradient Theorem은 다음과 같이 증명할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of the Policy Gradient Theorem for Episodic Case&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;간단한 미적분학과 식의 변형으로 Policy Gradient Theorem을 증명할 수 있습니다. 표기법을 단순하게 유지하기 위해, 모든 경우에 $\pi$가 $\boldsymbol{\theta}$에 대한 함수이고, 모든 Gradient도 $\boldsymbol{\theta}$에 대해 표현할 수 있다는 것을 가정하겠습니다. 먼저 State-Value Function의 Gradient는 다음과 같이 Action-Value Function의 관점에서 나타낼 수 있습니다.&lt;/p&gt;

\[\begin{align}
\nabla v_{\pi} (s) &amp;amp;= \nabla \left[ \sum_a \pi (a|s) q_{\pi} (s, a) \right], \quad \text{for all } s \in \mathcal{S} \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi}(s, a) + \pi (a|s) \nabla q_{\pi} (s, a) \right] \tag{product rule of calculus} \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi} (s,a) + \pi(a|s) \nabla \sum_{s&apos;, r} p (s&apos;, r|s, a) (r + v_{\pi} (s&apos;)) \right] \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi} (s,a) + \pi (a|s) \sum_{s&apos;} p(s&apos; |s, a) \nabla v_{\pi} (s&apos;) \right] \tag{Equation 3.4} \\ \\
&amp;amp;= \sum_a \Bigg[ \nabla \pi (a|s) q_{\pi}(s,a) + \pi(a|s) \sum_{s&apos;} p(s&apos; |s, a) \\ \\
&amp;amp; \qquad \sum_{a&apos;} [\nabla \pi (a&apos;|s&apos;) q_{\pi} (s&apos;, a&apos;) + \pi (a&apos;|s&apos;) \sum_{s&apos;&apos;} p (s&apos;&apos; | s&apos;, a&apos;) \nabla v_{\pi} (s&apos;&apos;)] \Bigg] \tag{unrolling} \\ \\
&amp;amp;= \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \text{Pr} (s \to x, k, \pi) \sum_{a} \nabla \pi (a|s) q_{\pi} (x, a)
\end{align}\]

&lt;p&gt;Unrolling 부분은 식이 너무 길어서 2줄로 나누었습니다. 위 식에서 $\text{Pr} \left( s \to x, k, \pi \right)$는 Policy $\pi$에 따라 $k$단계 후에 State $s$에서 State $x$로 전환될 확률입니다. 이것을 이용하여 $\nabla J(\boldsymbol{\theta})$의 식을 변형하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp;= \nabla v_{\pi} (s_0) \\ \\
&amp;amp;= \sum_s \left( \sum_{k=0}^{\infty} \text{Pr} (s_0 \to s, k, \pi) \right) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \\ \\
&amp;amp;= \sum_a \eta (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Equation 9.2} \\ \\
&amp;amp;= \sum_{s&apos;} \eta (s&apos;) \sum_s \frac{\eta (s)}{\sum_{s&apos;} \eta(s&apos;)} \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \\ \\
&amp;amp;= \sum_{s&apos;} \eta (s&apos;) \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Equation 9.3} \\ \\
&amp;amp;\propto \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Q.E.D.}
\end{align}\]

&lt;h2 id=&quot;reinforce-monte-carlo-policy-gradient&quot;&gt;REINFORCE: Monte Carlo Policy Gradient&lt;/h2&gt;

&lt;p&gt;이제 Policy Gradient를 사용한 첫 번째 학습 알고리즘을 만들 준비가 되었습니다. 학습의 기본 전략은 이전에 다룬 식 (13.1)의 Stochastic Gradient Ascent를 기반으로 합니다. 이것을 수행하기 위해서는 Sample Gradient의 기대값이 매개변수의 함수로써 성능 측정의 실제 Gradient에 비례하도록 Sample을 얻는 방법이 필요합니다. Sample의 Gradient는 원래의 Gradient에 비례하기만 하면 되는데, 왜냐하면 비례 상수는 Step-size Parameter인 $\alpha$와 통합하여 취급할 수도 있고, 임의로 설정할 수도 있기 때문입니다. Policy Gradient Theorem은 Gradient에 비례하는 정확한 표현을 제공하므로, 필요한 것은 기대값이 이 표현식과 같거나 가까운 Sampling 방법입니다. Policy Gradient Theorem의 오른쪽 항은 Target Policy $\pi$ 하에서 State가 얼마나 자주 발생하는지에 따라 Weight가 부여된 State의 합입니다. 즉, Policy $\pi$를 따를 경우, 이러한 비율로 State가 발생합니다. 이것을 식으로 표현하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp; \propto \sum_s \mu (s) \sum_a q_{\pi} (s, a) \nabla \pi (a | s, \boldsymbol{\theta}) \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ \sum_a q_{\pi} (S_t, a) \nabla \pi (a|S_t, \boldsymbol{\theta}) \right] \tag{13.6}
\end{align}\]

&lt;p&gt;또한 식 (13.1)의 Stochastic Gradient Ascent 알고리즘을 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha \sum_a \hat{q} (S_t, a, \mathbf{w}) \nabla \pi (a | S_t, \boldsymbol{\theta}) \tag{13.7}\]

&lt;p&gt;식 (13.7)에서 $\hat{q}$는 $q_{\pi}$에 대한 학습된 근사치입니다. 업데이트가 모든 Action을 포함하기 때문에 이 알고리즘은 &lt;span style=&quot;color:red&quot;&gt;All-actions&lt;/span&gt; 방법이라고 불리며, 이 방법 중 대표적인 것으로 시간 $t$에서 실제로 취한 Action인 $A_t$에 대한 업데이트가 포함된 고전적인 &lt;span style=&quot;color:red&quot;&gt;REINFORCE&lt;/span&gt; 알고리즘입니다. (Willams, 1992 참고)&lt;/p&gt;

&lt;p&gt;REINFORCE의 완전한 알고리즘을 유도하기 위해서는 식 (13.6)가 $S_t$를 포함한 것과 마찬가지로 $A_t$를 포함시켜야 합니다. 즉, 확률 변수의 가능한 값에 대한 합을 Policy $\pi$ 하의 기대값으로 대체하여 Sampling합니다. 식 (13.6)은 Action에 대한 적절한 합을 포함하지만, 각 항은 $\pi (a \mid S_t, \boldsymbol{\theta})$에 Weight가 부여되지 않습니다. 그래서 우리는 전체적인 식의 관계가 변경되지 않도록 $\pi (a \mid S_t, \boldsymbol{\theta})$으로 나누어 식을 수정합니다. 이 과정을 식 (13.6)에 이어서 작성하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp; \propto \mathbb{E}_{\pi} \left[ \sum_a \pi (a|S_t, \boldsymbol{\theta}) q_{\pi} (S_t, a) \frac{\nabla \pi (a | S_t, \boldsymbol{\theta})}{\pi (a | S_t, \boldsymbol{\theta})} \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ q_{\pi} (S_t, A_t) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \right] \qquad \text{(replacing } a \text{ by the sample } A_t \sim \pi \text{)} \\ \\
&amp;amp;=\mathbb{E}_{\pi} \left[ G_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \right] \qquad \text{(because } \mathbb{E}_{\pi} [G_t | S_t, A_t] = q_{\pi} (S_t, A_t) \text{)}
\end{align}\]

&lt;p&gt;위 식에서 $G_t$는 일반적인 Return입니다. 마지막 식의 결과는 기대값의 Gradient에 비례하는 각 시간 단계에서의 Sampling할 수 있는 양입니다. 이 Sample을 사용하여 식 (13.1)의 Stochastic Gradient Ascent 알고리즘을 인스턴스화하면 REINFORCE 업데이트 식이 유도됩니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha G_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \tag{13.8}\]

&lt;p&gt;식 (13.8)과 같은 업데이트 식은 직관적입니다. 각 시간별 증가분은 Return $G_t$와 Action을 취할 확률의 Gradient를 그 확률로 나눈 값에 비례합니다. 그 벡터의 방향은 미래에 State $S_t$를 방문할 때 Action $A_t$를 반복할 확률을 가장 많이 증가시키는 매개변수 공간의 방향입니다. 즉, 업데이트는 Return $G_t$에 비례하고, Action 확률에 반비례하는 방향으로 매개변수 벡터를 증가시킵니다. Return에 비례하는 것은 가장 높은 Reward를 얻을 수 있는 Action을 선호하는 방향으로 매개변수를 움직인다는 의미가 있습니다. Action 확률에 비례하는 것은 자주 선택되는 Action을 선호한다는 뜻입니다. 자주 선택되는 Action은 가장 높은 Reward를 내지 못하더라도 유리한 선택이 될 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;REINFORCE는 Episode가 끝날 때까지 미래의 모든 Reward를 포함하는 시간 $t$부터 완전한 Return을 사용합니다. 이런 의미에서 REINFORCE는 Monte Carlo 알고리즘이라고 볼 수 있으며, Episode가 끝난 후 모든 업데이트가 수행되는 Episode Task에만 잘 정의됩니다. REINFORCE 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Pseudocode에서 마지막 줄은 REINFORCE 업데이트인 식 (13.8)과 다른 점이 있습니다. 먼저 $\nabla \ln x = \frac{\nabla x}{x}$라는 것을 이용하여, $\frac{\nabla \pi (A_t \mid S_t, \boldsymbol{\theta}_t)}{\pi (A_t \mid S_t, \boldsymbol{\theta}_t)}$를  $\nabla \ln \pi (A_t \mid S_t, \boldsymbol{\theta}_t)$로 바꾸었습니다. 이 벡터의 이름은 문헌에 따라 다르지만, 여기서는 간단하게 &lt;span style=&quot;color:red&quot;&gt;Eligibility Vector&lt;/span&gt;라고 명칭하겠습니다. 이 부분이 알고리즘에서 Policy 매개변수화가 나타나는 유일한 부분입니다.&lt;/p&gt;

&lt;p&gt;마지막 줄에서의 또 다른 차이점으로는 $\gamma^t$의 유무입니다. 식 (13.8)을 논할 때는 Discounting을 가정하지 않았기 때문에(즉, $\gamma = 1$) 이것을 생략하였지만, 위의 Pseudocode는 일반적인 경우에 대한 알고리즘이기 때문에 포함되었습니다.&lt;/p&gt;

&lt;p&gt;다음의 그래프는 Example 13.1에서 REINFORCE의 성능을 나타내고 있습니다. $\alpha$에 값에 따라 성능의 차이가 크기 때문에, 좋은 $\alpha$를 정하는 것이 중요하다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;REINFORCE는 Stochastic Gradient Method로 인해 좋은 이론적 수렴 특성을 가지고 있습니다. Episode에 대한 예상 업데이트는 성능에 대한 Gradient와 같은 방향입니다. 이것은 충분히 작은 $\alpha$에 대해 예상되는 성능의 개선과, $\alpha$가 감소하는 일반적인 확률적 근사 조건 하에 Local Optimum에 수렴하는 것을 보장합니다. 그러나 Monte Carlo Method인 REINFORCE는 Variance가 커서 학습이 느려질 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;reinforce-with-baseline&quot;&gt;REINFORCE with Baseline&lt;/h2&gt;

&lt;p&gt;Policy Gradient Theorem을 나타내는 식 (13.5)는 상대적인 Action-Value를 비교하는데 사용되는 임의의 baseline $b(s)$를 포함하여 일반화할 수 있습니다.&lt;/p&gt;

\[\nabla J(\boldsymbol{\theta}) \propto \sum_s \mu (s) \sum_a \left( q_{\pi} (s, a) - b(s) \right) \nabla (a | s, \boldsymbol{\theta}) \tag{13.10}\]

&lt;p&gt;Baseline은 $a$에 의해 변하지 않는 한, 확률 변수를 포함한 어떤 함수든 될 수 있습니다. 다음과 같이 Baseline은 Gradient를 취했을 때 0이 되기 때문입니다.&lt;/p&gt;

\[\sum_a b(s) \nabla \pi (a|s, \boldsymbol{\theta}) = b(s) \nabla \sum_a \pi (a|s, \boldsymbol{\theta}) = b(s) \nabla 1 = 0\]

&lt;p&gt;식 (13.10)과 같이 Baseline이 있는 Policy Gradient Theorem을 사용하여 이전 Section에서와 유사하게 업데이트 규칙을 유도할 수 있습니다. 다음은 Baseline을 포함한 새로운 버전의 REINFORCE 업데이트 식입니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha \left( G_t - b(S_t) \right) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \tag{13.11}\]

&lt;p&gt;Baseline은 0으로 균일할 수 있으므로 위의 식 (13.11)은 REINFORCE의 엄격한 일반화입니다. 일반적으로 Baseline은 업데이트의 Expected Value를 변경하지 않지만, Variance에 큰 영향을 줄 수 있습니다. 예를 들어, Section 2.8에서와 유사한 Baseline은 Gradient Bandit Algorithm의 Variance를 크게 줄일 수 있습니다. Variance가 줄어든다는 것은 그만큼 학습 속도가 빨라진다는 의미입니다.&lt;/p&gt;

&lt;p&gt;Bandit Algorithm에서 Baseline은 숫자(=평균 Reward)에 불과했지만, MDP의 경우 Baseline은 State에 따라 달라져야 합니다. 어떤 State에서는 모든 Action이 높은 Value를 가질 수 있기 때문에, 더 높은 Value와 덜 높은 Value의 Action을 구별하기 위해 높은 Baseline이 필요합니다. 물론 반대로 모든 Action이 낮은 Action에서는 낮은 Baseline을 통해 Action의 상대적 Value를 구별해야 합니다.&lt;/p&gt;

&lt;p&gt;Baseline에 대한 자연스러운 선택 중 하나는 State-Value의 추정값인 $\hat{v} (S_t, \mathbf{w})$ 입니다. 여기서 $\mathbf{w} \in \mathbb{R}^d$는 이전 장에서 제시된 방법 중 하나로 학습된 Weight Vector입니다. REINFORCE는 Policy 매개변수 $\boldsymbol{\theta}$를 학습하기 위한 Monte Carlo Method이기 때문에 Monte Carlo Method를 사용하여 State-Value의 Weight인 $\mathbf{w}$를 학습할 수도 있습니다. 학습된 State-Value Function을 Baseline으로 사용하는 REINFORCE의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 알고리즘에는 두 개의 Step-size Parameter인 $\alpha^{\boldsymbol{\theta}}$와 $\alpha^{\mathbf{w}}$가 있습니다. 이 중 $\alpha^{\mathbf{w}}$를 선택하는 것은 비교적 쉽습니다. 예를 들어, Section 9.6에서와 같이 Linear의 경우 $\alpha^{\mathbf{w}} = 0.1 / \mathbb{E} [ \lVert \nabla \hat{v} (S_t, \mathbf{w}) \rVert^2_{\mu}]$라는 경험적인 법칙이 있었습니다. 하지만 $\alpha^{\boldsymbol{\theta}}$는 Reward의 범위와 Policy 매개변수화에 따라 결정해야하기 때문에 명확한 방법이 존재하지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 예제 13.1에서 Baseline이 있는 경우와 없는 경우 REINFORCE의 성능을 비교합니다. 이 비교에 사용된 State-Value Function의 추정값은 $\hat{v} (s, \mathbf{w}) = w$입니다. 즉, $\mathbf{w}$는 단일 요소 $w$로 구성되어 있습니다. 그래프를 보시면, 수렴되는 값은 두 방법이 차이가 없지만, Baseline을 사용하는 방법이 더 빠르게 수렴함을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;actorcritic-methods&quot;&gt;Actor–Critic Methods&lt;/h2&gt;

&lt;p&gt;Baseline이 있는 REINFORCE에서 학습된 State-Value Function은 각 State Transition에서 첫 번째 State의 Value를 추정합니다. 이 추정값은 후속 Return에 대한 Baseline을 설정하지만, Action이 Tranistion되기 전에 이루어지므로 해당 Action을 평가하는데 사용할 수 없습니다.&lt;/p&gt;

&lt;p&gt;반면, Actor-Critic Method에서는 State-Value Function이 두 번째 State에도 적용됩니다. 두 번째 State의 Estimated Value는 Discount되어 Reward에 추가될 때, 1-step Return $G_{t:t+1}$을 포함하며, 이것은 실제 Return에 대한 올바른 추정이므로 Action을 평가할 수 있습니다. 이전에 배운 TD Learning에서도 보았듯이, 1-step Return은 Bias를 감안하더라도 Variance 및 계산 적합성 측면에서 실제 Return보다 우수합니다. 또한 7장 및 12장에서와 같이 $n$-step Return 및 Eligibility Trace를 사용하여 Bias의 범위를 유연하게 조정할 수도 있습니다. 이와 같이 State-Value Function을 사용하여 Action을 평가할 때, 이를 &lt;span style=&quot;color:red&quot;&gt;Critic&lt;/span&gt;이라고 하며, 전체 Policy Gradient Method를 &lt;span style=&quot;color:red&quot;&gt;Actor-Critic Method&lt;/span&gt;라고 합니다. Gradient 추정값의 Bias는 Bootstrapping으로 인한 것이 아니기 때문에 Critic이 Monte Carlo Method로 학습하더라도 Actor는 Bias될 것입니다.&lt;/p&gt;

&lt;p&gt;먼저 1-step Actor-Critic Method를 소개하겠습니다. 1-step 방법의 장점은 완전한 On-line 및 Incremental 방식임에도 Eligibility Trace의 복잡함을 피할 수 있다는 것입니다. 1-step Actor-Critic Method는 REINFORCE 방법인 식 (13.11)의 전체 Return을 다음과 같이 1-step Return으로 대체합니다. (단, 이때 Baseline은 학습된 State-Value Function을 사용합니다)&lt;/p&gt;

\[\begin{align}
\boldsymbol{\theta}_{t+1} &amp;amp; \doteq \boldsymbol{\theta}_t + \alpha \Big( G_{t:t+1} - \hat{v} (S_t, \mathbf{w}) \Big) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.12} \\ \\
&amp;amp;= \boldsymbol{\theta}_t + \alpha \Big( R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w}) \Big) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.13} \\ \\
&amp;amp;= \boldsymbol{\theta}_t + \alpha \delta_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.14}
\end{align}\]

&lt;p&gt;이 업데이트와 짝을 이루는 State-Value Function의 학습 방법은 Semi-gradient TD(0)입니다. 이것을 반영한 전체 알고리즘의 Pseudocode는 다음과 같습니다. 이 알고리즘은 State, Action, 그리고 Reward가 발생하는 즉시 처리되는 완전한 On-line Incremental 알고리즘이라는 점에 유의하시기 바랍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 1-step 방법의 Forward View에 대한 일반화 및 $n$-step으로의 일반화는 간단합니다. 이 때, 식 (13.12)의 1-step Return은 각각 $G_{t:t+n}$ 또는 $G^{\lambda}_t$로 대체됩니다. $\lambda$-return 알고리즘의 Backward View도 12장의 패턴을 따라 Actor와 Critic에 대해 별도의 Eligibility Trace를 사용하여 처리합니다. 이것을 반영한 전체 알고리즘의 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;policy-gradient-for-continuing-problems&quot;&gt;Policy Gradient for Continuing Problems&lt;/h2&gt;

&lt;p&gt;Section 10.3에서와 같이, Continuing Task의 성능은 다음과 같이 시간 단계에서 Average Reward로 정의해야 합니다.&lt;/p&gt;

\[\begin{align}
J(\boldsymbol{\theta}) &amp;amp; \doteq r(\pi) \doteq \lim_{h \to \infty} \frac{1}{h} \sum_{t=1}^h \mathbb{E} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \tag{13.15} \\ \\
&amp;amp;= \lim_{t \to \infty} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \\ \\
&amp;amp;\sum_s \mu (s) \sum_a \pi (a | s) \sum_{s&apos;, r} p (s&apos;, r | s, a) r
\end{align}\]

&lt;p&gt;위 식에서 $\mu$는 Policy $\pi$ 하에서의 Steady-state Distribution, $\mu (s) \doteq \underset{t \to \infty}{\operatorname{lim}} \text{Pr} \{ S_t = s \mid A_{0:t} \sim \pi \}$는 State $S_0$와 독립적으로 존재한다고 가정합니다. 이것은 $\pi$에 따라 Action을 선택하는 경우, 동일한 Distribution를 유지하는 특별한 Distribution임을 유의하시기 바랍니다.&lt;/p&gt;

\[\sum_s \mu (s) \sum_a \pi (a|s, \boldsymbol{\theta}) p(s&apos;|s,a) = \mu (s&apos;), \quad \text{for all } s&apos; \in \mathcal{S} \tag{13.16}\]

&lt;p&gt;(Backward View) Continuing Task에서 Actor-Critic 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 Continuing Task에서 Value Function은 각각 $v_{\pi} (s) \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right]$ (State-Value)와 $q_{\pi} (s, a) \doteq \mathbb{E} \left[ G_t \mid S_t = s, A_t = a \right]$ (Action-Value)로 정의됩니다. 이 때, Return $G_t$는 다음과 같습니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \cdots \tag{13.17}\]

&lt;p&gt;이렇게 정의가 바꾸더라도 식 (13.5) Policy Gradient Theorem은 유효합니다. Continuing Task에서 Policy Gradient Theorem에 대한 증명은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of the Policy Gradient Theorem for Continuing Case&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Episodic Task에서와 마찬가지로 먼저 모든 경우에 $\pi$가 $\boldsymbol{\theta}$에 대한 함수이고, Gradient가 $\boldsymbol{\theta}$로 표현될 수 있다는 가정이 필요합니다. 식 (13.15)에 의하여 Continuing Task에서 $J(\boldsymbol{\theta}) = r(\pi)$이고, $v_{\pi}$와 $q_{\pi}$는 식 (13.17)의 Return으로 구성되어 있습니다. 따라서 State-Value Function의 Gradient는 임의의 State $s \in \mathcal{S}$에 대해 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\nabla v_{\pi} (s) &amp;amp;= \nabla \left[ \sum_a \pi (a | s) q_{\pi} (s, a) \right], \quad \text{for all } s \in \mathcal{S} \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \nabla q_{\pi} (s, a) \Big] \tag{product rule of calculus} \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \nabla \sum_{s&apos; ,r} p(s&apos;, r|s, a) \big(r - r(\boldsymbol{\theta}) + v_{\pi} (s&apos;) \big) \Big] \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \big[ - \nabla r(\boldsymbol{\theta}) + \sum_{s&apos;} p (s&apos; |s, a) \nabla v_{\pi} (s&apos;) \big] \Big] \end{align}\]

&lt;p&gt;이 식을 정리하면 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\nabla r(\boldsymbol{\theta}) = \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \sum_{s&apos;} p(s&apos; |s, a) \nabla v_{\pi} (s&apos;) \Big] - \nabla v_{\pi} (s)\]

&lt;p&gt;위 식의 좌변은 $J(\boldsymbol{\theta})$로 표기할 수 있으며, State $s$에 의존하지 않습니다. 따라서 우변 역시 State $s$에 의존하지 않으므로 식의 변경 없이 Weight $\mu (s)$를 붙여 합산할 수 있습니다. ($\because \sum_s \mu (s) = 1$)&lt;/p&gt;

\[\begin{align}
J(\boldsymbol{\theta}) &amp;amp;= \sum_s \mu (s) \Bigg( \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \sum_{s&apos;} p(s&apos; | s, a) \nabla v_{\pi} (s&apos;) \Big] - \nabla v_{\pi} (s) \Bigg) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_s \mu (s) \sum_a \pi (a|s) \sum_{s&apos;} p(s&apos; | s, a) \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_{s&apos;} \underbrace{\sum_s \mu (s) \sum_a \pi (a|s) p (s&apos; | s, a)}_{\mu (s&apos;) \text{( 13.16)}} \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_{s&apos;} \mu (s&apos;) \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \tag{Q.E.D.}
\end{align}\]

&lt;h2 id=&quot;policy-parameterization-for-continuous-actions&quot;&gt;Policy Parameterization for Continuous Actions&lt;/h2&gt;

&lt;p&gt;Policy에 기반한 방법은 Action Space가 매우 큰 경우(=Action의 수가 매우 많은 경우)나 Action의 수가 무한한 연속적인 Space에서도 효과적인 해법을 제공합니다. 이런 경우, Action 각각에 대해 학습된 확률을 계산하는 대신 확률 분포의 통계를 학습합니다. 예를 들어, Action 집합은 Normal Distribution(=Gaussian Distribution)에서 선택된 Action을 포함하는 실수 집합일 수 있습니다.&lt;/p&gt;

&lt;p&gt;아시다시피, Normal Distribution에 대한 Probability Density Function은 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[p(x) \doteq \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(x - \mu)^2}{2 \sigma^2} \right) \tag{13.18}\]

&lt;p&gt;Probability Density Function에서 $\mu$는 Normal Distribution의 평균이고, $\sigma$는 표준편차입니다. 이 때 $p(x)$는 $x$가 일어날 확률이 아니라 $x$에서의 Probability Density입니다. 즉, 1보다 클 수도 있습니다. 합이 1이 되어야 하는 부분은 $p(x)$와 $x$축 사이의 총 넓이입니다. 일반적으로 $x$의 값을 범위로 정하여 적분을 취하면 해당 범위 내에 $x$가 존재할 확률을 구할 수 있습니다. Probability Density Function에서 평균과 표준편차의 값에 따라 나타낸 그래프는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Probability Density Function을 기반으로 Policy 매개변수화를 생성하기 위해서는, Policy를 State에 따라 달라지는 매개변수 함수 근사가 제공하는 평균 및 표준편차와 함께 실수 값 Scalar Action을 Normal Probability Density로 정의하면 됩니다. 즉,&lt;/p&gt;

\[\pi (a|s, \boldsymbol{\theta}) \doteq \frac{1}{\sigma(s, \boldsymbol{\theta}) \sqrt{2 \pi}} \exp \left( - \frac{(a - \mu (s, \boldsymbol{\theta}))^2}{2 \sigma (s, \boldsymbol{\theta})^2} \right) \tag{13.19}\]

&lt;p&gt;식 (13.19)에서 $\mu : \mathcal{S} \times \mathbb{R}^{d’} \to \mathbb{R}$과 $\sigma : \mathcal{S} \times \mathbb{R}^{d’} \to \mathbb{R}^+$는 매개변수화된 함수 근사입니다.&lt;/p&gt;

&lt;p&gt;위 식을 완성하기 위해서는 두 개의 함수 근사에 대한 형태를 정의해야 합니다. 이를 위해 Policy의 매개변수 벡터를 $\boldsymbol{\theta} = [\boldsymbol{\theta}_{\mu}, \boldsymbol{\theta}_{\sigma}]^{\sf T}$와 같이 두 부분으로 나눕니다. 이 두 부분은 각각 평균과 표준편차에 대한 근사를 의미합니다. 이 중 평균은 Linear Function으로 근사할 수 있습니다. 표준편차의 경우에는 항상 양수여야하는 조건이 있기 때문에 Linear Function의 지수 형태로 근사할 수 있습니다. 이를 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\mu (s, \boldsymbol{\theta}) \doteq \boldsymbol{\theta}_{\mu}^{\sf T} \mathbf{x}_{\mu} (s) \quad \text{and} \quad \sigma (s, \boldsymbol{\theta}) \doteq \exp \left( \boldsymbol{\theta}_{\sigma}^{\sf T} \mathbf{x}_{\sigma} (s) \right) \tag{13.20}\]

&lt;p&gt;식 (13.20)에서 $\mathbf{x}_{\mu} (s)$와 $\mathbf{x}_{\sigma} (s)$는 Section 9.5에서 설명된 방법 중 하나로 구성되는 Feature Vector입니다. 이러한 정의를 사용하면 이번 장에서 배웠던 알고리즘 중 하나를 적용하여 실수값으로 정의된 Action을 선택하는 것을 학습할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장을 배우기 전까지는 Action-Value를 학습한 다음, Action을 선택하는데 사용하는 방법(Action-Value Method)에 초점을 맞췄습니다. 반면에 이번 장에서는 Action-Value의 추정값을 참조하지 않고 Action을 선택할 수 있도록 매개변수화된 Policy를 학습하는 방법을 고려하였습니다. 특히, Policy 매개변수에 대한 성능의 Gradient를 추정하는 방법인 &lt;strong&gt;Policy Gradient Method&lt;/strong&gt;를 고려하였습니다.&lt;/p&gt;

&lt;p&gt;Policy 매개변수를 학습하는 방법에는 많은 이점이 있습니다. 먼저, Action을 취할 특정한 확률을 학습할 수 있습니다. 이 방법들은 적절한 수준의 Exploration을 수행하고 Deterministic Policy에 점근적으로 접근할 수 있습니다. 또한 연속적인 Action Space도 다룰 수 있습니다. 이것은 Policy 기반 방법에서는 쉽지만 $\epsilon$-greedy나 일반적인 Action-Value 방법으로는 어렵거나, 불가능합니다. 또한 일부 문제에서는 Value Function을 매개변수로 표현하는 것보다, Policy를 매개변수로 표현하는 것이 더 간단합니다. 이러한 문제들은 매개변수화된 Policy 방법에 더 적합합니다.&lt;/p&gt;

&lt;p&gt;매개변수화된 Policy 방법은 &lt;strong&gt;Policy Gradient Theorem&lt;/strong&gt;을 기반으로 한 중요한 이론적 장점이 있습니다. 이 정리는 State Distribution에 대한 정보를 포함하지 않는 Policy 매개변수에 의해 성능이 어떻게 영향을 받는지에 대한 정확한 공식을 제공합니다. 이 정리는 모든 Policy Gradient Method에 대한 이론적인 토대를 마련합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;REINFORCE&lt;/strong&gt;는 Policy Gradient Theorem을 직접적으로 따르는 방법입니다. 또한 State-Value Function을 Baseline으로 추가하면 Bias를 피하면서 REINFORCE의 Variance를 줄일 수 있습니다. State-Value Function이 Policy의 Action 선택을 평가하거나 비판하는데 사용되는 경우, Value Function을 &lt;strong&gt;Critic&lt;/strong&gt;이라고 하고 Policy를 &lt;strong&gt;Actor&lt;/strong&gt;라고 합니다. 이 방법은 &lt;strong&gt;Actor-Critic Method&lt;/strong&gt;이라고 합니다. Critic은 Actor의 Gradient 추정에 Bias를 추가하지만, Bootstrapping TD 방법이 종종 Monte Carlo Method보다 Variance가 낮다는 측면에서 우수하기 때문에 때때로 더 우수합니다.&lt;/p&gt;

&lt;p&gt;정리하자면, Policy Gradient Method는 Action-Value 방법과는 상당히 다른 장점과 단점을 지닙니다. 현재 이 분야는 아직 활발하게 연구되는 주제이기 때문에, 앞으로 더 흥미로운 결과가 나오는 것을 기대하고 있습니다.&lt;/p&gt;

&lt;p&gt;이로써 길고 길었던 강화학습 포스팅이 얼추 마무리가 되었습니다. 당분간 새로운 이론적인 포스트를 작성하기 보다는, 쉬면서 작성했던 포스트를 검토하여 부족했던 부분을 수정하거나 추가하도록 하겠습니다. 지금까지 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장은 드디어 마지막 장인 Policy Gradient입니다. 이번 장에서는 지금까지 이 교재에서 다룬 방법들과는 다르게, Policy 자체를 매개변수화하는 방법을 알아보겠습니다. 지금까지의 방법들은 Estimated Action-Value를 기반으로 Action을 선택했기 때문에 Action-Value를 추정하는 것이 중요했습니다. 하지만 이번 장에서 배울 새로운 방법인 Policy Gradient는 Action을 선택하는 데 Value Function을 사용하지 않습니다. 이번 장에서 사용할 새로운 표기는 Policy에 대한 매개변수 벡터인 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$입니다. 따라서 Policy는 이제 매개변수 $\boldsymbol{\theta}$를 포함하여 $\pi (a \mid s, \boldsymbol{\theta}) = Pr \{ A_t = a \mid S_t = s, \boldsymbol{\theta}_t = \boldsymbol{\theta} \}$로 표현합니다. 이것은 시간 $t$에서 State가 $s$이고 매개변수가 $\boldsymbol{\theta}$일 때 Action $a$를 선택할 확률로 정의됩니다. 만약 학습 알고리즘 안에서 Value Function에 대한 추정을 포함하는 경우, 이전과 마찬가지로 여전히 Weight Vector $\mathbf{w} \in \mathbb{R}^d$를 포함하여 $\hat{v}(s, \mathbf{w})$로 표현합니다.</summary></entry><entry><title type="html">Eligibility Trace</title><link href="http://localhost:4000/rl/eligibility-traces/" rel="alternate" type="text/html" title="Eligibility Trace" /><published>2022-09-22T00:00:00+09:00</published><updated>2022-09-22T00:00:00+09:00</updated><id>http://localhost:4000/rl/eligibility-traces</id><content type="html" xml:base="http://localhost:4000/rl/eligibility-traces/">&lt;p&gt;이번 장에서 새로 배우는 &lt;span style=&quot;color:red&quot;&gt;Eligibility Trace&lt;/span&gt;는 강화학습의 기본 메커니즘 중 하나입니다. 예를 들어, TD($\lambda$)에서 $\lambda$는 Eligibility Trace를 사용한다는 것을 의미합니다. Q-learning과 Sarsa를 포함한 대부분의 TD 방법은 Eligibility Trace와 결합하여 보다 효율적으로 학습할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace는 TD와 Monte Carlo Method를 통합하여 일반화하는 방법입니다. TD 방법을 Eligibility Trace를 사용하여 일반화하면 $\lambda = 1$일 때 Monte Carlo Method처럼 동작하며, $\lambda = 0$일 때 1-step TD로 동작합니다. 이로 인해 Eligibility Trace는 온라인으로 Monte Carlo Method을 구현할 수 있고, Episode가 없는 Continuing Problem에 대한 학습 방법을 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace의 메커니즘을 간단히 설명하자면, Eligibility Trace의 Short-term Memory Vector $\mathbf{z}_t \in \mathbb{R}^d$는 Long-term Weight Vector $\mathbf{w}_t \in \mathbb{R}^d$와 평행합니다. $\mathbf{w}_t$를 통해 함수를 추정할 때, $\mathbf{z}_t$의 구성 요소와 충돌한 후, $\mathbf{z}_t$는 사라지기 시작합니다. 이 &lt;strong&gt;Trace&lt;/strong&gt;가 0으로 감소하기 전에 0이 아닌 TD Error가 발생하면, $\mathbf{w}_t$의 해당 구성 요소에서 학습이 일어납니다. 이 때 $\lambda \in \left[ 0, 1 \right]$는 Trace가 얼마나 빨리 0으로 감소하는 지 나타내는 Trace-decay Parameter입니다.&lt;/p&gt;

&lt;p&gt;그런데 우리는 이미 7장에서 Monte Carlo와 1-step TD를 조율한 $n$-step TD를 배웠습니다. 하지만 Eligibility Trace는 $n$-step TD에 비해 계산적으로 이점이 있습니다. $n$-step TD는 마지막 $n$개의 Feature Vector를 저장했지만, Eligibility Trace는 1개의 Trace Vector만 필요합니다. 또한 $n$-step TD에서의 학습은 Episode가 끝나기 전까지 지연되는 방식이지만, Eligibility Trace는 지속적이고 균일하게 학습이 일어납니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 통해 학습 알고리즘은 계산상의 이점을 위해 때때로 다른 방법으로 구현될 수도 있다는 것을 보여줍니다. 기존 방법을 예로 들자면, Monte Carlo Method와 $n$-step TD는 Episode의 마지막부터 Episode의 처음까지 학습하거나, $n$-step만큼 학습하였습니다. 이것을 &lt;strong&gt;Forward View&lt;/strong&gt;라고 하는데, Forward View는 막상 알고리즘을 수행할 때 바로 사용할 수 없는 미래의 요소에 의존하기 때문에 구현하는 것이 상당히 복잡합니다. 그러나 Eligibility Trace는 알고리즘이 수행하는 순서와 거의 동일한 Update를 구현할 수 있습니다. 이것을 Backward View라고 합니다. 이번 장에서 이것에 대해 조금 더 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;이번 장 역시 이전 장들과 마찬가지로, State-Value의 Prediction에 대한 개념을 먼저 다룬 다음에, Action-Value 및 Control 문제로 확장합니다. 또한 마찬가지로 On-policy 학습을 먼저, Off-policy 학습을 나중에 다룰 예정입니다. Function Approximation은 Linear Function Approximation에 중점을 둘 것이며, Tabular 방법과 State Aggregation 경우에도 적용할 수 있다는 것을 보일 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-lambda-return&quot;&gt;The $\lambda$-Return&lt;/h2&gt;

&lt;p&gt;먼저 7장에서 배운 $n$-step Return을 복습해봅시다. 식 (7.1)에서 $n$-step Return은 처음 $n$개의 Discounted Reward와 방문한 State의 Estimated Value의 합으로 정의했습니다. 이 식을 매개변수를 사용한 Function Approximation 식으로 수정하면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{v} \left( S_{t+n}, \mathbf{w}_{t+n-1} \right), \quad 0 \le t \le T-n \tag{12.1}\]

&lt;p&gt;식 (12.1)에서 $\hat{v} \left( s, \mathbf{w} \right)$는 Weight Vector $\mathbf{w}$가 주어졌을 때 State $s$의 근사값이고, $T$는 Episode가 종료되는 시간입니다.&lt;/p&gt;

&lt;p&gt;또한 이러한 Update는 $n$-step Return 뿐만 아니라 다른 모든 $n$에 대한 $n$-step Return의 평균에 대해서도 유효합니다. 예를 들자면, 2-step Return의 절반과 4-step Return의 절반의 합으로 구성된 $\frac{1}{2} G_{t:t+2} + \frac{1}{2} G_{t:t+4}$와 같은 식에 대해서도 Update를 수행할 수 있다는 것입니다. 이렇게 각 Return의 Weight가 양수이면서 합이 1인 조건 하에서는 모든 $n$-step Return이 이런 방식으로 평균을 낼 수 있습니다. 심지어 항의 개수가 무한해도 말입니다. 이러한 평균화 기법을 이용하면 새로운 알고리즘을 개발할 수 있습니다. 예를 들어, TD나 Monte Carlo Method를 연결하기 위해 1-step 및 무한 단계 Return을 평균화하는 식으로 말입니다. 이와 같이 간단하게 구성 요소를 Update 하는 평균화 기법을 &lt;span style=&quot;color:red&quot;&gt;Compound Update&lt;/span&gt; 라고 합니다. 이에 대한 Backup Diagram은 Update 식에 따라 달라지는데, 방금 다룬 2-step Return의 절반과 4-step Return의 절반을 합친 식에 대한 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Backup Diagram에서 볼 수 있듯이, Compound Update의 Update를 수행하기 위해서는 가장 긴 구성 요소의 Update가 완료되어야 수행할 수 있습니다. 예를 들어, 위의 Backup Diagram에서 가장 긴 구성 요소는 4-step Return이므로, 시간 $t$에서의 추정치는 시간 $t+4$에 도달해야만 추정이 가능합니다. 이러한 문제로 인해 Update에 지연이 발생할 수 있으므로, 일반적으로는 가장 긴 구성 요소의 길이를 제한하는 방식으로 해결합니다.&lt;/p&gt;

&lt;p&gt;$n$-step update를 평균화하는 방법 중 대표적으로 &lt;span style=&quot;color:red&quot;&gt;TD($\lambda$)&lt;/span&gt;가 있습니다. 이 방법은 모든 $n$-step update에 대해 $\lambda^{n-1}$의 Weight를 부여한 평균화 방법입니다. 그리고 이 Weight의 합을 1로 만들기 위해 맨 앞에 $1 - \lambda$를 곱해줍니다. 이것을 식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+n} \tag{12.2}\]

&lt;p&gt;식 (12.2)의 Update 식을 &lt;span style=&quot;color:red&quot;&gt;$\lambda$-Return&lt;/span&gt; 이라고 합니다. $\lambda$-Return의 Backup Diagram은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\lambda$-Return에서는 각 항의 계수가 다르기 때문에 Weight 또한 항 마다 다릅니다. 예를 들어, 1-step Return의 계수는 $(1 - \lambda)$이지만, 2-step Return은 $(1 - \lambda) \lambda$의 계수를 갖습니다. 이렇게 시간 $t$를 기준으로 멀어질수록 $\lambda$를 곱하기 때문에 Weight가 낮아집니다. 이것을 그림으로 표현하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 보시다피시 마지막 항의 계수만 다른 항과 표현 방식이 다르기 때문에, 식 (12.2)를 다음과 같이 마지막 항만 분리하여 표현할 수도 있습니다.&lt;/p&gt;

\[G_t^{\lambda} = (1 - \lambda) \sum_{n=1}^{T-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{T-t-1}G_t \tag{12.3}\]

&lt;p&gt;식 (12.3)에서 $\lambda = 1$인 경우를 따져봅시다. $(1 - \lambda)$ 항이 0이 되므로 2번째 항만 살아남아 Monte Carlo Return이 됩니다. 반대로 $\lambda = 0$인 경우라면 2번째 항이 사라짐은 물론, 1번째 항의 첫 번째 Return을 제외하고 모두 0이 되기 때문에 1-step Return이 됩니다. 따라서 $\lambda = 0$인 경우라면 $\lambda$-Return은 1-step TD 방법이 된다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이제 $\lambda$-Return을 기반으로 만든 첫 번째 학습 알고리즘인 &lt;span style=&quot;color:red&quot;&gt;Off-line $\lambda$-Return Algorithm&lt;/span&gt;을 소개하겠습니다. &lt;strong&gt;Off-line&lt;/strong&gt; Algorithm이므로 Episode가 수행되는 동안에는 Weight Vector가 변경되지 않습니다. 그 후 Episode가 끝날 때 $\lambda$-Return을 Target으로 사용하여 일반적인 Semi-gradient Rule에 따라 Off-line Update의 전체 과정을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G_t^{\lambda} - \hat{v} (S_t, \mathbf{w}) \right] \nabla \hat{v} (S_t, \mathbf{w}), \quad t = 0, \ldots, T - 1 \tag{12.4}\]

&lt;p&gt;$\lambda$-Return은 7장에서 배웠던 $n$-step bootstrapping 방법과 다른 방법으로 Monte Carlo와 1-step TD 사이를 조절할 수 있는 대안을 제시합니다. 이에 대한 예시로 교재에서는 Random Walk Example을 제시하는데, 이 예제를 제가 7장에서 소개하지 않았습니다. 일단 여기에서 비교 내용만 설명하고, 추후 7장에 이 예제를 추가하겠습니다.&lt;/p&gt;

&lt;p&gt;아래 그림은 19개의 State를 가진 Random Walk Example에서의 $\lambda$-Return Algorithm과 $n$-step TD 방법 비교 그래프입니다. 두 방법 모두 처음 10개의 Episode에 대한 평균을 나타내며 그래프의 세로축은 Root Mean Square Error를 의미하기 때문에 낮을 수록 좋습니다. 그래프를 보면 두 방법 모두 성능이 비슷하며, 중간 정도의 $\lambda$와 $n$일 때 최적의 성능을 보임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 지금까지 취한 접근 방식은 학습 알고리즘에 대한 Theoretical View, 혹은 Forward View라고 부를 수 있습니다. 방문하는 각 State에 대해 향후 얻을 수 있는 모든 Reward에 대한 기대값과 이를 결합하는 최선의 방법을 결정하기 때문입니다. 아래 그림과 같이 Update를 결정하기 위해 각 State에서 기다리면서 State의 흐름을 타고 있다고 볼 수 있습니다. 한 State를 Update 한 후, 다음 State로 이동한 후에는 이 작업을 반복할 필요가 없습니다. 반면에 미래 State는 이전의 유리한 지점에서 한 번씩 반복적으로 처리됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tdlambda&quot;&gt;TD($\lambda$)&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;TD($\lambda$)&lt;/span&gt;는 강화학습에서 가장 오래되고 널리 사용되는 알고리즘 중 하나입니다. TD($\lambda$)는 Eligibility Trace를 사용하여 Forward View와 Backward View 사이의 형식적인 관계를 나타내는 최초의 알고리즘입니다. Forward View는 이론적인 면에서, Backward View는 계산적인 면에서 각각 이점이 있습니다. 이번 Section에서는 TD($\lambda$)가 이전 Section에서 배운 Off-line $\lambda$-Return Algorithm에 근접함을 경험적으로 보여줄 것입니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)는 Off-line Return Algorithm을 세 가지 방식으로 개선합니다. 첫째, Episode가 끝날 때 뿐만 아니라 Episode의 모든 단계에서 Weight Vector를 Update함으로써 추정치를 더 빠르게 계산합니다. 둘째, Episode가 끝낼 때 한번에 계산하지 않고, 시간에 따라 계산이 균등하게 분산됩니다. 셋째, Episodic Task 뿐만 아니라 Continuing Task에도 적용할 수 있습니다. 이번 Section에서는 Function Approximation을 사용하여 TD($\lambda$)의 Semi-gradient 버전을 제시합니다.&lt;/p&gt;

&lt;p&gt;Function Approximation에서 Eligibility Trace는 Weight Vector $\mathbf{w}_t$와 동일한 수의 구성 요소를 갖는 Vector $\mathbf{z}_t \in \mathbb{R}^d$입니다. Weight Vector는 시스템의 전체 수명 동안 누적되는 Long-term Memory이지만, Eligibility Trace는 일반적으로 Episode의 길이보다 짧은 시간 동안만 지속되는 Short-term Memory입니다. Eligibility Trace의 결과는 Weight Vector에 영향을 미치고 Weight Vector가 추정한 값을 결정함으로써 학습 과정에 도움이 됩니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)에서 Eligibility Trace의 Vector는 Episode 시작 시 Zero Vector로 초기화되고, 각 시간 단계에서 이전 Vector의 $\gamma \lambda$만큼 감소한 후, Value Function의 Gradient만큼 증가합니다. 이것을 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_{-1} &amp;amp; \doteq \mathbf{0} \\ \\
\mathbf{z}_t &amp;amp; \doteq  \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{v} (S_t, \mathbf{w}_t), \quad 0 \le t \le T \tag{12.5}
\end{align}\]

&lt;p&gt;식 (12.5)에서 $\gamma$는 Discount Factor이고, $\lambda$는 이전 Section에서 소개한 매개변수인데, 앞으로 이것을 Trace-decay 매개변수라고 부르겠습니다. Eligibility Trace는 Weight Vector의 어떤 구성 요소가 최근 State에 대한 평가에 긍정적으로/부정적으로 기여했는지 &lt;strong&gt;Trace&lt;/strong&gt;합니다. 여기서 &lt;strong&gt;최근&lt;/strong&gt;은 $\gamma \lambda$로 정의됩니다. Trace는 학습 이벤트가 발생할 경우 그것에 의해 변경이 일어날 수 있는 Weight Vector에서 각 구성 요소들의 &lt;strong&gt;Eligibility&lt;/strong&gt;를 나타냅니다. 여기서 우리가 우려할 수 있는 학습 이벤트는 매 순간의 1-step TD Error입니다. State-Value Prediction에 대한 TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t \doteq R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v} (S_t, \mathbf{w}_t) \tag{12.6}\]

&lt;p&gt;TD($\lambda$)에서 Weight Vector는 Scalar TD Error 및 Vector에 대한 Eligibility Trace에 비례하여 각 단계에서 다음과 같이 Update됩니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t \tag{12.7}\]

&lt;p&gt;Semi-gradient TD($\lambda$)의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TD($\lambda$)는 시간적으로 Backward View라고 볼 수 있습니다. 매 순간 현재의 TD Error를 확인하고, 그 State가 당시 Eligibility Trace에 얼마나 기여했는지에 따라 각각의 이전 State에 거꾸로 반영합니다. State가 미래에 다시 발생할 때를 대비하여 아래 그림과 같이 State의 흐름과 TD Error를 계산하고 식 (12.7)에 의해 얻은 Update를 이용하여 과거의 Value를 변경합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것을 조금 더 잘 이해하기 위해서는 $\lambda$에 값에 따라 어떻게 달라지는지 생각해보면 됩니다. 만약 $\lambda = 0$인 경우라면 식 (12.5)에 의해 시간 $t$ 에서의 Trace는 정확히 State $S_t$에서 Value의 Gradient와 같습니다. 따라서 이 때의 TD($\lambda$) Update인 식 (12.7)은 9장에서 배운 1-step TD Update와 동일합니다. 이것이 그 당시 1-step TD Update를 TD(0)로도 불렀던 이유입니다. 위의 그림을 토대로 설명하자면, TD(0)는 현재 State를 기준으로 한 단계 이전의 State에 대한 TD Error로만 Update하는 경우입니다. 하지만 만약 $\lambda &amp;lt; 1$ 조건 하에 $\lambda$의 값이 증가한다면 더 많은 이전 State들이 Update되는데, 그림에서 볼 수 있듯이 시간적으로 멀리 떨어진 State일수록 Eligibility Trace가 더 작기 때문에 덜 Update됩니다. 이것을 &lt;strong&gt;초기 State는 TD Error에 대해 더 적은 Credit을 받았다&lt;/strong&gt;라고 표현하기도 합니다.&lt;/p&gt;

&lt;p&gt;만약 $\lambda = 1$인 경우라면, 이전 State에 부여된 Credit은 단계당 $\gamma$만큼 떨어집니다. 예를 들어, TD Error $\delta_t$는 Discount 되지 않은 $R_{t+1}$를 포함합니다. 그리고 이전의 $k$ 단계에 대한 Return을 계산할 때는 Reward에 $\gamma^k$ 만큼의 Discount가 곱해지는데, 이것은 점점 감소하는 Eligibility Trace가 됩니다. 만약 $\lambda = 1$이고 $\gamma = 1$일 때는 시간적으로 아무리 떨어져 있더라도 Eligibility Trace가 소멸되지 않습니다. 이 경우에는 Discount가 없는 Episodic Task에 대한 Monte Carlo Method처럼 작동합니다. $\lambda = 1$인 경우 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;TD(1)&lt;/span&gt;으로도 부릅니다.&lt;/p&gt;

&lt;p&gt;TD(1)은 기존의 Monte Carlo Method를 더 일반적으로 구현한 방법입니다. 기존의 Monte Carlo Method는 Episodic Task에 한정되었지만, TD(1)은 Discounted Continuing Task에도 적용할 수 있습니다. 또한 TD(1)은 점진적으로, 온라인으로 수행할 수도 있습니다. Monte Carlo Method는 Episode가 끝날 때까지 아무것도 학습하지 못한다는 단점이 있지만, TD(1)는 Episode가 끝나지 않은 상황에서도 그 일부분을 $n$-step TD 방식으로 학습할 수 있다는 장점이 있습니다. 예를 들어, 만약 Episode 중 비정상적으로 좋거나 나쁜 일이 발생하면 TD(1)에 기반한 Control은 즉시 이전까지의 내용을 학습하고 Episode를 변경할 수 있습니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)가 Off-line $\lambda$-Return Algorithm을 근사하는데 얼마나 성능이 좋은지 알아보기 위해 또 다시 19개의 State를 가진 Random Walk Example을 놓고 비교해보겠습니다. 아래 그림을 보시면 그래프의 모양 자체는 차이가 있지만, $\lambda$의 값이 최적인 State에서는 거의 동일한 성능을 보임을 알 수 있습니다. 다만 $\lambda$가 최적보다 크게 선택되는 상황을 보면 TD($\lambda$)는 Off-line $\lambda$-Return Algorithm보다 성능이 더 나쁘다는 것을 알 수 있습니다. 일반적으로 최적의 State를 제외하고는 $\lambda$를 사용하지 않기 때문에 큰 문제는 아닙니다만, TD($\lambda$)가 &lt;strong&gt;더 불안정하다&lt;/strong&gt;라고는 말할 수 있을 정도의 단점은 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Linear TD($\lambda$)는 On-policy인 경우 조건 식 (2.7)에 따라 Step-size Parameter가 시간에 따라 감소한다면 수렴합니다. Section 9.4에서 다룬 바와 같이 수렴한다는 것은 Weight Vector의 최소 오차가 $\lambda$에 따른다는 것을 의미합니다. 식 (9.14)에서 배운 오차 한계식은 $\lambda$에 의해 일반화될 수 있습니다. 만약 Discounted Continuing Task라면, 다음과 같습니다.&lt;/p&gt;

\[\overline{\text{VE}}(\mathbf{w}_{\text{TD}}) \le \frac{1 - \gamma \lambda}{1 - \gamma} \min_{\mathbf{w}} \overline{\text{VE}}(\mathbf{w}) \tag{12.8}\]

&lt;p&gt;즉, 점근적인 오차는 가능한 최소 오차의 $\frac{1 - \gamma \lambda}{1 - \gamma}$배를 넘지 않는 다는 뜻입니다. $\lambda$가 1에 가까워질수록 최소 오차에 가까워집니다. 이렇게 보면 $\lambda$를 1에 가깝게 잡는 것이 좋아보이지만, 실제로는 가장 좋지 않은 선택이 될 가능성이 높은데, 그 이유는 나중에 밝혀집니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-truncated-lambda-return-methods&quot;&gt;$n$-step Truncated $\lambda$-Return Methods&lt;/h2&gt;

&lt;p&gt;Off-line $\lambda$-Return Algorithm은 중요하지만, Episode가 끝날 때까지 알 수 없는 $\lambda$-Return을 이용하기 때문에 효용이 제한적입니다. (식 12.2 참고) Continuing Task의 경우, $\lambda$-Return은 기술적으로 계산할 수 없기 때문에 임의적으로 큰 $n$에 대해 $n$-step Return에 의존합니다. 하지만 시간적으로 멀리 떨어진 Reward일수록 $\gamma \lambda$만큼의 비율로 계속 비중이 줄어들기 때문에, 이 경우 근사를 하기 위해서는 일정 Step 마다 구간을 나누는 것이 좋습니다. $n$-step Return은 누락된 Reward를 추정된 값으로 대체하는 개념을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;Episode를 일정 구간인 $h$만큼 자르는 경우, 시간 $t$에 대한 Truncated $\lambda$-Return 식은 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[G_{t:h}^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{h-t-1}G_{t:h}, \quad 0 \le t &amp;lt; h \le T \tag{12.9}\]

&lt;p&gt;이 식은 $h$의 역할이 식 (12.3)에서 $T$의 역할과 동일함을 알 수 있습니다. 또 다른 차이점을 굳이 찾자면 두 번째 항의 $G_t$ 대신 $G_{t:h}$로 변경된 것 정도가 있고, 그 외에는 식 (12.3)과 동일합니다.&lt;/p&gt;

&lt;p&gt;Truncated $\lambda$-Return은 7장에서의 $n$-step 방법과 유사한 $n$-step Return Algorithm을 즉시 생성하는 방식으로 구성됩니다. 이 때의 Update는 $n$-step 만큼 지연되고 처음 $n$-step만 고려되었지만, 이제는 모든 $k$-step ($1 \le k \le n$) Return이 포함됩니다. State-Value의 경우 이 알고리즘과 같은 종류를 &lt;span style=&quot;color:red&quot;&gt;Truncated TD($\lambda$)&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;TTD($\lambda$)&lt;/span&gt;라고 부릅니다. 아래 그림의 복합적인 Backup Diagram은 가장 긴 구성 요소에 대한 Update가 항상 Episode의 끝까지 진행되는 것이 아니라 최대 $n$-step이라는 점을 명시하고 있습니다. 그 점을 제외한다면 이전에 보여드린 $\lambda$-Return의 Backup Diagram과 유사합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TTD($\lambda$)에 대한 식은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \left[ G_{t:t+n}^{\lambda} - \hat{v} (S_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{v} (S_t, \mathbf{w}_{t+n-1}), \quad 0 \le t &amp;lt; T\]

&lt;p&gt;이 알고리즘은 각 단계별 계산이 $n$으로 확장되지 않도록 효율적으로 구현할 수 있습니다. (즉, 시간 복잡도가 $n$에 비례하지 않도록) $n$-step TD 방법과 마찬가지로 각 Episode의 처음 $n-1$ 시간 단계에서는 Update가 수행되지 않으며, Episode가 종료 후 $n-1$에 대한 추가적인 Update가 수행됩니다. 효율적인 구현을 위해 $k$-step $\lambda$-Return은 다음과 같이 표현으로 수정할 수 있습니다.&lt;/p&gt;

\[G_{t:t+k}^{\lambda} = \hat{v} (S_t, \mathbf{w}_{t-1}) + \sum_{i=t}^{t+k-1} (\gamma \lambda)^{i-t} \delta_i &apos; \tag{12.10}\]

&lt;p&gt;이 때, $\delta_i^{\prime} \doteq R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v} (S_t, \mathbf{w}_{t-1})$입니다.&lt;/p&gt;

&lt;h2 id=&quot;redoing-updates-online-lambda-return-algorithm&quot;&gt;Redoing Updates: Online $\lambda$-Return Algorithm&lt;/h2&gt;

&lt;p&gt;Truncated TD($\lambda$)에서 Truncation Parameter $n$을 선택할 때는 Trade-off가 있습니다. Truncated TD($\lambda$)가 Off-line $\lambda$-Return Algorithm에 근접하기 위해서 $n$이 커야 하지만, Update가 더 빨리 이루어지기 위해서는 $n$이 작아야 합니다. 이런 상황에서 둘 다 포기하지 않는 방법은 있지만, 그만큼 계산 복잡도가 증가하는 문제가 있습니다. 이번 Section에서는 이 방법을 소개하겠습니다.&lt;/p&gt;

&lt;p&gt;기본적인 아이디어는 새로운 데이터의 Increment를 얻을 때마다 현재 Episode의 시작 부분으로 되돌아가 모든 Update를 다시 실행하는 것입니다. 그러면 각 시간 단계에서 새로운 데이터를 고려할 수 있기 때문에 새 Update는 이전에 계산한 결과보다 더 나을 것이기 때문입니다. 즉, Update는 항상 최신 Horizon $h$를 사용하여 $n$-step Truncated $\lambda$-Return의 Target을 계산하는 것입니다. 각 Episode가 끝날 때마다 약간 더 긴 Horizon $h$를 사용하면 약간 더 나은 결과를 얻을 수 있습니다. 먼저, 식 (12.9)에서의 Truncated $\lambda$-Return은 다음과 같이 정의했었습니다.&lt;/p&gt;

\[G_{t:h}^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{h-t-1}G_{t:h}\]

&lt;p&gt;계산 복잡도가 문제가 되지 않는 상황에서 이 Target을 이상적으로 사용할 수 있는 방법을 단계별로 살펴보겠습니다. 각각의 Episode는 이전 Episode의 끝에서 $\mathbf{w}_0$을 사용하여 시간 0에서의 추정으로 시작합니다. 데이터 Horizon이 시간 단계 1로 확장될 때 학습이 시작됩니다. Horizon 1까지의 데이터가 주어지면, 시간 단계 0에서의 추정 Target은 $R_1$과 추정치 $\hat{v}(S_1, \mathbf{w}_0)$의 Bootstrap을 포함한 1-step Return $G_{0:1}$입니다. 이것은 정확하게 $G_{0:1}^{\lambda}$이며, 위 식의 첫 번째 항의 합은 0으로 감소합니다. 그 후 이 Target Update를 사용하여 $\mathbf{w}_1$을 만듭니다. 그 두 데이터 Horizon을 시간 단계 2로 진행한 후, $R_2$, $S_2$를 얻을 수 있으므로 $S_0$의 더 나은 Update인 $G_{0:2}^{\lambda}$와 $S_1$의 Update인 $G_{1:2}^{\lambda}$를 계산할 수 있습니다. 이렇게 더 나아진 Target을 사용하여 $S_1$과 $S_2$를 다시 Update하고, Weight Update를 $\mathbf{w}_0$부터 시작하여 $\mathbf{w}_2$를 계산합니다. 데이터 Horizon이 시간 단계 3으로 넘어가면 이 과정을 또다시 반복하는 것입니다. 이렇게 새로운 데이터 Horizon을 얻을 때마다 Weight Update를 $\mathbf{w}_0$부터 다시 계산하여 Update를 수행합니다.&lt;/p&gt;

&lt;p&gt;이러한 개념적인 알고리즘은 각각의 Horizon $h$에서 동일한 Episode에 대한 서로 다른 Weight Vector를 생성합니다. 이것을 명확하게 설명하기 위해서는 다른 Horizon에서 계산된 Weight Vector를 구별할 수 있어야 합니다. Horizon $h$까지의 과정에서 시간 $t$의 Value를 추정하는데 사용한 Weight를 $\mathbf{w}_t^h$라 합시다. 각 과정에서의 첫 번째 Weight Vector $\mathbf{w}_0^h$는 이전 Episode로부터 상속된 것이고 (모든 $h$에 대해 마찬가지), 각 과정의 마지막 Weight Vector $\mathbf{w}_h^h$는 알고리즘의 궁극적인 Weight Vector를 정의합니다. 마지막 Horizon $h = T$에서는 다음 Episode의 초기 Weight를 생성하기 위해 전달할 최종 Weight $\mathbf{w}_T^T$를 얻습니다. 이러한 과정을 $h = 3$까지 수식으로 표현하자면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
h = 1 : \mathbf{w}_1^1 \doteq \mathbf{w}_0^1 + \alpha \left[ G_{0:1}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^1) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^1) \\ \\
h = 2 : \mathbf{w}_1^2 \doteq \mathbf{w}_0^2 + \alpha \left[ G_{0:2}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^2) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^2) \\ \\
\quad \mathbf{w}_2^2 \doteq \mathbf{w}_1^2 + \alpha \left[ G_{1:2}^{\lambda} - \hat{v} (S_1, \mathbf{w}_1^2) \right] \nabla \hat{v} (S_1, \mathbf{w}_1^2) \\ \\
h = 3 : \mathbf{w}_1^3 \doteq \mathbf{w}_0^3 + \alpha \left[ G_{0:3}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^3) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^3) \\ \\
\quad \mathbf{w}_2^3 \doteq \mathbf{w}_1^3 + \alpha \left[ G_{1:3}^{\lambda} - \hat{v} (S_1, \mathbf{w}_1^3) \right] \nabla \hat{v} (S_1, \mathbf{w}_1^3) \\ \\
\quad \mathbf{w}_3^3 \doteq \mathbf{w}_2^3 + \alpha \left[ G_{2:3}^{\lambda} - \hat{v} (S_2, \mathbf{w}_2^3) \right] \nabla \hat{v} (S_2, \mathbf{w}_2^3)
\end{align}\]

&lt;p&gt;이것을 일반화하면 다음과 같은 수식을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1}^h \doteq \mathbf{w}_t^h + \alpha \left[ G_{t:h}^{\lambda} - \hat{v} (S_t, \mathbf{w}_t^h) \right] \nabla \hat{v} (S_t, \mathbf{w}_t^h), \quad 0 \le t &amp;lt; h \le T\]

&lt;p&gt;만약 $\mathbf{w}_t \doteq \mathbf{w}_t^t$로 정의된다면 &lt;span style=&quot;color:red&quot;&gt;On-line $\lambda$-Return Algorithm&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;On-line $\lambda$-Return Algorithm은 완전하게 On-line으로 동작하며, 시간 $t$에서 사용할 수 있는 정보만 사용하여 새로운 Weight Vector $\mathbf{w}_t$를 계산합니다. 이 때의 단점은 매 시간 단계마다 경험한 Episode의 일부를 사용하여 계산하는 것이 계산 복잡도가 높다는 것입니다. Off-line $\lambda$-Return Algorithm은 Episode를 수행하는 동안 Update를 수행하지 않고, Episode를 종료하는 시점에서 모든 시간 단계에 대한 Update를 수행했기 때문에 계산 복잡도가 높지 않았습니다. On-line $\lambda$-Return Algorithm은 그 계산 복잡도를 대가로 Episode가 진행되는 도중 뿐만 아니라 Episode가 끝날 때도 더 나은 성능을 기대할 수 있습니다. Bootrstrapping에 사용되는 Weight Vector에 반영되는 정보가 더 많기 때문입니다. 아래 그림은 지금까지 보았던 Random Walk Example에서 On-line과 Off-line 알고리즘을 비교한 그래프입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;true-online-tdlambda&quot;&gt;True Online TD($\lambda$)&lt;/h2&gt;

&lt;p&gt;이전 Section에서 제시한 On-line $\lambda$-Return Algorithm은 현재 가장 성능이 좋은 시분할 알고리즘입니다. 즉, On-line TD($\lambda$)를 근사화하는 이상적인 알고리즘입니다. On-line $\lambda$-Return Algorithm은 Forward View Algorithm이지만, 효율적으로 구현하기 위해서 Backward View Algorithm으로 변형시킬 방법이 있을까요? Linear Function Approximation의 경우라면 그 대답은 Yes입니다. 이 구현은 TD($\lambda$) Algorithm보다 On-line $\lambda$-Return Algorithm에 이상적으로 가깝기 때문에 &lt;span style=&quot;color:red&quot;&gt;True On-line TD($\lambda$) Algorithm&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)를 유도하는 과정을 여기에서 보이기에는 너무 복잡하기 때문에 여기에서는 생략하겠습니다. (다음 Section 및 van Seijen et al., 2016) 대략적인 아이디어를 소개하자면, 먼저 On-line $\lambda$-Return Algorithm은 다음과 같이 삼각형으로 나열할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 삼각형에서 하나의 행은 각 시간 단계에서 생성됩니다. 삼각형을 구성하는 요소는 많지만, 이전 Section에서 보았듯이 우리에게 필요한 것은 대각선 요소인 $\mathbf{w}_t^t$뿐입니다. 첫 번째 요소인 $\mathbf{w}_0^0$는 Episode의 초기 Weight Vector이고, 마지막 요소인 $\mathbf{w}_T^T$는 최종 Weight Vector이며, 그 중간 요소인 $\mathbf{w}_t^t$는 Update에 필요한 $n$-step Return을 얻기 위한 Bootstrapping 역할을 합니다.&lt;/p&gt;

&lt;p&gt;이제 삼각형의 대각선 구성 요소(가장 오른쪽 요소)는 표기의 편의를 위해 $\mathbf{w}_t \doteq \mathbf{w}_t^t$로 재정의하겠습니다. 이제 해야할 것은 대각선 구성 요소인 $\mathbf{w}_t$를 간결하고 효율적으로 계산하는 방법을 찾는 것입니다. 그렇게 하면 $\hat{v}(\mathbf{s}, \mathbf{w}) = \mathbf{w}^{\sf T} \mathbf{x} (\mathbf{s})$와 같은 Linear에 대해 다음과 같은 True On-line TD($\lambda$) Algorithm을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t + \alpha \left( \mathbf{w}_t^{\sf T} \mathbf{w}_t - \mathbf{w}_{t-1}^{\sf T} \mathbf{x}_t \right) \left( \mathbf{z}_t - \mathbf{x}_t \right),\]

&lt;p&gt;위 식에서 $\mathbf{x}_t \doteq \mathbf{x} (S_t)$이며, $\delta_t$는 TD($\lambda$)인 식 (12.6)을 의미합니다. 또한 $\mathbf{z}_t$는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma \lambda \mathbf{z}_{t-1} + \left( 1 - \alpha \gamma \lambda \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t \tag{12.11}\]

&lt;p&gt;이 알고리즘은 On-line $\lambda$-Return Algorithm과 정확하게 동일한 Weight Vector $\mathbf{w}_t (0 \le t \le T)$를 생성하는 것으로 증명되었습니다. (van Seijen et al., 2016) 이전 Section에서의 마지막 그림인 Random Walk의 On-line $\lambda$-Return Algorithm도 이것을 사용한 결과입니다. 이제 지금까지 단점으로 남아있던 높은 계산 복잡도가 해결되었습니다. 공간 복잡도 측면에서 보면 On-line TD($\lambda$)의 메모리 요구량은 기존 TD($\lambda$)의 메모리 요구량과 동일하고, 시간 복잡도 측면에서 보면 각 단계별 계산량은 약 50% 증가했지만, 전체적으로 보았을 때 각 단계별 시간 복잡도는 TD($\lambda$)와 동일하게 $O(d)$로 유지됩니다.&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)에 사용된 Eligibility Trace 식 (12.11)은 기존의 TD($\lambda$)에서 사용한 Eligibility Trace 식 (12.5)와 구분하기 위해 &lt;span style=&quot;color:red&quot;&gt;Dutch Trace&lt;/span&gt;라고 부릅니다. 참고로 식 (12.5)와 같은 Eligibility Trace은 Accumulating Trace라고 부르기도 합니다.&lt;/p&gt;

&lt;p&gt;이전에는 Tabular 방법이나 Tile Coding과 같은 Binary Feature Vector에 대해서는 &lt;span style=&quot;color:red&quot;&gt;Replacing Trace&lt;/span&gt;라고 하는 또 다른 방법을 사용했었습니다. Replacing Trace는 Feature Vector의 구성 요소가 1인지, 0인지에 따라 다르게 정의됩니다.&lt;/p&gt;

\[z_{i, t} \doteq \begin{cases} 1, &amp;amp; \text{if } x_{i, t} = 1 \\ \gamma \lambda z_{i, t-1}, &amp;amp; \text{otherwise} \end{cases} \tag{12.12}\]

&lt;p&gt;요즘에는 Replacing Trace를 Dutch Trace의 조잡한 근사치 정도로 간주합니다. Replacing Trace는 일반적으로 Dutch Trace보다 성능이 낮기 때문에 Dutch Trace로 이를 대체하는 경우가 많습니다. 물론, 이에 대한 이론적인 근거 또한 있습니다. Accumulating Trace는 Dutch Trace를 사용할 수 없는 non-Linear Function Approximation에서 사용하기 때문에 중요한 Trace로 간주합니다.&lt;/p&gt;

&lt;h2 id=&quot;dutch-traces-in-monte-carlo-learning&quot;&gt;Dutch Traces in Monte Carlo Learning&lt;/h2&gt;

&lt;p&gt;Eligibility Trace는 TD 학습과 밀접한 관련이 있는 것 같지만 실제로는 별로 관련이 없습니다. 이번 Section에서 보이겠지만, Monte Carlo 학습도 Eligibility Trace가 발생합니다. 9장에서 다루었던 Forward View에서 본 Linear Monte Carlo 알고리즘에 Dutch Trace를 사용하여 더 계산적으로 효율적인 Backward View 알고리즘을 유도할 수 있다는 것을 보일 예정입니다. 이점은 이 책의 저자가 인정하는 Forward View와 Backward View의 유일한 동치 부분입니다. 또한 이것은 True On-line TD($\lambda$)와 On-line $\lambda$-Return Algorithm의 동등성 증명 방향을 제공하지만, 훨씬 더 간단합니다.&lt;/p&gt;

&lt;p&gt;먼저 Gradient Monte Carlo 예측 알고리즘의 Linear 버전은 Episode의 각 시간 단계에서 하나씩 다음과 같은 Update가 발생합니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G - \mathbf{w}_t^{\sf T} \mathbf{x}_t \right] \mathbf{x}_t , \quad 0 \le t &amp;lt; T \tag{12.13}\]

&lt;p&gt;예시를 단순화하기 위해 Return $G$는 Episode가 끝날 때 받은 단일 Reward이고 Discount가 없다고 가정합니다. (Return $G$는 &lt;strong&gt;단일&lt;/strong&gt; 보상이기 때문에 $G_t$와 같이 시간에 대한 첨자가 붙지 않습니다.) 이 경우 Update는 Least Mean Square (LMS) 규칙이라고도 합니다. Monte Carlo 알고리즘에서의 모든 Update는 최종 Reward/Return에 따라 달라지므로 Episode가 끝날 때까지 아무것도 할 수 없기 때문에 Monte Carlo 알고리즘은 Off-line 알고리즘입니다. 그래서 여기서는 계산상의 이점이 있는 새로운 알고리즘의 구현을 목적으로 합니다. 새로운 알고리즘에서도 기존과 마찬가지로 Episode가 끝날 때만 Weight Vector를 Update하겠지만, Episode의 각 단계에서 약간의 계산을 수행함으로써 전체적으로 계산량을 고르게 분배할 계획입니다. 이것을 통해 단계당 $O(d)$의 시간 복잡도가 소요되지만, 각 단계에서 Feature Vector를 저장할 필요가 없습니다. 대신 Eligibility Trace를 도입하여 지금까지 경험한 모든 Feature Vector의 요점만을 저장합니다. 이 방법은 Episode가 끝날 때까지 식 (12.13)의 Update 과정과 정확히 동일한 Update를 효율적으로 재생성합니다. 식에 대한 전개 과정은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{T} &amp;amp;= \mathbf{w}_{T-1} + \alpha \left( G - \mathbf{w}_{T-1}^{\sf T} \mathbf{x}_{T-1} \right) \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{w}_{T-1} + \alpha \mathbf{x}_{T-1} \left( - \mathbf{x}_{T-1}^{\sf T} \mathbf{w}_{T-1} \right) + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \left( \mathbf{I} - \alpha \mathbf{x}_{T-1} \mathbf{x}_{T-1}^{\sf T} \right) \mathbf{w}_{T-1} + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{w}_{T-1} + \alpha G \mathbf{x}_{T-1}
\end{align}\]

&lt;p&gt;이 때 $\mathbf{F}_{t} \doteq \mathbf{I} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T}$는 &lt;span style=&quot;color:red&quot;&gt;Forgetting&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;Fading&lt;/span&gt;이라고 부르는 행렬입니다. 이제 위 식을 재귀적으로 전개해보면,&lt;/p&gt;

\[\begin{align}
&amp;amp;= \mathbf{F}_{T-1} \left( \mathbf{F}_{T-2} \mathbf{w}_{T-2} + \alpha G \mathbf{x}_{T-2} \right) + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{w}_{T-2} + \alpha G \left( \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \left( \mathbf{F}_{T-3} \mathbf{w}_{T-3} + \alpha G \mathbf{x}_{T-3} \right) + \alpha G \left( \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{F}_{T-3} \mathbf{w}_{T-3} + \alpha \left( \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{x}_{T-3} + \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp; \qquad \qquad \vdots \\ \\
&amp;amp;= \underbrace{\mathbf{F}_{T-1} \mathbf{F}_{T-2} \cdots \mathbf{F}_0 \mathbf{w}_0}_{\mathbf{a}_{T-1}} + \alpha G \underbrace{\sum_{k=1}^{T-1} \mathbf{F}_{T-1} \mathbf{F}_{T-2} \cdots \mathbf{F}_{k+1} \mathbf{x}_k}_{\mathbf{z}_{T-1}} \\ \\
&amp;amp;= \mathbf{a}_{T-1} + \alpha G \mathbf{z}_{T-1} \tag{12.14}
\end{align}\]

&lt;p&gt;여기서 $\mathbf{a}_{T-1}$과 $\mathbf{z}_{T-1}$는 시간 $T-1$에서의 메모리 Vector로써, $G$에 대한 정보 없이 각 시간 단계에서 $O(d)$의 시간 복잡도로 Update할 수 있습니다. 이 중 Vector $\mathbf{z}_{t}$는 Dutch-style Eligibility Trace입니다. 이 Vector는 시간 단계 0에서 $\mathbf{z}_0 = \mathbf{x}_0$으로 초기화된 후 다음과 같이 Update됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_t &amp;amp; \doteq \sum_{k=1}^t \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_{k+1} \mathbf{x}_k, \quad 1 \le t &amp;lt; T \\ \\
&amp;amp;= \sum_{k=0}^{t-1} \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_{k+1} \mathbf{x}_k + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{F}_t \sum_{k=1}^{t-1} \mathbf{F}_{t-1} \mathbf{F}_{t-2} \cdots \mathbf{F}_{k+1} \mathbf{x}_k + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{F}_t \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \left( I - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \right) \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} - \alpha \left( \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} + \left( 1 - \alpha \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t
\end{align}\]

&lt;p&gt;이것은 식 (12.11)에서 $\gamma \lambda = 1$인 경우에 대한 Dutch Trace입니다. Auxiliary Vector $\mathbf{a}_t$는 시간 단계 0에서 $\mathbf{a}_0 = \mathbf{w}_0$로 초기화 된 후, 다음과 같이 Update됩니다.&lt;/p&gt;

\[\mathbf{a}_t \doteq \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_0 \mathbf{w}_0 = \mathbf{F}_t \mathbf{a}_{t-1} = \mathbf{a}_{t-1} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \mathbf{a}_{t-1}, \quad 1 \le t &amp;lt; T\]

&lt;p&gt;Auxiliary Vector $\mathbf{a}_t$와 Dutch Trace $\mathbf{z}_{t}$는 각 시간 단계 $t &amp;lt; T$에서 Update되고 $G$를 알 수 있는 시간 단계 $T$에서는 $\mathbf{w}_T$를 계산하기 위해 식 (12.14)에서 사용됩니다. 이런 방법으로 계산 복잡도가 높은 식 (12.13)과 같은 MC/LMS 알고리즘과 정확히 동일한 최종 결과를 얻었습니다. 새로 구한 방법은 각 시간 단계별 시간 복잡도 및 공간 복잡도가 $O(d)$인 증분 알고리즘을 사용합니다. 이것은 TD Learning이 아닌 환경에서 Eligibility Trace의 개념이 발생했기 때문에 흥미로운 결과입니다. 그러므로 Eligibility Trace는 TD Learning에만 국한되지 않는다는 것을 알 수 있습니다. Eligibility Trace의 필요성은 효율적인 방식으로 장기적인 예상치를 학습하고자 할 때 나타나는 것으로 보면 될 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;sarsalambda&quot;&gt;Sarsa($\lambda$)&lt;/h2&gt;

&lt;p&gt;Eligibility Trace를 Action-Value로 확장하기 위해서는 다행스럽게도 이미 제시된 아이디어에서 거의 변경할 필요가 없습니다. Estimated Value인 $\hat{q} (s, a, \mathbf{w})$를 학습하기 위해서는 아래와 같이 10장에서 배운 $n$-step Return의 Action-Value 형태를 사용해야 합니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{q} \left( S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1} \right), \quad t+n &amp;lt; T\]

&lt;p&gt;이 때 만약 $t + n \ge T$인 경우라면 $G_{t:t+n} \doteq G_t$입니다. 이 방법을 통해 $\lambda$-Return의 Action-Value 형태를 만들 수 있습니다. Off-line $\lambda$-Return Algorithm의 Action-Value 형식은 단순히 $\hat{v}$를 $\hat{q}$로 대체하면 됩니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G_t^{\lambda} - \hat{q} \left( S_t, A_t, \mathbf{w}_t \right) \right] \nabla \hat{q} \left( S_t, A_t, \mathbf{w}_t \right), \quad t = 0, \ldots, T - 1 \tag{12.15}\]

&lt;p&gt;이 때 $G_t^{\lambda} \doteq G_{t:\infty}^{\lambda}$입니다. 이 Forward View 에 대한 복합적인 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Backup Diagram과 TD($\lambda$)의 Backup Diagram을 비교해보면 굉장히 유사하다는 것을 알 수 있습니다. 또한 $\lambda$-Return에서 각 $n$-step Update의 Weight는 TD($\lambda$) 및 $\lambda$-Return 알고리즘에서와 같습니다.&lt;/p&gt;

&lt;p&gt;Action Value에 대한 TD 방법은 &lt;span style=&quot;color:red&quot;&gt;Sarsa($\lambda$)&lt;/span&gt;로 알려져 있는데, 이것은 이 Forward View를 추정합니다. 이 방법의 Update 규칙은 아래와 같이 TD($\lambda$)와 동일합니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t\]

&lt;p&gt;단, TD Error의 Action-Value 형태는 예외적입니다.&lt;/p&gt;

\[\delta_t \doteq R_{t+1} + \gamma \hat{q} \left( S_{t+1}, A_{t+1}, \mathbf{w}_t \right) - \hat{q} \left( S_t, A_t, \mathbf{w}_t \right) \tag{12.16}\]

&lt;p&gt;또한 Eligibility Trace의 Action-Value 형태는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_{-1} &amp;amp; \doteq \mathbf{0} \\ \\
\mathbf{z} &amp;amp; \doteq \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{q} \left( S_t, A_t, \mathbf{w} \right), \quad 0 \le t \le T
\end{align}\]

&lt;p&gt;Sarsa($\lambda$)의 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 12.1) Traces in Gridworld&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 사용하면 1-step 방법이나 $n$-step 방법보다 Control 알고리즘의 효율성을 크게 높일 수 있습니다. Gridworld 예제를 이용하여 이것을 설명하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 그림은 단일 Episode에서 Agent가 이동한 경로를 나타냅니다. 초기 Estimated Value는 0이고, G로 표시된 Target 지점을 제외하면 모든 Reward는 0입니다. 나머지 그림에 나타난 화살표는 각각의 알고리즘에 대해 어떤 Action-Value가 얼마나 증가하는지를 나타냅니다. 1-step Sarsa는 Target에 도달했을 때 마지막 Action에 대한 Value만 증가시키지만, $n$-step 방법은 마지막 $n$개의 Action에 대한 Value를 동일하게 증가시킵니다. ($\gamma = 1$이라고 가정) 가장 오른쪽에 있는 Sarsa($\lambda$) 방법은 Episode에서의 모든 Action에 대한 Value를 Update 하지만, Target 지점에서 (시간적으로) 멀어질수록 더 적게 반영됩니다. 이러한 Update 방법을 &lt;span style=&quot;color:red&quot;&gt;Fading&lt;/span&gt;이라고 하는데, 일반적으로 Fading 방법이 제일 좋은 경우가 많습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 12.2) Sarsa($\lambda$) on Mountain Car&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 10장에서 다루었던 Mountain Car 예제에 Sarsa($\lambda$)를 적용해 보겠습니다. 기본적인 예제의 세팅은 10장에서와 동일합니다. 위의 그림은 Mountain Car 문제에 대해  Sarsa($\lambda$)와 $n$-step Sarsa의 성능을 비교한 그래프입니다. $n$-step Sarsa에서는 변수로써 $n$의 값을 변경하며 비교했지만, Sarsa($\lambda$)에서는 $\lambda$의 값을 변경하며 비교합니다. 두 그래프를 비교해보면 Sarsa($\lambda$)의 Fading-trace bootstrapping 전략이 이 문제에 대해 더 효율적인 학습 방법이라는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;또한 이상적인 TD 방법의 Action-Value 버전을 On-line $\lambda$-Return 알고리즘 및 True On-line TD($\lambda$)으로 구현할 수도 있습니다. Section 12.4에서 다룬 On-line $\lambda$-Return 알고리즘의 Action-Value 버전은 $n$-step Return을 Action-Value 형식으로 바꾸는 것 외에는 변경할 부분이 없습니다. 또한 Section 12.5와 12.6에서의 분석은 Action-Value에 대해서도 동일하며, 유일한 차이점은 State에 대한 Feature Vector를 $\mathbf{x}_t = \mathbf{x}(S_t)$ 대신 $\mathbf{x}_t = \mathbf{x}(S_t, A_t)$로 사용한다는 것입니다. True On-line Sarsa($\lambda$)에 대한 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 그림은 Mountain Car 예제에서 Sarsa($\lambda$)의 여러 버전에 대해 성능을 비교하는 그래프입니다. True On-line Sarsa($\lambda$)는 일반 Sarsa($\lambda$)보다 더 나은 성능을 보여줌을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;variable-lambda-and-gamma&quot;&gt;Variable $\lambda$ and $\gamma$&lt;/h2&gt;

&lt;p&gt;이제 기본적인 TD Learning 알고리즘에 대한 개발은 끝을 향해 달려가고 있습니다. 최종 알고리즘을 일반적인 형태로 나타내기 위해서는 State와 Action에 잠재적으로 의존하는 함수에 대해 일정한 매개변수는 물론, Bootstrapping 및 Discounting에 대한 정도를 일반화하는 것이 좋습니다. 즉, 각 시간 단계에 대해 서로 다른 $\lambda$및 $\gamma$를 설정하여, 이것을 각각 $\lambda_t$와 $\gamma_t$로 표현하는 것입니다. 또한 표기법을 변경하여 기존의 $\lambda$는 $\lambda : \mathcal{S} \times \mathcal{A} \to [0, 1]$와 같이 State와 Action에 대한 함수로 정의하고, 시간에 따라 변하는 $\lambda_t$는 $\lambda_t \doteq \lambda (S_t, A_t)$와 같이 함수 $\lambda$를 사용하여 표현합니다. 비슷하게, $\gamma$ 또한 $\gamma : \mathcal{S} \to [0, 1]$로 정의하고, $\gamma_t$를 $\gamma_t \doteq \gamma (S_t)$로 표현합니다.&lt;/p&gt;

&lt;p&gt;함수 $\gamma$는 특히 더 중요한데, 우리가 추정하고자 하는 기본 확률 변수인 Return을 변경하기 때문입니다. 이 함수 $\gamma$를 앞으로 &lt;span style=&quot;color:red&quot;&gt;Termination Function&lt;/span&gt;이라고 부르겠습니다. 이제 Return을 다음과 같이 더 일반적으로 정의하겠습니다.&lt;/p&gt;

\[\begin{align}
G_t &amp;amp; \doteq R_{t+1} + \gamma_{t+1} G_{t+1} \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} R_{t+2} + \gamma_{t+1} \gamma_{t+2} R_{t+3} + \gamma_{t+1} \gamma_{t+2} \gamma_{t+3} R_{t+4} + \cdots \\ \\
&amp;amp;= \sum_{k=t}^{\infty} \left( \prod_{i=t+1}^{k} \gamma_i \right) R_{k+1} \tag{12.17}
\end{align}\]

&lt;p&gt;식 (12.17)에서 합이 유한함을 보장하기 위해서는 모든 $t$에 대해 1의 확률로 $\prod_{k=t}^{\infty} \gamma_k = 0$를 만족해야 합니다.&lt;/p&gt;

&lt;p&gt;위와 같은 정의의 편리한 점은 Episode의 설정이나 알고리즘이 특정한 Terminal State나 Start Distribution, 종료 시간과 같은 특별한 설정 없이 단일 경험의 관점에서 표시될 수 있다는 것입니다. 특별한 경우로, 이전의 Terminal State는 $\gamma (s) = 0$인 State가 되어 Start Distribution으로 전환됩니다. 그리고 다른 모든 State에서 상수로 $\gamma ( \cdot )$를 선택함으로써 고전적인 Episode Task로 설정할 수 있습니다. State에 의존적인 종료에는 Markov Process의 흐름을 변경하지 않고 수량을 예측하는 Pseudo Termination과 같은 다른 예측 사례가 포함됩니다. Discounted Return은 그러한 수량으로 생각할 수 있으며, 이 경우 State에 의존적인 종료는 Episodic Task과 Discounted-Continuing Task 모두를 통합합니다. 물론 unDiscounted-Continuing Task의 경우 여전히 특별한 해결 방법이 필요합니다. (이 문단의 번역이 굉장히 어렵네요. 최대한 노력했습니다만 이해가 어려우실 것 같아 원문을 함께 봐주시기 바랍니다)&lt;/p&gt;

&lt;p&gt;가변 Bootsrtapping에 대한 일반화는 Discounting과 같은 문제의 변경이 아니라 해결 방법의 변경입니다. 일반화하는 State와 Action을 위한 $\lambda$-Return에 영향을 미칩니다. 새로운 State 기반 $\lambda$-Return은 다음과 같이 재귀적으로 작성할 수 있습니다.&lt;/p&gt;

\[G_t^{\lambda s} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \hat{v} (S_{t+1}, \mathbf{w}_t) + \lambda_{t+1} G_{t+1}^{\lambda s} \right) \tag{12.18}\]

&lt;p&gt;식 (12.18)은 위 첨자 $s$를 추가하여 이것이 State-Value에서 Bootstrapping 하는 Return임을 나타내고 있습니다. 이와 반대로 아래 식 (12.19)는 위 첨자로 $a$를 추가하여 Action-Value에서 Bootstrapping 하는 Return임을 나타냅니다. 이 식의 첫 번째 항은 $\lambda$-Return에서 Bootstrapping에 영향받지 않고 unDiscounted인 첫 번째 Reward를 의미합니다. 두 번째 항은 만약 다음 State가 Terminal State라면 0이 됩니다.&lt;/p&gt;

&lt;p&gt;만약 다음 State가 Terminal State가 아니라면, 두 번째 항은 State의 Bootstrapping 정도에 따라 두 가지 경우로 구분됩니다. Bootstrapping하는 범위 내에서 이 항은 State에서 추정된 값이지만, Bootstrapping 하지 않는 범위에서 이 항은 다음 시간 단계에서의 $\lambda$-Return입니다.&lt;/p&gt;

&lt;p&gt;Action 기반의 $\lambda$-Return Sarsa 형태는 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda a} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t) + \lambda_{t+1} G_{t+1}^{\lambda a} \right) \tag{12.19}\]

&lt;p&gt;위 식을 Expected Sarsa 형태로 수정하면 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda a} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \bar{V}_t (S_{t+1}) + \lambda_{t+1} G_{t+1}^{\lambda a} \right) \tag{12.20}\]

&lt;p&gt;식 (12.20)에서 $\bar{V}_t (s)$는 다음과 같이 Function Approximation으로 정의됩니다.&lt;/p&gt;

\[\bar{V}_t (s) \doteq \sum_a \pi (a | s) \hat{q} (s, a, \mathbf{w}_t) \tag{12.21}\]

&lt;h2 id=&quot;off-policy-traces-with-control-variates&quot;&gt;Off-policy Traces with Control Variates&lt;/h2&gt;

&lt;p&gt;Eligibility Trace의 마지막 단계는 Importance Sampling을 통합하는 것입니다. non-Truncated $\lambda$-Return을 사용하는 방법의 경우 Importance Sampling의 Weight가 Target Return에 적용할 수 있는 옵션이 없습니다. (ex. Section 7.3의 $n$-step 방법) 그래서 대신 Section 7.4에서와 같이 Control Variate가 있는 Per-decision Importance Sampling의 Bootstrapping 일반화로 해결하고자 합니다.&lt;/p&gt;

&lt;p&gt;State의 경우, 식 (12.18)의 $\lambda$-Return 일반화에 대한 최종 정의는 식 (7.13)과 결합하여 다음과 같이 정의됩니다.&lt;/p&gt;

\[G_t^{\lambda s} \doteq \rho_t \Big( R_{t+1} + \gamma_{t+1} \big( (1 - \lambda_{t+1}) \hat{v} (S_{t+1}, \mathbf{w}) + \lambda_{t+1} G_{t+1}^{\lambda s} \big) \Big) + (1 - \rho_t) \hat{v} (S_t, \mathbf{w}_t) \tag{12.22}\]

&lt;p&gt;여기서 $\rho_t = \frac{\pi (A_t \mid S_t)}{b (A_t \mid S_t)}$는 단일 단계 Importance Sampling Ratio입니다. 이 교재에서 다루었던 다른 Return과 마찬가지로 이 최종 $\lambda$-Return은 단순히 State 기반 TD Error의 합계로 근사할 수 있습니다. 먼저 State 기반 TD Error는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\delta_t^s \doteq R_{t+1} + \gamma_{t+1} \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{12.23}\]

&lt;p&gt;이것을 통해 $G_t^{\lambda s}$를 근사하면,&lt;/p&gt;

\[G_t^{\lambda s} \approx \hat{v}(S_t, \mathbf{w}_t) + \rho_t \sum_{k=t}^{\infty} \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \tag{12.24}\]

&lt;p&gt;이 때, Approximate Value Function $\hat{v}$가 변하지 않으면 식 (12.24)의 근사는 정확해집니다.&lt;/p&gt;

&lt;p&gt;식 (12.24)와 같은 $\lambda$-Return의 형태는 Forward-View Update에서 사용하기 편리해 보입니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t + \alpha \big( G_t^{\lambda s} - \hat{v} (S_t, \mathbf{w}_t) \big) \nabla \hat{v} (S_t, \mathbf{w}_t) \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \rho_t \left( \sum_{k=t}^{\infty} \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \right) \nabla \hat{v} (S_t, \mathbf{w}_t)
\end{align}\]

&lt;p&gt;위 식은 Eligibility에 기반한 TD Update처럼 보입니다. $\prod$ 연산 부분은 Eligibility Trace와 같으며, 여기에 TD Error가 곱해집니다. 그러다 이것은 Forward View의 한 단계일 뿐입니다. 우리가 찾고 있는 관계는 시간이 지남에 따라 합산되는 Forward View Update가 역시 시간이 지남에 따라 합산되는 Backward View Update와 거의 같다는 것입니다. (다만 이 관계는 Value Function의 변경을 무시하기 때문에 대략적인 것으로만 성립합니다) 시간 경과에 따른 Forward View Update의 합계는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\sum_{t=0}^{\infty} (\mathbf{w}_{t+1} - \mathbf{w}_t) &amp;amp; \approx \sum_{t=0}^{\infty} \sum_{k=t}^{\infty} \alpha \rho_t \delta_k^s \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;= \sum_{k=0}^{\infty} \sum_{t=0}^k \alpha \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;(\text{using the summation rule : } \sum_{t=x}^y \sum_{k=t}^y = \sum_{k=x}^y \sum_{t=x}^k) \\ \\
&amp;amp;= \sum_{k=0}^{\infty} \alpha \delta_k^s \sum_{t=0}^k \rho_t \nabla (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i
\end{align}\]

&lt;p&gt;위 식의 두 번째 합계부터는 전체 표현식이 Eligibility Trace으로 작성되고 점진적으로 Update 될 수 있는 경우 Backward View TD Update의 합계 형태가 될 것입니다. 즉, 이 표현식이 시간 $k$에서의 trace이면, 시간 $k-1$의 Value에서 이것을 Update할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_k &amp;amp;= \sum_{t=0}^k \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;= \sum_{t=0}^{k-1} \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i + \rho_k \nabla \hat{v}(S_k, \mathbf{w}_k) \\ \\
&amp;amp;= \gamma_k \lambda_k \rho_k \underbrace{\sum_{t=0}^{k-1} \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^{k-1} \gamma_i \lambda_i, \rho_i}_{\mathbf{z}_{k-1}} + \rho_k \nabla \hat{v} (S_k, \mathbf{w}_k) \\ \\
&amp;amp;= \rho_k \big( \gamma_k \lambda_k \mathbf{z}_{k-1} + \nabla \hat{v} (S_k, \mathbf{w}_k) \big)
\end{align}\]

&lt;p&gt;위 전개식에서는 아래 첨자가 $k$로 나와있지만, 시간에 대해 일반화하는 것을 명시하기 위해 아래 첨자를 $t$로 수정하여 최종 결과를 정리하겠습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \rho_t \big( \gamma_t \lambda_t \mathbf{z}_{t-1} + \nabla \hat{v} (S_t, \mathbf{w}_t) \big) \tag{12.25}\]

&lt;p&gt;이 Eligibility Trace는 식 (12.7)과 같은 TD($\lambda$)에 대한 일반적인 Semi-gradient 매개변수 Update 규칙과 함께 On-policy 또는 Off-policy 데이터에 적용할 수 있는 일반적인 TD($\lambda$) 알고리즘을 형성합니다. On-policy의 경우 $\rho_t$가 항상 1이므로 식 (12.25)가 식 (12.5)와 동일해지기 때문에 알고리즘은 정확히 TD($\lambda$)입니다. Off-policy의 경우 알고리즘이 잘 작동하는 경우가 많지만 Semi-gradient 방법으로는 안정성이 보장되지 않습니다. 이어지는 다음 여러 Section을 통해 안정성을 보장할 수 있는 방법을 고려할 것입니다.&lt;/p&gt;

&lt;p&gt;위와 유사한 과정을 거쳐 Action-Value에 대한 방법과 이에 해당하는 일반적인 Sarsa($\lambda$) 알고리즘에 대한 Off-policy Eligibility Trace를 얻을 수 있습니다. 전자는 식 (12.19)나 (12.20)과 같은 일반적인 Action 기반 $\lambda$-Return에 대한 재귀 방법으로 시작해야 하지만, 후자(Expected Sarsa 형식)는 더 간단합니다. 식 (12.20)에 식 (7.14)를 결합하여 다음과 같이 Off-policy로 확장하면 되기 때문입니다.&lt;/p&gt;

\[\begin{align}
G_t^{\lambda a} &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \Big( (1 - \lambda_{t+1}) \bar{V}_{t} (S_{t+1}) + \lambda_{t+1} [ \rho_{t+1} G_{t+1}^{\lambda a} + \hat{V}_t (S_{t+1}) - \rho_{t+1} \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t) ] \Big) \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} \Big( \bar{V}_t (S_{t+1}) + \lambda_{t+1} \rho_{t+1} \left[ G_{t+1}^{\lambda a} - \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t ) \right] \Big) \tag{12.26}
\end{align}\]

&lt;p&gt;식 (12.26)에서 $\bar{V}_t (S_{t+1})$은 식 (12.21)과 동일합니다. 그리고 또 다시  $\lambda$-Return은 TD Error의 합으로 표현할 수 있습니다.&lt;/p&gt;

\[G_t^{\lambda a} \approx \hat{q}(S_t, A_t, \mathbf{w}_t) + \sum_{k=t}^{\infty} \delta_k^a \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \tag{12.27}\]

&lt;p&gt;식 (12.27)의 $\delta_t^a$는 다음과 같이 Action 기반 TD Error의 기대 형식으로 정의됩니다.&lt;/p&gt;

\[\delta_t^a = R_{t+1} + \gamma_{t+1} \bar{V}_t (S_{t+1}) - \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{12.28}\]

&lt;p&gt;식 (12.24)와 마찬가지로, Approximate Value Function $\hat{q}$가 변하지 않으면 식 (12.27)의 근사값은 정확해집니다.&lt;/p&gt;

&lt;p&gt;State의 경우에 대한 과정과 유사한 과정을 사용하여 식 (12.27)에 기반한 Forward View Update를 유도하고, Summation Rule을 사용하여 Update의 합계를 변환한 후, 마지막으로 Action-Value에 대한 Eligibility Trace를 다음과 같이 유도할 수 있습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma_t \lambda_t \rho_t \mathbf{z}_{t-1} + \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{12.29}\]

&lt;p&gt;이 Eligibility Trace는 식 (12.28)과 같은 TD Error 및 식 (12.7)의 일반적인 Semi-gradient Update 규칙과 함께 On-policy, 또는 Off-policy에 적용할 수 있는 효율적인 Expected Sarsa($\lambda$) 알고리즘을 생성합니다. 이것은 아마도 현 시점에서 가장 좋은 알고리즘일 것입니다. (물론, 위에서 언급했듯이 아직까지는 안정성이 보장되지 않습니다.) On-policy의 경우 상수 $\lambda$와 $\gamma$, 그리고 식 (12.16)과 같은 State-Action TD Error를 사용한 알고리즘은 Section 12.7에서 제시된 Sarsa($\lambda$)와 동일합니다.&lt;/p&gt;

&lt;p&gt;$\lambda = 1$에서 이러한 알고리즘은 Monte Carlo 알고리즘과 밀접한 관련이 있습니다. Episodic Task와 Off-line Update에 대해 정확하게 동일할 것으로 생각할 수도 있지만, 실제 관계는 그것보단 약합니다. 가장 간단한 조건은 Episode별로 Update가 없으며 기대치만 있는 경우입니다. 이 방법은 Trajectory가 이어짐에 따라 (철회할 수 없는) Update를 만들지만, True Monte Carlo Method는 Trajectory가 있는 경우 Target Policy 하에 0의 확률을 가진 Action이 있을 경우 Trajectory를 Update하지 않습니다. 특히 이 모든 방법들은 $\lambda = 1$일지라도 Target이 현재 Value에 대한 추정치에 의존하기 때문에 여전히 Bootstrap합니다. 이것이 실제로 좋은지 나쁜지는 또 다른 문제입니다.&lt;/p&gt;

&lt;p&gt;관련 연구를 하나 소개하자면 (Sutton, Mahmood, Precup and van Hasselt, 2014)의 논문에서 정확한 동등성을 달성하는 방법이 제안되었습니다. 이 방법은 Update를 추적하지만, 나중에 취한 Action에 따라 철회할 수 있는 &lt;strong&gt;Provisional Weight&lt;/strong&gt;라는 추가적인 Vector를 이용합니다. 이 방법의 State 및 State-Action 방법 버전을 각각 &lt;span style=&quot;color:red&quot;&gt;PTD($\lambda$)&lt;/span&gt;와 &lt;span style=&quot;color:red&quot;&gt;PQ($\lambda$)&lt;/span&gt;라고 합니다. 여기서 ‘P’는 &lt;strong&gt;Provisional&lt;/strong&gt;의 약자입니다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 새로운 Off-policy 방법의 실질적인 결과는 아직 확립되지 않았습니다. 확실한 것은, Importance Sampling을 사용하는 모든 Off-policy 방법과 마찬가지로 높은 Variance 문제가 발생할 것입니다.&lt;/p&gt;

&lt;p&gt;만약 $\lambda &amp;lt; 1$이면 모든 Off-policy 알고리즘은 Bootstrapping을 포함하고 Section 11.3에서 언급한 &lt;strong&gt;Deadly Traid&lt;/strong&gt;가 적용됩니다. 이것은 Tabular, State Aggregation 및 기타 제한된 형태의 Function Approximation에 대해서만 안정성이 보장될 수 있다는 것을 의미합니다. Linear과 같은 보다 일반적인 형태의 Function Approximation의 경우, 매개변수 Vector는 11장의 예제에서와 같이 무한대로 발산할 수 있습니다. 11장에서 논의한 바와 같이 Off-policy 학습의 과제는 크게 두 부분으로 나눌 수 있습니다. Off-policy Eligibility Trace는 문제의 첫 번째 부분은 효과적으로 처리하여 Target의 Expected Value를 추정하지만, Update의 Distribution과 관련된 두 번째 문제는 전혀 처리하지 못합니다. Eligibility Trace를 포함한 Off-policy 학습에서 두 번째 문제를 해결하기 위한 알고리즘 전략은 Section 12.11에서 보일 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;watkinss-qlambda-to-tree-backuplambda&quot;&gt;Watkins’s Q($\lambda$) to Tree-Backup($\lambda$)&lt;/h2&gt;

&lt;p&gt;Q-learning을 Eligibility Trace로 확장하기 위해 여러 방법이 제안되었습니다. 가장 처음 제안된 방법은 Watkins의 &lt;span style=&quot;color:red&quot;&gt;Q($\lambda$)&lt;/span&gt;로, Greedy Action이 수행되는 한 일반적인 방식으로 Eligibility Trace을 감소시킨 다음, 첫 번째 non-Greedy Action 후에 Trace를 0으로 줄입니다. Q($\lambda$)의 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;6장에서 Q-learning과 Expected Sarsa를 통합하여 임의의 Target Policy로 일반화했으며, 이 장의 이전 Section에서 Expected Sarsa를 Off-policy Eligibility Trace로 일반화하였습니다. 그러나 7장에서는 Importance Sampling을 사용하지 않는 속성을 유지한 $n$-step Tree Backup과 $n$-step Expected Sarsa를 구분했습니다. 이제 우리는 Tree Backup의 Eligibility Trace 버전인 &lt;span style=&quot;color:red&quot;&gt;Tree Backup($\lambda$)&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;TB($\lambda$)&lt;/span&gt;를 제시해야 합니다. 이는 Off-policy 데이터에 적용할 수 있음에도 불구하고 Importance Sampling이 없다는 장점이 있기 때문에 Q-learning의 진정한 확장이라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;TB($\lambda$)의 개념은 간단합니다. Section 7.5에서와 같이 Tree Backup의 Update는 Bootstrapping Parameter $\lambda$에 따라 일반적인 방식으로 Weight가 부여됩니다. TB($\lambda$)의 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반적인 Bootstrapping 및 Discounting Parameter에 대한 올바른 Index를 사용하여 구체적인 방정식을 얻으려면, 다음과 같이 식 (12.20) $\lambda$-Return의 재귀 형식으로 시작한 다음, 식 (7.16) Target의 Bootstrapping 경우로 확장하는 것이 가장 좋습니다.&lt;/p&gt;

\[\begin{align}
G_t^{\lambda a} &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \Big( (1 - \lambda_{t+1}) \bar{V}_t (S_{t+1}) + \lambda_{t+1} \big[ \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) \hat{q} (S_{t+1}, a, \mathbf{w}_t) + \pi (A_{t+1} | S_{t+1}) G_{t+1}^{\lambda a} \big] \Big) \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} \Big( \bar{V}_t (S_{t+1}) + \lambda_{t+1} \pi (A_{t+1} | S_{t+1}) \big( G_{t+1}^{\lambda a} - \hat{v} (S_{t+1}, A_{t+1}, \mathbf{w}_t) \big) \Big)
\end{align}\]

&lt;p&gt;이전과 마찬가지로, $G_t^{\lambda a}$를 TD Error의 합으로 근사할 수도 있습니다. 이 때 TD Error는 식 (12.28)과 같은 형태를 사용합니다.&lt;/p&gt;

\[G_t^{\lambda a} \approx \hat{q} (S_t, A_t, \mathbf{w}_t) + \sum_{k=t}^{\infty} \delta_k^a \prod_{i=t+1}^k \gamma_i \lambda_i \pi (A_i | S_i)\]

&lt;p&gt;이전 Section과 동일한 단계에 따라, 선택한 Action의 Target Policy 확률과 관련된 특별한 Eligibility Trace Update를 유도할 수 있습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma_t \lambda_t \pi (A_t | S_t) \mathbf{z}_{t+1} + \nabla \hat{q}(S_t, A_t, \mathbf{w}_t)\]

&lt;p&gt;이것은 식 (12.7)과 같은 일반적인 매개변수 Update 규칙과 함께 TB($\lambda$) 알고리즘을 정의합니다. 모든 Semi-gradient 알고리즘과 마찬가지로 TB($\lambda$)는 Off-policy 데이터와 강력한 Function Approximation 방법과 함께 사용했을 때 안정성이 보장되지 않습니다. 안정성을 보장받기 위해서는 TB($\lambda$)를 다음 Section에 나오는 방법 중 하나와 결합해야 합니다.&lt;/p&gt;

&lt;h2 id=&quot;stable-off-policy-methods-with-traces&quot;&gt;Stable Off-policy Methods with Traces&lt;/h2&gt;

&lt;p&gt;Eligibility Trace를 사용한 Off-policy 학습에서 안정성을 보장하기 위한 여러 방법이 제안되었습니다. 여기에서는 일반적인 Bootstrapping 및 Discount Function을 포함한 네 가지 방법을 제시합니다. 이 방법들은 모두 Section 11.7과 11.8에 제시한 Gradient-TD 또는 Emphatic-TD의 아이디어에 기반하고 있습니다. 모든 알고리즘은 Linear Function Approximation를 사용한다고 가정하지만, non-Linear Function Approximation에 대한 확장도 여러 논문에서 찾을 수 있습니다.&lt;/p&gt;

&lt;p&gt;첫 번째 방법으로 &lt;span style=&quot;color:red&quot;&gt;GTD($\lambda$)&lt;/span&gt;는 TDC와 유사한 Eligibility Trace 알고리즘으로, Section 11.7에서 제시한 두 가지 State-Value Gradient TD 예측 알고리즘보다 우수합니다. 이 알고리즘의 목표는 Behavior Policy $b$를 따르는 데이터를 위해 $\hat{v} (s, \mathbf{w}) \doteq \mathbf{w}_t^{\sf T} \mathbf{x} (s) \approx v_{\pi}(s)$와 같은 식에서 매개변수 $\mathbf{w}_t$를 학습하는 것입니다. 이 방법의 Update 식은 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t^s \mathbf{z}_t - \alpha \gamma_{t+1} (1 - \lambda_{t+1}) (\mathbf{z}_t^{\sf T} \mathbf{v}_t) \mathbf{x}_{t+1}\]

&lt;p&gt;위 식에서 $\delta_t^s$는 식 (12.23), $\mathbf{z}$는 식 (12.25), $\rho_t$는 식 (11.1)과 같습니다. 그리고 $\mathbf{v}_t$는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\mathbf{v}_{t+1} \doteq \mathbf{v}_t + \beta \delta_t^s \mathbf{z}_t - \beta (\mathbf{v}_t^{\sf T} \mathbf{x}_t) \mathbf{x}_t \tag{12.30}\]

&lt;p&gt;Section 11.7에서와 같이 $\mathbf{v} \in \mathbb{R}^d$는 $\mathbf{w}$와 같은 차원의 Vector이고, $\mathbf{v} = \mathbf{0}$으로 초기화됩니다. 그리고 $\beta &amp;gt; 0$은 두 번째 Step-size Parameter입니다.&lt;/p&gt;

&lt;p&gt;두 번째 방법인 &lt;span style=&quot;color:red&quot;&gt;GQ($\lambda$)&lt;/span&gt;는 Eligibility Trace가 포함된 Action-Value에 대한 Gradient-TD 알고리즘입니다. 이 알고리즘의 목표는 Off-policy 데이터에서 $\hat{q} (s, a, \mathbf{w}_t) \doteq \mathbf{w}_t^{\sf T} \mathbf{x}(s, a) \approx q_{\pi} (s, a)$와 같은 식의 매개변수 $\mathbf{w}_t$를 학습하는 것입니다. 만약 Target Policy가 $\epsilon$-greedy인 경우, 또는 $\hat{q}$에 대한 Greedy Policy로 Bias되는 경우 GQ($\lambda$)를 Control 알고리즘으로 사용할 수 있습니다. 이 방법의 Update는 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w} + \alpha \delta_t^a \mathbf{z}_t - \alpha \gamma_{t+1} (1 - \lambda_{t+1}) (\mathbf{z}_t^{\sf T} \mathbf{v}_t) \bar{\mathbf{x}}_{t+1}\]

&lt;p&gt;위 식에서 $\bar{\mathbf{x}}_t$는 Target Policy를 따르는 $S_t$에 대한 평균 Feature Vector로 정의됩니다.&lt;/p&gt;

\[\bar{\mathbf{x}}_t \doteq \sum_a \pi (a | S_t) \mathbf{x} (S_t, a)\]

&lt;p&gt;또한 $\delta_t^a$는 다음과 같은 TD Error로 정의됩니다.&lt;/p&gt;

\[\delta_t^a \doteq R_{t+1} + \gamma_{t+1} \mathbf{w}_t^{\sf T} \bar{\mathbf{x}}_{t+1} - \mathbf{w}_t^{\sf T} \mathbf{x}_t\]

&lt;p&gt;$\mathbf{z}_t$는 식 (12.29)와 동일하게 정의되고, $\mathbf{v}_t$의 Update를 포함한 나머지는 GTD($\lambda$)와 동일합니다.&lt;/p&gt;

&lt;p&gt;세 번째로 &lt;span style=&quot;color:red&quot;&gt;HTD($\lambda$)&lt;/span&gt;는 GTD($\lambda$)와 TD($\lambda$)를 결합한 State-Value 알고리즘입니다. 이 알고리즘의 가장 큰 장점은 TD($\lambda$)를 Off-policy 학습으로 엄격하게 일반화한다는 것입니다. &lt;strong&gt;엄격하게&lt;/strong&gt; 라는 의미는 Behavior Policy가 Target Policy와 같게 되면 HTD($\lambda$)가 TD($\lambda$)와 동일하게 된다는 뜻입니다. (GTD($\lambda$)는 그렇게 되지 않습니다.) 보통 TD($\lambda$)가 GTD($\lambda$)보다 빠르게 수렴하기 때문에 이 장점은 매력적입니다. 또한 TD($\lambda$)는 단일 Step-size Parameter만 필요하다는 장점도 있습니다. HTD($\lambda$)는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_t + \alpha \delta_t^s \mathbf{z}_t + \alpha \big( (\mathbf{z}_t - \mathbf{z}_t^b)^{\sf T} \mathbf{v}_t \big) (\mathbf{x}_t - \gamma_{t+1} \mathbf{x}_{t+1}) \\ \\
\mathbf{v}_{t+1} &amp;amp; \doteq \mathbf{v}_t \beta \delta_t^s \mathbf{z}_t - \beta \Big( \mathbf{z}_t^{b^{\sf T}} \mathbf{v}_t \Big) (\mathbf{x}_t - \gamma_{t+1} \mathbf{x}_{t+1}) \quad \text{with} \quad \mathbf{v}_0 \doteq \mathbf{0} \\ \\
\mathbf{z}_t &amp;amp; \doteq \rho_t \big( \gamma_t \lambda_t \mathbf{z}_{t-1} + \mathbf{x}_t \big) \quad \text{with} \quad \mathbf{z}_{-1} \doteq \mathbf{0} \\ \\
\mathbf{z}_t^b &amp;amp; \doteq \gamma_t \lambda_t \mathbf{z}_{t-1}^b + \mathbf{x}_t \quad \text{with} \quad \mathbf{z}_{-1}^b \doteq \mathbf{0}
\end{align}\]

&lt;p&gt;위 식에서 $\beta &amp;gt; 0$은 두 번째 Step-size Parameter입니다. 또한 두 번째 Weight 집합인 $\mathbf{v}_t$ 외에도 HTD($\lambda$)는 두 번째 Eligibility Trace 집합인 $\mathbf{z}_t^b$가 있습니다. 이것들은 Behavior Policy에 대한 누적된 Eligibility Trace이며, 모든 $\rho_t$가 1이면 $\mathbf{w}_t$ Update의 마지막 항이 0이 되면서 $\mathbf{z}_t$와 같아집니다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;span style=&quot;color:red&quot;&gt;Emphatic TD($\lambda$)&lt;/span&gt;는 1-step Emphatic-TD 알고리즘을 Eligibility Trace로 확장한 것입니다. (Section 9.11과 11.8 참고) 결과적으로 이 알고리즘은 강력한 Off-policy 수렴을 보장하면서 어느 정도 Bootstrapping도 가능하게 하지만, 높은 Variance를 가지고 수렴 속도가 느리다는 단점이 있습니다. Emphatic TD($\lambda$)는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t \\ \\
\delta_t &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \mathbf{w}_t^{\sf T} \mathbf{x}_{t+1} - \mathbf{w}_t^{\sf T} \mathbf{x}_t \\ \\
\mathbf{z}_t &amp;amp; \doteq \rho_t \left( \gamma_t \lambda_t \mathbf{z}_{t-1} + M_t \mathbf{x}_t \right) \quad \text{with} \quad \mathbf{z}_{-1} \doteq \mathbf{0} \\ \\
M_t &amp;amp; \doteq \lambda_t I_t + \left( 1 - \lambda_t \right) F_t \\ \\
F_t &amp;amp; \doteq \rho_{t-1} \gamma_t F_{t-1} + I_t \quad \text{with} \quad F_0 \doteq i (S_0)
\end{align}\]

&lt;p&gt;위 식에서 $M_t \ge 0$은 &lt;strong&gt;Emphasis&lt;/strong&gt;의 일반적인 형태이고, $F_t \ge 0$은 &lt;span style=&quot;color:red&quot;&gt;Followon Trace&lt;/span&gt;라고 하며, $I_t \ge 0$은 Section 11.8에서 설명한 &lt;strong&gt;Interest&lt;/strong&gt;입니다. 이 알고리즘의 중요한 점은 $\delta_t$와 같이 $M_t$ 또한 공간 복잡도를 높이지 않는다는 것입니다. 이 식의 정의를 Eligibility Trace 식에 대입하여 처리가 가능합니다. Emphatic TD($\lambda$)의 True On-line 버전에 대한 Pseudocode 및 프로그램은 (Sutton, 2015b) 논문에서 확인이 가능합니다.&lt;/p&gt;

&lt;p&gt;On-policy의 경우 (즉, 모든 $t$에 대해 $\rho_t = 1$) Emphatic TD($\lambda$)는 기존 TD($\lambda$)와 유사하지만 차이점도 많습니다. 예를 들어, Emphatic TD($\lambda$)는 모든 State에 종속적인 $\lambda$ 함수에 대해 수렴이 보장되지만, TD($\lambda$)는 그렇지 않습니다. TD($\lambda$)는 모든 상수 $\lambda$에 대해서만 수렴이 보장됩니다. 이에 대한 반례는 (Ghiassian, Rafiee, and Sutton, 2016) 논문을 참고해주시기 바랍니다.&lt;/p&gt;

&lt;h2 id=&quot;implementation-issues&quot;&gt;Implementation Issues&lt;/h2&gt;

&lt;p&gt;처음에는 Eligibility Trace를 사용하는 Tabular 방법이 1-step 방법보다 복잡해 보일 수도 있습니다. 단순한 구현은 모든 State(또는 State-Action 쌍)가 모든 시간 단계에서 Estimated Value와 Eligibility Trace를 모두 Update해야 합니다. 이것은 단일 명령, 다중 데이터, 병렬 컴퓨터 또는 Artificial Neural Network에서 구현하는 경우 문제가 되지 않지만, 기존의 직렬 컴퓨터에서 구현하는 경우에는 문제가 될 수 있습니다. 다행히도 일반적인 $\lambda$와 $\gamma$에 대해 거의 모든 State에서의 Eligibility Trace는 항상 0에 가깝습니다. 최근에 방문한 State에서만 0보다 훨신 큰 Trace가 있을 것이기 때문에 이러한 몇 개의 State만 Update하여 구현함으로써 간단하게 알고리즘을 근사적으로 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;실제로 이러한 방법을 사용하면 기존 컴퓨터로도 0보다 훨씬 큰 일부 Trace만 Update함으로써 구현할 수 있습니다. 이러한 꼼수를 사용하면 Tabular 방법에서 Trace를 사용하는 계산 비용이 일반적인 1-step 방법의 몇 배에 불과합니다. 물론 정확한 배수는 $\lambda$ 및 $\gamma$의 값과 다른 계산 비용에 따라 달라집니다. Tabular 방법의 경우는 어떤 의미에서 Eligibility Trace의 최악의 계산 복잡도를 가지고 있습니다. Function Approximation을 사용할 때는 Function Approximation 방법 자체의 계산 복잡도가 높기 때문에 Eligibility Trace를 사용하지 않는 것과 크게 차이가 나지 않기 때문입니다. 예를 들어, Artificial Neural Network 및 Backpropagation Algorithm을 사용하는 경우 Eligibility Trace를 추가해도 각 단계별로 필요한 메모리나 계산량이 두 배 정도만 늘어납니다. Section 12.3의 Truncated $\lambda$-Return 방법은 항상 추가적인 메모리 용량이 필요하지만, 기존 컴퓨터에서 계산적으로 효율적인 구현이 가능하기도 합니다.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;TD Error를 사용하는 Eligibility Trace는 Monte Carlo와 TD 방법의 중간 지점을 선택할 수 있는 효율적이고 점진적인 방법을 제공합니다. 7장의 $n$-step TD 방법도 이것을 가능하게 했지만 Eligibility Trace 방법은 더 일반적이고 더 빨리 학습할 수도 있으며 Trade-off를 통해 다른 계산 복잡성을 가질 수도 있습니다. 이번 장에서는 On-policy와 Off-policy 학습에서 Variable Bootstrapping 및 Discounting을 위해 Eligibility Trace에 대한 새로운 이론을 제시했습니다. 이 이론의 하나로써 기존 TD 방법의 계산 복잡도를 유지한 채 이상적인 방법의 Action을 정확하게 재현하는 True On-line 방법이 있습니다. 또 다른 것으로는 직관적인 Forward View 방법에서 보다 계산적으로 효율적인 Backward View로 전환할 수 있는 가능성입니다. 이제 고전적이고 계산량이 많은 Monte Carlo 알고리즘으로 시작하여 True On-line TD 방법에 사용한 것과 동일한 Eligibility Trace를 사용하여 계산량이 적은 Incremental non-TD를 구현함으로써 이 일반적인 아이디어를 설명했습니다.&lt;/p&gt;

&lt;p&gt;5장에서 언급했듯이 Monte Carlo Method은 Bootstrap하지 않기 때문에 non-Markov Process에서 이점이 있을 수 있습니다. Eligibility Trace는 TD 방법을 Monte Carlo Method와 유사하게 만들기 때문에 이런 경우에도 이점을 가질 수 있습니다. 예를 들어, TD 방법의 장점으로 인해 이것을 사용하고 싶지만, 일부 작업이 non-Markov인 경우 Eligibility Trace를 도입함으로써 이 문제를 해결할 수 있습니다. Eligibility Trace는 장기간의 지연된 Reward와 non-Markov Process 모두에 대한 해결 방법을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;$\lambda$의 값을 조절하여 Monte Carlo에서 1-step TD 방법에 이르기까지 Eligibility Trace를 어디에나 사용할 수 있습니다. 그렇다면 어느 단계에서 사용하는 것이 가장 좋을까요? 안타깝게도 이 질문에 대한 명확한 이론적인 답이 없습니다. 대신 경험적인 답으로써, Episode당 단계가 많거나 Discounting이 반감기 내에 단계가 많은 작업에서 Eligibility Trace를 사용하는 것이 더 좋다고 판단됩니다. 아래의 그래프는 $\lambda$에 따른 강화학습의 성능을 나타내고 있는데, 이것을 통해 대략적인 답을 낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;반면에 순수한 Monte Carlo Method에 가까워지면 성능이 급격히 저하됩니다. 그렇기 때문에 적당히 중간 정도의 Step이 최선의 선택이라고 볼 수 있습니다. 미래에는 $\lambda$를 사용하여 TD와 Monte Carlo Method 간 Trade-off를 더 미세하게 조절하는 것이 가능할 수도 있겠지만, 현재로서는 이것을 어떻게 안정적이고 유용하게 사용할 수 있을지 명확한 결론을 내릴 수가 없습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 사용하게 되면 1-step 방법보다 더 많은 계산이 필요하지만, 그 대가로 Reward가 여러 단계로 지연되는 경우 훨씬 더 빠른 학습 속도를 제공합니다. 따라서 On-line과 같이 데이터가 부족하고 반복적으로 처리할 수 없는 경우에는 Eligibility Trace를 사용하는 것이 좋습니다. 반면에, 시뮬레이션을 통해 데이터를 쉽게 생성할 수 있는 Off-line의 경우에는 Eligibility Trace를 사용하는데 큰 이점이 없는 경우가 많습니다. 이 때의 목표는 제한된 데이터에서 더 많은 것을 얻는 것이 아니라 가능한 한 빠르게 많은 데이터를 처리하는 것인데, Eligibility Trace로 인한 데이터의 속도 향상은 그만한 계산 비용를 소모할 가치가 없기 때문에 1-step 방법이 선호됩니다.&lt;/p&gt;

&lt;p&gt;이번 장은 내용이 길고 어려워서 그런지 깔끔하게 포스트를 작성하지 못한 것 같네요. 포스트를 먼저 게시한 다음 나중에 다시 읽어보며 조금씩 매끄럽게 수정하겠습니다.&lt;/p&gt;

&lt;p&gt;다음 장은 강화학습의 마지막 장인 Policy Gradient Methods입니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장에서 새로 배우는 Eligibility Trace는 강화학습의 기본 메커니즘 중 하나입니다. 예를 들어, TD($\lambda$)에서 $\lambda$는 Eligibility Trace를 사용한다는 것을 의미합니다. Q-learning과 Sarsa를 포함한 대부분의 TD 방법은 Eligibility Trace와 결합하여 보다 효율적으로 학습할 수 있습니다.</summary></entry><entry><title type="html">JBL Quantum 810</title><link href="http://localhost:4000/unboxing/jbl-quantum-810/" rel="alternate" type="text/html" title="JBL Quantum 810" /><published>2022-07-26T00:00:00+09:00</published><updated>2022-07-26T00:00:00+09:00</updated><id>http://localhost:4000/unboxing/jbl-quantum-810</id><content type="html" xml:base="http://localhost:4000/unboxing/jbl-quantum-810/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;안녕하세요, 어쩌다보니 지난번에 이어서 또 헤드셋을 개봉하게 되었네요. (정확하게 말하면 지난번에는 헤드폰이었고 이번에는 헤드셋이라는 차이가 있습니다만)&lt;/p&gt;

&lt;p&gt;원래 저는 커세어 보이드 무선 제품을 쭉 사용해오고 있었습니다. 착용감이 편하기도 했고, 제가 막귀라 음질에 크게 구애받지 않았기 때문에 나름 만족하며 사용했습니다. 다만 구매한지 5년이나 지나다보니 쿠션 부분이 다 떨어지고, 가죽 부분이 벗겨지는 바람에 어쩔 수 없이 헤드셋을 새로 구매했습니다. 마이크를 많이 사용하는 것은 아니었지만 마이크 소리가 너무 작아지는 문제도 있었습니다.&lt;/p&gt;

&lt;p&gt;이번에 새로 헤드셋을 구매하면서 가장 염두에 둔 것은 노이즈 캔슬링 기능이었습니다. 이 헤드셋을 구매할 당시 컴퓨터 본체의 소음이 너무 심해서 헤드셋을 껴도 차단이 안될 정도였거든요. (그런데 결국 컴퓨터 본체도 지난 주에 바꾸고 말았습니다) 다만 게이밍 헤드셋 중에 노이즈 캔슬링 기능이 있는 헤드셋은 많지 않았습니다. 처음에는 그냥 에어팟 맥스를 살까 하다가 가격이 너무 비싸기도 했고, 게이밍에 쓰기에는 지연시간이 너무 크다는 소리가 많아 그냥 게이밍 제품 중에서 찾아보기로 했습니다.&lt;/p&gt;

&lt;p&gt;그러다가 마침 찾은 것이 JBL 사의 제품이었습니다. 원하던 노이즈 캔슬링 기능도 들어가 있었고, JBL은 하만의 자회사이기 때문에 삼성의 AS 서비스를 받을 수 있기 때문입니다. 삼성 서비스 센터를 몇 번 방문해보았는데, AS 만큼은 굉장히 만족스러웠습니다.&lt;/p&gt;

&lt;p&gt;JBL의 게이밍 헤드셋 브랜드는 JBL Quantum인데, 구매할 당시 가장 최신 제품은 810 이었습니다. 문제는 불행히도 한국에 810은 아직 정식 발매가 안된 상황이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아무래도 JBL 사는 전통적인 음향 회사의 이미지가 있다보니 게이밍 기기의 수요가 많지 않아 정발이 늦는 것 같았습니다. JBL Quantum 810이 2022년 2월에 출시된 것으로 아는데 아직까지 감감무소식이거든요. 하는 수 없이 810 제품을 구하기 위해 백방으로 알아본 결과 JBL 영국 지사에서 판매하고 있는 것을 보았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한국으로 직배송이 안된다는 단점은 있었으나 가격적인 면을 비교해봐도 전 버전인 800과 크게 차이나지 않았습니다. 당장 급한 물건은 아니었으니 배대지를 이용해 구매하기로 결정했습니다. 굳이 이렇게 귀찮은 짓을 하면서까지 구매한 이유는 810이 800과 음질 자체는 큰 차이가 없으나, 배터리 지속 시간이 2배 이상 늘어나고(14시간 -&amp;gt; 30시간) 무게가 50g 정도 더 가벼워졌기 때문입니다. 이 정도면 충분히 직구를 할만한 가치가 있다고 생각했습니다.&lt;/p&gt;

&lt;p&gt;배송 기간은 약 열흘 정도 걸렸습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;패키지 디자인은 게이밍 제품 답게 상당히 요란합니다. 전면에는 오디오 포멧이 주르륵 나열되어 있고, 액티브 노이즈 캔슬링과 같은 기능이 나열되어 있습니다. 또한 하단에는 어떤 제품과 연결이 가능한지 나와있는데, PC와 플레이스테이션은 무선으로 연결 가능하고 XBOX와 스위치는 유선으로 연결 가능하다고 나와 있습니다. 이왕 무선을 지원해 줄거면 다 지원해주지… 라는 생각이 드네요.&lt;/p&gt;

&lt;p&gt;후면에도 호환되는 프로그램이나 제품 특징이 나와 있습니다. RGB 기능을 끄면 최대 43시간까지 가동이 가능하다고 하네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;신기한 것은 한국에 정발된 제품이 아님에도 한국어로 설명이 나와있다는 것입니다. 아무래도 JBL의 모기업이 삼성인 것과 관련이 있어 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래쪽에 보시면 “재생과 충전을 동시에” 라는 문구가 있습니다. 이 기능이 당연한 것임에도 불구하고 나온 이유는 위 댓글에서 보이듯이 전작인 800에서 재생과 충전이 동시에 되지 않았기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;옆면 디자인은 딱히 눈에 띄는 점은 없습니다. 3D 서라운드 어쩌고 하는 문구가 있는데 저 기능이 뭔지 아직은 잘 모르겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하단에는 하만의 로고가 나와있습니다. JBL이 하만 소속이기 때문에 당연합니다. 다만 삼성의 로고는 제품 어디에서도 찾을 수 없었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;겉 포장을 벗기고 나면 이렇게 검은색 종이 박스가 나옵니다. 포장 자체는 딱히 다른 제품에 비해 좋다거나 특이하다는 생각이 들지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;열어보니 헤드셋이 이렇게 천으로 된 주머니에 담겨있었습니다. 이것 자체에 딱히 불만이 있지는 않았습니다만 헤드셋이 박스에 고정되어 있지 않다는 점이 더 불만스러웠습니다. 이렇게 넣어줄 거면 쿠션이라도 넣어주던가요. 그래도 나름 가격이 있는 물건인데 포장을 좀 정성들여 해줄 수 있는거 아닌가라는 생각이 드네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/12.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;헤드셋 본체 아래에는 이렇게 USB 케이블이나 수신기 같은 것이 담겨 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/13.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;USB 케이블은 Type-C to Type-C 입니다. 요즘 대부분 Type-C를 쓰다 보니 이해가 되네요. 그리고 2.4Ghz 수신기는 가운데에 고정되어 담겨 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/14.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/15.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;헤드셋을 보관할 때는 이런 식으로 쿠션 부분이 돌아갑니다. 문제는 저게 고정되지 않아서 자기 혼자 막 빙글빙글 도는 문제가 있네요. 이 점은 좀 실망스럽습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/16.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽에는 이렇게 마이크 설정과 헤드셋 음량을 조절할 수 있는 기능이 달려 있습니다. 여기에서 가장 이해가 안가는 기능은 마이크 음소거 기능입니다. 마이크를 그냥 위로 올리면 자동으로 음소거가 되는데 뭐하러 저 기능을 또 만들어놨을까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/067/17.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오른쪽에는 전원을 켜는 부분과 블루투스 페어링 버튼이 있습니다. 저 부분을 테이프로 고정한 이유는 박스 안에서 켜질 수도 있기 때문이 아닐까로 생각하고 있습니다. 헤드셋이 박스에 고정되어 있지 않아서 그럴 가능이 있겠다 싶거든요.&lt;/p&gt;

&lt;h2 id=&quot;간단-후기&quot;&gt;간단 후기&lt;/h2&gt;

&lt;p&gt;구매하고 약 한 달 정도 사용해봤습니다만, 결론적으로 썩 마음에 들지 않았습니다. 돈 값을 못한다고 해야할까요? 기본적으로 불편한 점이 너무 많았습니다.&lt;/p&gt;

&lt;p&gt;첫 번째는 전원을 켜는 기능은 있으나 끄는 기능이 없다는 것입니다. 진짜입니다. 전원을 끄는 기능이 없기 때문에 사용 후 알아서 꺼지기를 기다려야 합니다.&lt;/p&gt;

&lt;p&gt;두 번째로는 착용감이 생각보다 불편했습니다. 아니, 이전에 사용한 커세어 보이드가 너무 편했던 것일까요? 제품 후기에 착용감이 편하다는 의견이 많았는데, 이게 헤드셋 중에서는 그나마 편한 편일지도 모르겠습니다. 저는 비교군이 많지 않기 때문에 객관적인 의견은 아닙니다. 다만 커세어 보이드나 에어팟 맥스와 비교해보면 확실히 불편합니다.&lt;/p&gt;

&lt;p&gt;세 번째로는 그렇게 자랑하던 노이즈 캔슬링 기능입니다. 물론 제가 써본 노이즈 캔슬링 제품은 에어팟 프로/맥스 밖에 없기 때문에 감안하고 봐주시기 바랍니다. 에어팟 시리즈에 비해면 이 제품은 노이즈 캔슬링 기능이 있나 싶을 정도로 잘 체감이 되지 않습니다. 지난번 에어팟 맥스 개봉기에서 에어팟 맥스의 노이즈 캔슬링이 단순히 헤드 쿠션 때문이 아닌가? 라고 후기를 남겼는데, 이 말을 취소해야겠습니다. 에어팟 맥스의 노이즈 캔슬링은 정말 대단한 것이었습니다. 물론 가격이 두 배 넘게 차이나는 제품이긴 하지만 그래도 음향 전문 회사다보니 이 기능에 대해 기대를 상당히 많이 했었는데 실망스럽기만 하네요.&lt;/p&gt;

&lt;p&gt;차라리 가격이라도 저렴했으면 싼 맛에 쓴다고 자기위안이라도 할 수 있었겠지만 그것도 아니라 기분이 좋지 않네요. 차라리 커세어 보이드 신형이나 살껄 그랬습니다.&lt;/p&gt;

&lt;p&gt;물론 이 의견은 주관적이며, 제가 모르는 이 헤드셋의 장점이 있을 수도 있습니다. 특히 타 제품과 비교하여 JBL 헤드셋의 음향 차이를 체감하실 수 있는 분이라면 그 돈에 맞는 가치를 할 수 있을 것 같습니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">초속 5센티미터 감상평</title><link href="http://localhost:4000/talk/5-centimeters-per-second/" rel="alternate" type="text/html" title="초속 5센티미터 감상평" /><published>2022-06-17T00:00:00+09:00</published><updated>2022-06-17T00:00:00+09:00</updated><id>http://localhost:4000/talk/5-centimeters-per-second</id><content type="html" xml:base="http://localhost:4000/talk/5-centimeters-per-second/">&lt;p&gt;지난 포스트에 이어 제출한 두 번째 과제입니다. 역시 기록을 위해 포스팅합니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;(주의) 마법소녀 마도카 마기카의 결말에 대한 스포일러가 포함되어 있습니다.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;과제-내용&quot;&gt;과제 내용&lt;/h2&gt;

&lt;p&gt;교재 ‘애니메이션에 빠진 인문학’의 내용과 7주차~11주차 수업 내용을 참고하여 작성하기 바랍니다.&lt;/p&gt;

&lt;p&gt;현대인의 개념, 속성, 가치관, 상황 등을 이해하기 쉽다고 판단되는 애니메이션 작품을 하나 선택하고(교재와 수업에 소개된 작품으로 한정함),&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;해당 작품에서 현대인의 어떤 점이 잘 드러났는지 구체적인 예를 들어 설명하고,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;작품에 드러난 그런 현대인의 일면이 실제 현대사회에서 어떤 의미(영향)를 가지는지 서술한 뒤,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;그런 의미나 영향에 대하여 자신은 어떻게 생각하는지 명확하고 구체적으로 논하시오.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;과제-애니메이션-목록&quot;&gt;과제 애니메이션 목록&lt;/h2&gt;

&lt;p&gt;7주차 : 현대인의 특징을 나타낸 애니메이션 - 천원돌파 그렌라간, 원피스&lt;/p&gt;

&lt;p&gt;8주차 : 현대사회의 문제점을 나타내는 애니메이션 - 강철의 연금술사, 충사, 진격의 거인&lt;/p&gt;

&lt;p&gt;9주차 : 미야자키 하야오 애니메이션 - 센과 치히로의 행방불명, 벼랑 위의 포뇨, 마루 밑 아리에티&lt;/p&gt;

&lt;p&gt;10주차 : 신카이 마코토 애니메이션 - 그녀와 그녀의 고양이, 누군가의 시선, 초속 5cm, 너의 이름은&lt;/p&gt;

&lt;p&gt;11주차 : 호쇼다 마모루 애니메이션 - 시간을 달리는 소녀, 늑대 아이, 괴물의 아이, 미래의 미라이&lt;/p&gt;

&lt;p&gt;저는 이 중 10주차의 초속 5cm를 선택하여 작성하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;제출한-과제&quot;&gt;제출한 과제&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;제목 : 추억은 정말 아름다운가&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“옛날이 좋았다”라는 말은 전 세계적으로, 세대에 상관없이 자주하는 말 중 하나이다. 이것은 심리학 용어로 Good-Old-Days bias (좋았던 옛날 편향)으로 부르며, 실제로 이 현상에 대해 발표한 논문이 존재한다 [1]. 하지만 “편향”이라는 단어에서부터 알 수 있듯이, 실제로 과거가 현재보다 좋은 것이 아니라 단지 추억에 그리움과 같은 보정이 들어가 있을 뿐이다.&lt;/p&gt;

&lt;p&gt;신카이 마코토의 《초속 5cm》는 바로 이러한 현대인의 향수를 자극한 애니메이션이다. 《초속 5cm》는 토오노 타카키라는 남자의 입장에서 바라본 추억을 나타내고 있다. 《초속 5cm》는 총 3부로 나뉘어져 있는데, 각각 유년기, 소년기, 그리고 청년기의 토오노 타카키를 보여준다. 이 영화 내내 토오노 타카키에게 중요하게 묘사되는 인물은 유년기에 만난 시노하라 아카리라는 여자아이다. 그러나 1부에서 두 사람은 이사로 인해 헤어지게 되고, 편지로 안부를 전하는 사이가 된다. 소년기를 다룬 2부에서도 토오노 타카키는 시노하라 아카리를 잊지 못하고 계속 연락하며, 3부에서는 다른 여자와 연애를 했다는 것이 묘사되지만 가슴 한켠에는 아직도 시노하라 아카리를 잊지 못하는 모습을 보여준다.&lt;/p&gt;

&lt;p&gt;그러나 이 애니메이션에서 묘사되는 토오노 타카키와 시노하라 아카리의 관계는 따지고 보면 그렇게 대단한 관계가 아니었음을 알 수 있다. 어린 시절 친하게 지낸 관계이기는 하나, 두 사람은 거리 문제로 인해 같이 지낸 시간은 잠깐뿐이기 때문이다. 나를 포함한 많은 현대인에게도 이렇게 누구나 한번쯤은 곱씹어볼만한 유소년기의 추억이 하나쯤은 있지만, 결국 나중에 생각해보면 사실 그것이 엄청 대단한 것은 아니었음을 알 수 있다. 그러나 많은 사람들이 이렇게 보정받은 추억에 매몰되어 정작 중요한 현재에 집중하지 못한다 [2]. 또는 현재의 힘든 상황에 무기력함을 느끼고 도피하려고 한다. 《초속 5cm》의 토오노 타카키 또한 마음속에 남아있는 시노하라 아카리의 존재로 인해 현재의 애인에게 집중하지 못하고, 무력감에 빠져 회사마저 퇴직하고 만다.&lt;/p&gt;

&lt;p&gt;물론 이러한 것들을 알고 있다고 해도 의식적으로 극복하는 것은 쉽지 않다. 머릿속으로는 현재 하고 있는 일에 집중해야 한다고 생각하지만, 정작 몸은 힘든 현실에 쉽게 무력감에 빠지고 만다. 나는 이럴 때 옛날이 정말 좋았는지를 진지하게 따져보곤 한다. 예를 들어, 스마트폰으로 인해 청소년, 청년들이 고립되어 외부와 단절된다는 뉴스 기사가 자주 보인다 [3]. 이것을 보고 스마트폰이 없던 옛날이 좋았다라고 말하는 사람도 종종 보인다. 하지만 스마트폰이 없던 시절과 현재의 소통은 방식만 달라졌을 뿐 큰 차이가 없다. 크게 외향적인 몇몇 사람은 눈에 띄겠지만, 그렇지 않은 대부분의 사람은 그 때도 조용히 몇몇 사람들과만 소통하였고, 지금은 그 수단이 SNS나 인터넷 커뮤니티와 같은 전자적인 수단으로 바뀌었을 뿐이다. 오히려 스마트폰이 없다면 간편 결제나 정보 검색 등을 할 수 없어 불편한 점만 생길 뿐이다.&lt;/p&gt;

&lt;p&gt;최근 각지에서 레트로 아이템이 유행하고 있다 [4] [5]. 이런 마케팅 열풍이 분다는 것은 그만큼 과거의 추억을 그리워하는 사람들이 많다는 뜻일 것이다. 적절한 향수는 미래로 나아가는 동력이 될 수 있지만, 그것에 얶매이게 되면 오히려 장애물이 될 수 있음을 상기하며 글을 마친다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;참고문헌&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[1] Eibach, R. P., &amp;amp; Libby, L. K. (2009). Ideology of the good old days: Exaggerated perceptions of moral decline and conservative politics. Social and psychological bases of ideology and system justification, pp. 402-423.&lt;/p&gt;

&lt;p&gt;[2] 박병률, “[영화 속 경제]좋은 기억만 떠올리는 ‘므두셀라 증후군’”, 경향신문, 2020.03.02.&lt;/p&gt;

&lt;p&gt;[3] 고혜지, 송수연, “하루 10시간, 스마트폰 세상에 갇혀 어느새 ‘학포자’… 게임 캐릭터 친구뿐”, 서울신문, 2021.02.17.&lt;/p&gt;

&lt;p&gt;[4] 신미진, “”내가 산 건 ‘추억’이야”…레트로 아이템에 신난 어른들”, 서울경제, 2022.05.28.&lt;/p&gt;

&lt;p&gt;[5] 신원선, “MZ세대는 왜 레트로에 열광할까… 韓 사회 현상 진단”, 메트로신문, 2022.05.29&lt;/p&gt;

&lt;h2 id=&quot;교수님의-피드백&quot;&gt;교수님의 피드백&lt;/h2&gt;

&lt;p&gt;우선 전근대나 근대인과 다른 현대인의 특징 관련해서 특화하여 서술하지 않은 것이 다음의 논의가 현대인에 대한 것이라기보다는 특정 작품과 그에 대한 느낌 정도로 받아들여질 우려가 있습니다. 현대인이 아니라 다른 시대를 살았던 사람들에게도 받아들여질 수 있는 과거 지향 혹은 향수 등은 현대인을 키워드로 하는 과제의 성격이나 내용으로 볼 때 다소 거리가 있게 느껴집니다. 만일 그것이 현대인의 특질이나 현상, 문제와 연결시키려고 한다면 다른 논거를 예로 들어 현대사회에서 그러한 과거지향이 다른 시대에 비해 두드러진다던가 혹은 사회적 이슈가 되어 문제의 소지가 있다던가 하는 연결 고리를 만들어줄 필요가 있겠습니다.&lt;/p&gt;

&lt;p&gt;사실 신카이 마코토 애니메이션에 보이는 다소 감성적인 측면과 과거의 소환 등은 해석에 있어 이론의 여지가 있습니다. 단순히 과거를 회상하고 그 시절에 비교하여 현재 잃어버린 것을 안타까워하는 느낌으로 해석하거나 서술하신 것처럼 과거 지향적인 시각으로 평가하기도 합니다. 그런데 신카이 마코토의 전체 작품을 보면 과거를 소환하되 언제나 현재를 중심에 놓고 있다고 생각되기도 합니다. 즉 과거를 추억거리나 감상의 소재가 아니라 현재를 파악하고 현재의 소중함을 얘기하고자 하는 것은 아닐까 생각되기도 하는 것입니다.&lt;/p&gt;

&lt;p&gt;과제는 요구하는 것이 어느 정도 틀에 규정되기 쉽습니다. 평가 때문에 그렇습니다. 그런 틀 속에서 보았을 때의 과제에 대한 코멘트였다는 점을 말씀 드립니다. 제출하신 과제는 그런 틀이 아니라면 정돈되고 깔끔하게 정리된 서술이라고 생각됩니다. 한 학기 열심히 참여해 주시고 과제도 제출해 주신 점 고맙고, 응원합니다. 수고하셨습니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="talk" /><summary type="html">지난 포스트에 이어 제출한 두 번째 과제입니다. 역시 기록을 위해 포스팅합니다.</summary></entry><entry><title type="html">마법소녀 마도카☆마기카 감상평</title><link href="http://localhost:4000/talk/puella-magi-madoka-magica/" rel="alternate" type="text/html" title="마법소녀 마도카☆마기카 감상평" /><published>2022-06-13T00:00:00+09:00</published><updated>2022-06-13T00:00:00+09:00</updated><id>http://localhost:4000/talk/puella-magi-madoka-magica</id><content type="html" xml:base="http://localhost:4000/talk/puella-magi-madoka-magica/">&lt;p&gt;이번 학기에 일본 애니메이션과 관련한 강의를 청강하면서 제출한 과제입니다. 대단한 글은 아니지만, 기록을 위해 포스트로 남깁니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;(주의) 마법소녀 마도카 마기카의 결말에 대한 스포일러가 포함되어 있습니다.&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;과제-내용&quot;&gt;과제 내용&lt;/h2&gt;

&lt;p&gt;2주차~6주차까지 수업에서 소개된 일본 애니메이션 장르 중 하나를 선택하고, 해당 장르가 현대인 및 현대사회를 잘 반영하고 있다고 생각하는 근거를 구체적인 작품(교재와 수업에 소개된 작품에 한정함)을 예로 들면서 서술하고, 그렇게 반영되어 있다고 생각하는 현대인 혹은 현대사회의 일면에 대한 자신의 생각((애니메이션에 대한 생각이 아님)을 명확하고 구체적으로 논하시오.&lt;/p&gt;

&lt;h2 id=&quot;과제-애니메이션-목록&quot;&gt;과제 애니메이션 목록&lt;/h2&gt;

&lt;p&gt;2주차 : 로봇 애니메이션 - 철완 아톰, 기동전사 건담, 신세기 에반게리온, 공각기동대&lt;/p&gt;

&lt;p&gt;3주차 : 스포츠 애니메이션 - 더 파이팅, 뱀부 블레이드, 원아웃, 자이언트 킬링&lt;/p&gt;

&lt;p&gt;4주차 : 마법소녀 애니메이션 - 마법소녀 마도카☆마기카, 마녀 배달부 키키, 메리와 마녀의 꽃&lt;/p&gt;

&lt;p&gt;5주차 : 영어덜트 애니메이션 - 공의 경계, 카이트 리버레이터, 성인 여성의 아니메 타임, 아인&lt;/p&gt;

&lt;p&gt;6주차 : 일상물 애니메이션 - 럭키스타, 역시 내 청춘 러브코미디는 잘못됐다, 남자 고교생의 일상&lt;/p&gt;

&lt;p&gt;저는 이 중 4주차의 마법소녀 마도카☆마기카를 선택하여 작성했습니다.&lt;/p&gt;

&lt;h2 id=&quot;제출한-과제&quot;&gt;제출한 과제&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;제목 : 현대인의 희생에 대한 인식의 변화&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;희생(犧牲)은 과거부터 현재까지 숭고한 것으로 여겨왔다. 나라를 위해 목숨을 바친 희생, 사랑하는 사람을 지키기 위한 희생, 또는 위험한 상황에 빠진 모르는 사람을 구하기 위한 희생 등 각각 이유는 다르지만, 희생에 대한 기록은 어디에서나 쉽게 찾아볼 수 있다. 게다가 그러한 희생을 영웅시하는 것은 현실에서도 물론이고, 창작물에서도 찾아보기 어렵지 않다.&lt;/p&gt;

&lt;p&gt;하지만 현대에 들어서 희생에 대한 인식이 변화하기 시작했다. 책 《애니메이션에 빠진 인문학》에 따르면, 현대인은 중세인이나 근대인에 비해 “개인”에 대한 인식이 확고하기 때문에 국가나 인류를 위해 자신을 희생하는 것을 원치 않는다고 한다. 이를 확연하게 보여줄 수 있는 현실의 예시가 바로 군대에 대한 인식의 변화이다. 불과 20년 전만 해도 한국에서 군대는 남자로 태어났으면 당연히 가야 하는 것으로 생각하였지만, 현대를 살아가는 젊은 세대들은 “군대는 뺄 수 있으면 무조건 빼는 것이 좋다”라는 생각으로 바뀌어버렸다. 심지어는 “전쟁이 나도 나라를 위해 싸우다가 죽는 것은 바보 같은 짓이고, 나는 해외로 도망칠 것이다”라는 의견을 공개적으로 표현해도 그를 비난하는 사람이 많지 않다. 이로 인한 원인은 복합적이지만, 타인을 위해 자신을 희생하는 것이 회의적으로 변하고 있는 것은 사실이다.&lt;/p&gt;

&lt;p&gt;이러한 희생에 대한 의식의 변화는 애니메이션에서도 드러난다. 1988년에 개봉한 &amp;lt;기동전사 건담 : 역습의 샤아&amp;gt;에서는 지구로 낙하하는 엑시즈를 막기 위해 아무로 레이가 홀로 모빌슈트에 탑승하여 손으로 막는다. 그 모습을 보고 다른 사람들도 모빌슈트에 탑승해 아무로 레이에게 동참하는데, 수많은 사람들의 의지가 합쳐 사이코 필드가 펼쳐침으로써 엑시즈의 추락을 저지하게 된다. 이 과정에서 아무로 레이는 결국 사망하고, 후속작 &amp;lt;기동전사 건담 : UC&amp;gt;에서 아무로 레이는 영웅으로 추서되었다고 표현된다. 이는 희생에 대한 숭고함이 남아있던 근대인의 생각이 반영되었다고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;하지만 2011년에 방영한 &amp;lt;마법소녀 마도카☆마기카&amp;gt;에서는 희생의 대해 매우 부정적으로 묘사한다. &amp;lt;마법소녀 마도카☆마기카&amp;gt;에서 묘사되는 마법소녀는 마녀와 싸울 수 있는 특별한 힘을 얻는 대신 본체라고 할 수 있는 소울 젬을 계속 정화해주어야 하는데, 소울 젬을 정화할 수 있는 그리프 시드를 얻기 위해서는 마녀를 죽여야 한다. 소울 젬을 정화하지 못하면 마법소녀는 마녀가 된다. 즉, 마법소녀의 미래는 마녀와 싸우다 죽거나, 마녀로 타락하는 것 밖에 없다. 이 가련한 운명을 대가로 마법소녀는 원하는 소원을 한 가지 이룰 수 있다.&lt;/p&gt;

&lt;p&gt;&amp;lt;마법소녀 마도카☆마기카&amp;gt;에 등장하는 5명의 마법소녀는 모두 각자의 이유로 희생을 하지만, 작품 내에서 그 희생이 보답 받지 못한다.&lt;/p&gt;

&lt;p&gt;미키 사야카는 좋아하는 남학생의 팔을 치료해달라는 소원을 빌었으나, 그 남학생은 사야카에게 호감이 있다는 묘사가 나오지 않는다. 오히려 사야카의 친한 친구가 그 남학생과 이어지는 듯한 묘사가 나오며, 결국 미키 사야카는 이로 인해 절망을 느끼고 마녀로 변하고 만다.&lt;/p&gt;

&lt;p&gt;사쿠라 쿄코는 성직자인 아버지의 목소리를 다른 사람들이 귀기울여달라는 소원을 빌었으나, 아버지는 자신의 목소리를 듣는 이유가 마법 때문임을 알고 오히려 절망하고, 가족들을 모두 죽인 후 자살하고 만다. 나중에 마녀가 된 미키 사야카를 다시 마법소녀로 되돌리려 하다 실패하고 본인마저 희생하였으나, 그 후 큐베로부터 사쿠라 쿄코의 시도는 애초에 무의미하였다는 확실한 묘사가 나온다.&lt;/p&gt;

&lt;p&gt;토모에 마미는 작중에서 나온 사쿠라 쿄코처럼 이기적으로 행동할 수도 있었으나, 유일하게 자신이 살기 위해 소원을 사용한 후, 마법소녀로써의 의무를 자각하고 미타키하라 시의 시민들을 지키기 위해 싸웠다. 그러나 이를 알고 있는 사람도, 같이 싸우는 동료조차 없었고, 마법소녀 후보인 마도카와 사야카를 만났지만 결국에는 혼자 싸우다 외롭게 죽고 만다.&lt;/p&gt;

&lt;p&gt;아케미 호무라는 유일한 친구인 마도카의 운명을 바꾸기 위해 몇 번이고 시간을 되돌렸다. 시간을 되돌려도 호무라 자신은 계속 기억이 남기 때문에 계속 마도카가 죽는 운명을 보면서 마음이 마모되었으나, 마도카를 구하겠다는 일념 하나로 버텼다. 하지만 호무라가 아무리 시간을 되돌려도 마도카를 구할 수는 없었고, 그저 마도카의 인과율만 늘리는 결과가 되어버리고 만다.&lt;/p&gt;

&lt;p&gt;카나메 마도카의 경우에는 조금 다르게 보일 수도 있다. 마지막화에서 카나마 마도카의 희생으로써 마녀의 존재를 없애는 장면은 유일하게 긍정적으로 보일 수 있는 희생이었지만, 마도카의 희생은 애초부터 선택의 여지가 없었다. 마도카가 희생하지 않는다면 발푸르기스의 밤을 막지 못해 마도카와 호무라는 물론이고 가족들을 포함해 수많은 사람들이 죽었을 가능성이 높기 때문이다. 게다가 후속작인 &lt;반역의 이야기=&quot;&quot;&gt;를 토대로 마도카가 정말로 원했던 것은 가족, 친구, 그리고 소중한 사람들과 헤어지고 싶지 않다는 대사를 통해 마도카가 정말로 원하던 희생이 아니었다는 결론을 도출할 수 있다. 즉, 본인이 의지대로 희생을 한 것이 아니라 희생을 당한 것에 가깝다.&lt;/반역의&gt;&lt;/p&gt;

&lt;p&gt;제작자가 &amp;lt;마법소녀 마도카☆마기카&amp;gt;를 통해 의도하고 싶은 것은 한 가지가 아니겠지만, 마법소녀들의 이러한 작중 행적을 볼 때 희생에 대한 기존의 인식을 부정했다는 것만은 분명하다.&lt;/p&gt;

&lt;p&gt;국어사전에 따르면, 희생의 사전적 의미는 다른 사람이나 어떤 목적을 위하여 자신의 목숨, 재산, 명예, 이익 따위를 바치거나 버리는 것이다. 그러나 또 다른 의미로 사고나 자연재해 따위로 애석하게 목숨을 잃는다는 뜻도 있다. 현대인이 희생에 대해 점점 부정적으로 생각하는 것을 볼 때, 과거의 사람들이 희생을 첫 번째 의미로 인식했다면, 현대인은 점점 두 번째 의미로 인식하는 단계라고 나는 생각한다.&lt;/p&gt;

&lt;h2 id=&quot;교수님의-피드백&quot;&gt;교수님의 피드백&lt;/h2&gt;

&lt;p&gt;희생을 키워드로 &lt;마법소녀 마도카=&quot;&quot; 마기카=&quot;&quot;&gt;를 분석한 글입니다. 장르로서의 마법소녀물이 현대사회와 가지는 연결성을 먼저 서술하는 것이 좋습니다. 키워드 설명 후에 일반적인 혹은 전체적인 마법소녀물에 희생이라는 키워드가 통하는 것이라는 것을 설명해야 이후 글이 설득력이 있습니다. 이 글은 장르에 대한 것보다는 특정 작품 분석에 가깝습니다. 또한 현대사회에서 희생에 대한 인식이나 가치관이 바뀌었다는 것을 주장할 수 있는 근거 제시가 다소 불충분합니다. 여론조사나 실제 사건 등의 지표가 없이는 설득력을 높일 수 없습니다.  마지막으로 자신의 의견 제시를 보다 분명하게 하고 그 주장의 근거나 논리를 보다 명확하게 할 필요가 있습니다. 대부분 의견을 진술하라고 하면 일반적인 혹은 선언적인 되풀이가 되는 경우가 많습니다. 하지만 그것은 너무 포괄적이면서 추상적인 것으로 구체성이 떨어질 우려가 많습니다. 개인적 차원에서 나의 행동반경과 현실적인 카테고리 내에서 실천할 수 있고 해결 가능한 것을 찾아 생각을 정리하고 의견으로 주장하는 것이 보다 긍정적이라고 생각합니다.&lt;/마법소녀&gt;&lt;/p&gt;

&lt;p&gt;이상 제출하신 과제 글에 대한 코멘트입니다. 전체적으로 정돈된 문장과 구성으로 이해하기 쉬운 논리적 연결성을 가지고 있다고 생각합니다. 위의 코멘트를 참고하시면 보다 설득력 있고 풍부한 내용의 글이 가능하리라 생각됩니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="talk" /><summary type="html">이번 학기에 일본 애니메이션과 관련한 강의를 청강하면서 제출한 과제입니다. 대단한 글은 아니지만, 기록을 위해 포스트로 남깁니다.</summary></entry><entry><title type="html">에어팟 맥스</title><link href="http://localhost:4000/unboxing/apple-airpods-max/" rel="alternate" type="text/html" title="에어팟 맥스" /><published>2022-06-05T00:00:00+09:00</published><updated>2022-06-05T00:00:00+09:00</updated><id>http://localhost:4000/unboxing/apple-airpods-max</id><content type="html" xml:base="http://localhost:4000/unboxing/apple-airpods-max/">&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/00.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;안녕하세요, 정말 오랜만에 개봉기 포스트를 작성합니다. 요즘 뭔가 물건을 살 일이 없어서 개봉기 포스트를 쓰고 싶어도 쓸 수가 없었네요. 사실 요즘도 비싼(?) 전자제품은 잘 사지 않습니다만, 이번에는 아는 후배에게 제품을 제공받아 개봉하게 되었습니다.&lt;/p&gt;

&lt;p&gt;오늘 개봉할 제품은 에어팟 맥스입니다. 제가 애플 제품을 좋아하긴 합니다만, 헤드셋에 70만원 이상을 쓰는 것은 도저히 엄두가 안나서 잘되었다 싶었습니다. 이런 기회가 아니면 에어팟 맥스를 써볼 일이 앞으로도 없을 것 같거든요. 착한 후배는 마침 제가 블로그를 하는 것을 알고 기꺼이 첫 개봉의 기회를 저에게 제공해주었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/01.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;애플 공식 홈페이지에서 주문하면 보통 이런식으로 배송이 됩니다. 예전에 아이패드하고 에어팟도 공식 홈페이지에서 구매했었는데, 애플만의 박스 포장 방식이 있는 것 같았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/02.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;애플의 박스 포장은 이렇게 칼을 쓰지 않고 박스를 개봉할 수 있다는 특징이 있습니다. 가운데에 저 부분을 잡아 당기면 바로 박스가 열리는 구조로 되어있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/03.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저 부분을 잡고 이렇게 쭉 당기면&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/04.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;박스가 위 아래로 열리는 구조입니다. 박스 크기가 딱 제품 박스에 맞게 되어 있어서, 흔히 택배 박스에 들어 있는 뽁뽁이 같은 것은 없습니다. 포장이 깔끔한 것은 좋은데, 저는 이렇게 포장하면 충격에 약할 것 같다는 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/05.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;박스를 제거하면 패키지는 애플 답게 흰색으로 디자인 되어있습니다. 최근 애플 패키지는 비닐 포장을 없애고 종이로만 포장한다고 하는데, 에어팟 맥스는 아직까지도 비닐로 포장되어 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/06.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;뒷면에는 이렇게 에어팟 맥스가 케이스에 들어있는 그림이 그려져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/07.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;케이스 그림을 보고 설마 케이스까지 들어있나? 싶었는데 저 부분을 보니 진짜 들어있는게 맞았습니다. 아무래도 에어팟 맥스는 휴대성을 염두에 두고 만든 제품이다 보니 케이스를 넣어준 것 같습니다. 조금 의외였던 것은 애플이라면 케이스를 당연히 별도로 팔 줄 알았거든요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/08.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;케이스에 씌여진 비닐도 저 부분을 잡아 당기면 칼 없이 깔끔하게 제거할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/09.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;패키지를 열어보니 에어팟 맥스 본체가 케이스에 끼워진 채로 담겨 있습니다. 회색 빛이 도는 것은 반투명 종이에 한 겹 더 쌓여있기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/10.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;패키지 뚜껑 부분에는 아무것도 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/11.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본체를 먼저 꺼내봤습니다. 헤드셋이라 어느 정도 클 것이라고 예상했는데, 생각보다 작아서 놀랐습니다. 물론 이것은 크기를 줄여 케이스에 넣은 상태라 그런 것이고, 실제로 사용할 때는 이것보다는 약간 더 커집니다. 그래도 이 정도면 문제없이 가방에 쉽게 넣어다닐 수 있을 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/12.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본체 아래쪽에는 품질 보증서가 들어있는 종이 뭉치와 USB-C to Lightning 케이블이 있습니다. 아쉽게도 충전기는 들어있지 않기 때문에, 별도로 구매를 하던가 아니면 컴퓨터에 연결하여 충전해야 합니다. 이제는 아이폰에도 충전기를 안넣어주기 때문에 딱히 놀랍지도 않네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/13.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;혹시나 애플 스티커가 들어있을까 싶어서 종이 뭉치를 열어봤는데, 들어있지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/14.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본체에 씌여져 있던 반투명 비닐을 벗기면 이런 모양입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/15.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;케이스와 에어팟 맥스 본체를 분리했습니다. 케이스 안쪽을 보여드리면 이렇게 아래쪽이 반쯤 뚫려 있는 구조입니다. 윗 덮개는 자석으로 붙는 구조인데, 자석이 생각보다 강해서 쉽게 떨어지지 않을까 하는 걱정은 안하셔도 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/16.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;처음에는 이어패드 색이 본체와 다른가 싶었는데, 알고보니 초기에는 흰색 종이로 이어패드가 덮여 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/17.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;흰색 종이 포장을 제거하고 이어패드를 착용할 때처럼 돌려보니 이런 느낌입니다. 제 손이 조금 크긴 한데, 그걸 감안해도 에어팟 맥스는 생각보다 작다고 생각합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/18.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오른쪽 위에는 애플 워치와 비슷하게 다이얼과 버튼이 있습니다. 버튼의 용도는 아마 전원일테고, 다이얼은 볼륨 조절인 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/19.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;충전 포트는 오른쪽 아래에 있습니다. 당연히 애플 제품 답게 Lightning 포트가 달려 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/20.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/21.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;에어팟 맥스는 대칭형 모양이기 때문에 어느 쪽이 왼쪽이고 오른쪽인지 구분이 잘 안됩니다. 그래서인지 이어패드 안쪽에 저렇게 R, L로 왼쪽과 오른쪽을 표시해 놓았습니다. 이걸 굉장히 작게 표시해둔 헤드셋도 있는데, 이건 확실히 마음에 드네요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Unboxing/066/22.jpg&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본체와 케이스를 따로 놓고 찍어봤습니다. 실제로 착용할 때는 프레임 부분을 늘려야합니다.&lt;/p&gt;

&lt;p&gt;헤드셋이 가지고있는 가장 큰 문제점은 안경을 쓴 상태에서 착용했을 때 불편한 경우가 많다는 것인데요, 제가 착용해보니 안경이 크게 거슬리지 않을 정도로 편안했습니다. 그 점에서는 높은 점수를 주고 싶네요.&lt;/p&gt;

&lt;p&gt;다만 문제는 노이즈 캔슬링입니다. 제가 현재 쓰고 있는 에어팟 프로와 노이즈 캔슬링을 비교해보면 확실히 주변 소음 차단은 에어팟 맥스가 더 우수했습니다. 하지만 막귀인 제 느낌으로는 이게 노이즈 캔슬링 기능 자체가 더 뛰어난 것인지, 아니면 이어패드의 쿠션때문에 그렇게 느껴지는 것인지 구분이 잘 되지 않았습니다. 음질도 에어팟 프로와 비교했을 때 2배가 넘는 돈을 내고 그것을 체감할 수 있는가? 도 솔직히 잘 모르겠습니다. 물론 제가 막귀라서 그런 것일수도 있고 기껏해야 하루 잠깐 써봤기에 그럴 수도 있습니다. 음질에 민감하신 분들은 체감을 하실 수도 있습니다만, 저처럼 음질에 크게 신경쓰지 않는 분이라면 아무리 애플 제품을 좋아하시는 분이라도 구매 전에 다시 한번 생각해보시면 좋겠습니다. 일단 저는 이 제품을 70만원에 살 생각이 전혀 없습니다.&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="interests" /><category term="unboxing" /><summary type="html"></summary></entry><entry><title type="html">Off-policy Methods with Approximation</title><link href="http://localhost:4000/rl/off-policy-methods-with-approximation/" rel="alternate" type="text/html" title="Off-policy Methods with Approximation" /><published>2022-06-03T00:00:00+09:00</published><updated>2022-06-03T00:00:00+09:00</updated><id>http://localhost:4000/rl/off-policy-methods-with-approximation</id><content type="html" xml:base="http://localhost:4000/rl/off-policy-methods-with-approximation/">&lt;p&gt;이 책은 5장 이후로 Generalized Policy Iteration (GPI)에서 내재된 Exploitation과 Exploration 사이의 Trade-off를 처리하는 방법으로 On-policy와 Off-policy를 사용했습니다. 9장과 10장에서는 On-policy의 경우를 Function Approximation로 처리했으며, 이번 장에서는 Off-policy에서의 Function Approximation을 다룰 예정입니다. Off-policy 방법을 Function Approximation로 확장하는 것은 On-policy의 경우에서와 다른 점도 많고 어려운 점도 많습니다.&lt;/p&gt;

&lt;p&gt;6장과 7장에서 소개한 Tabular 형식의 Off-policy 방법은 Semi-gradient 알고리즘으로 확장하기 쉽지만, On-policy의 경우처럼 강력하게 수렴하지 않습니다. 이번 장에서는 Linear Function Approximation을 통해 Learnability의 개념을 소개한 다음, Off-policy의 수렴 문제와 더 강력한 수렴 보장을 가진 새로운 알고리즘에 대해 논의하겠습니다. 안타깝게도 새로운 알고리즘은 On-policy의 경우에서보다 이론적으로도, 경험적으로도 강력하지 않습니다. 이 과정에서 Off-policy 학습 뿐만 아니라 On-policy 학습에 대해서도 더 깊게 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Off-policy는 Target Policy $\pi$와 Behavior Policy $b$로 분리하여 Value Function을 학습합니다. Value를 추정할 때 두 Policy는 모두 정적으로 주어졌으며, State-Value $\hat{v} \approx v_{\pi}$나 Action-Value $\hat{q} \approx q_{\pi}$ 중 하나를 학습했습니다. Control에서는 Action-Value를 학습하고 $\pi$는 Greedy Policy, $b$는 $\epsilon$-greedy와 같은 탐색적인 Policy로 변경됩니다.&lt;/p&gt;

&lt;p&gt;Off-policy 학습은 크게 두 부분으로 나눌 수 있습니다. 하나는 Tabular 경우에서 발생하고, 다른 하나는 Function Approximation에서만 나타납니다. 첫 번째 부분은 Update Target ($\ne$ Target Policy)과 관련이 있고 두 번째 부분은 Update Distribution과 관련이 있습니다. 첫 번째 부분은 5장과 7장에서 다루었던 Importance Sampling과 관련이 있습니다. Importance Sampling은 Variance를 증가시킬 수 있지만 필수불가결합니다. 첫 번째 부분에서는 Function Approximation에서 이것의 확장을 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;On-policy Distribution은 Semi-gradient 방법의 안정성을 위해 중요했습니다. 하지만 Off-policy의 경우에는 Update Distribution이 On-policy Distribution을 따르지 않기 때문에 이와 다른 접근 방식이 필요합니다. 첫 번째로 생각해볼 수 있는 방법은 Importance Sampling을 사용함으로써 Update Distribution을 On-policy Distribution으로 변형하여 Semi-gradient 방법이 수렴되도록 보장하는 것입니다. 두 번째로는 특별한 Distribution에 의존하지 않는 새로운 Gradient 방법을 개발하는 것입니다. 두 번째 부분에서는 이것에 대해 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;이 부분에서 다루는 내용은 아직 연구가 진행중인 부분이기 때문에 실제로 무엇이 가장 효과적인지는 확실하게 언급할 수 없습니다.&lt;/p&gt;

&lt;h2 id=&quot;semi-gradient-methods&quot;&gt;Semi-gradient Methods&lt;/h2&gt;

&lt;p&gt;가장 먼저 이전 장에서 배운 Semi-gradient 방법으로 Off-policy를 Function Approximation으로 확장하는 것으로 시작하겠습니다. 이 방법은 Off-policy에서 첫 번째 부분(Update Target 변경)을 해결하지만 두 번째 부분(Update Distribution 변경)은 해결하지 않습니다. 따라서 이 방법은 일부 경우에서 발산할 위험이 있지만, 일반적으로는 성공적으로 사용할 수 있습니다. 이 방법은 Tabular에서 안정적이고 점근적으로 Bias되지 않지만, Function Approximation에서만 발생하는 문제입니다. 이러한 단점에도 불구하고 이 방법은 간단하기 때문에 첫 주제로 다루는 것이 좋습니다.&lt;/p&gt;

&lt;p&gt;Off-policy 알고리즘을 Semi-gradient 방법으로 변환하려면 Approximate Value Function($\hat{v}$또는 $\hat{q}$)와 Gradient를 이용하여 배열($V$ 또는 $Q$)에 대한 Update를 Weight Vector $\mathbf{w}$에 대한 Update로 바꾸면 됩니다. 이러한 알고리즘은 다음과 같은 단계별 Importance Sampling Ratio를 사용합니다.&lt;/p&gt;

\[\rho_t \doteq \rho_{t:t} = \frac{\pi (A_t | S_t)}{b(A_t | S_t)}\]

&lt;p&gt;예를 들어, 1-step State-Value 알고리즘은 $\rho_t$가 추가된 것만 제외하면 On-policy 알고리즘과 같은 &lt;span style=&quot;color:red&quot;&gt;Semi-gradient Off-policy TD(0)&lt;/span&gt;입니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \rho_t \delta_t \nabla \hat{v}(S_t, \mathbf{w}_t) \tag{11.2}\]

&lt;p&gt;여기서 $\delta_t$는 문제가 어떻게 정의되었는지에 따라 다릅니다. 만약 Epsidoic이고 Discounted를 사용한다면 다음 식 (11.3)처럼, Continuing이고 Average Reward를 사용한다면 식 (11.4)과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\delta_t &amp;amp; \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{11.3} \\ \\
\delta_t &amp;amp; \doteq R_{t+1} - \bar{R}_t + \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{11.4}
\end{align}\]

&lt;p&gt;만약 Action-Value인 경우라면, 1-step 알고리즘은 다음과 같은 &lt;span style=&quot;color:red&quot;&gt;Semi-gradient Expected Sarsa&lt;/span&gt;입니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_t + \alpha \delta_t \nabla \hat{q} (S_t, A_t, \mathbf{w}_t), \tag{11.5} \\ \\
\delta_t &amp;amp; \doteq R_{t+1} + \gamma \sum_a \pi(a|S_{t+1}) \hat{q} (S_{t+1}, a, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \tag{episodic} \\ \\
\delta_t &amp;amp; \doteq R_{t+1} - \bar{R}_t + \sum_a \pi (a|S_{t+1}) \hat{q}(S_{t+1}, a, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \tag{continuing}
\end{align}\]

&lt;p&gt;이 알고리즘은 Importance Sampling을 사용하지 않습니다. Tabular의 경우 유일한 Sample Action이 $A_t$이기 때문에 다른 Action을 고려할 필요가 없으므로 적절합니다. 만약 Function Approximation을 사용하면 서로 다른 State-Action 쌍이 서로 다른 Weight를 적용하면서 동일한 근사값에 반영될 수 있으므로 애매합니다. 이 문제를 해결하기 위해서는 강화학습에서 Function Approximation 이론에 대해 조금 더 깊게 파고들 필요가 있습니다.&lt;/p&gt;

&lt;p&gt;이 알고리즘을 $n$-step 버전으로 일반화할 때 State-Value 및 Action-Value는 모두 Importance Sampling을 포함합니다. Semi-gradient Sarsa의 $n$-step 버전은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+n} &amp;amp; \doteq \mathbf{w}_{t+n} + \alpha \rho_{t+1} \cdots \rho_{t+n} \left[ G_{t:t+n} - \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_{t+n-1}) \tag{11.6} \\ \\
G_{t:t+n} &amp;amp; \doteq R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}) \tag{episodic} \\ \\
G_{t:t+n} &amp;amp; \doteq R_{t+1} - \bar{R}_t + \cdots + R_{t+n} - \bar{R}_{t+n-1} + \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}) \tag{continuing}
\end{align}\]

&lt;p&gt;이 때, Episode의 마지막을 처리하는 방법은 이전에 배운 $n$-step 버전과 약간 다릅니다. 식 (11.6)에서 $k \ge T$에 해당하는 $\rho_k$는 모두 1로 처리하고, $t+n \ge T$에 해당하는 $G_{t:t+n}$은 모두 $G_t$로 처리합니다.&lt;/p&gt;

&lt;p&gt;또한 7장에서 Importance Sampling을 사용하지 않은 Off-policy 알고리즘인 $n$-step Tree-backup 알고리즘도 배운 적이 있습니다. 이것을 Semi-gradient 버전으로 바꾸면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+n} &amp;amp; \doteq \mathbf{w}_{t+n-1} + \alpha \left[ G_{t:t+n} - \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_{t+n-1}) \tag{11.7} \\ \\
G_{t:t+n} &amp;amp; \doteq \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) + \sum_{k=t}^{t+n-1} \delta_k \prod_{i=t+1}^k \gamma \pi (A_i | S_i) \tag{11.8}
\end{align}\]

&lt;p&gt;식 (11.8)의 $\delta_t$는 위의 Expected Sarsa 부분에 나온 식과 동일합니다. 이 외에도 7장에서는 모든 Action-Value 알고리즘을 통합하는 $n$-step $Q(\sigma)$를 정의했습니다만, 이 식의 Semi-gradient 변형은 생략하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;examples-of-off-policy-divergence&quot;&gt;Examples of Off-policy Divergence&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 Off-policy 학습의 두 번째 부분에 대해 다루겠습니다. 가장 먼저, 지난 Section 첫 부분에서 언급했던 Off-policy 학습에서 생길 수 있는 문제점을 알아보겠습니다. 특히, Semi-gradient나 다른 알고리즘이 불안정하고 발산하는 경우입니다.&lt;/p&gt;

&lt;p&gt;Off-policy가 어떤 상황에서 문제가 발생하는지 직관적으로 알기위해 간단한 예시를 하나 소개하겠습니다. MDP 중 일부에서 다음과 같은 2개의 State가 있다고 가정합니다. 여기서 매개변수 Vector $\mathbf{w}$는 단일 구성 요소 $w$로만 구성됩니다. 만약 두 State에 대한 Feature Vector가 각각 단순한 숫자로 이루어진 경우, Linear Function Approximation을 할 수 있습니다. 여기서는 각 State의 Value를 $w$와 $2w$로 설정합니다. 첫 번째 State에서는 Reward 0을 받고 두 번째 State로 이동하는 단 한 개의 Action만 존재합니다. 이것을 도식화하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 간단한 예제에서 어떻게 발산이 발생하는지 계산해보겠습니다. 먼저 초기에 $w = 10$이라고 가정합니다. 그러면 Estimated Value가 10인 State에서 Estimated Value가 20인 State로 이동합니다. Value가 상승했으므로 좋은 Action으로 판단되기 때문에, $w$는 첫 번째 State의 Estimated Value를 높이기 위해 증가합니다. $\gamma$가 1에 가까우면 TD Error는 10에 가까워질 것이고, 만약 $\alpha = 0.1$이면 TD Error를 줄이기 위해 $w$는 11에 가깝게 증가시킬 것입니다. 문제는 두 번째 State가 $2w$로 근사되었기 때문에 동시에 22 정도가 증가한다는 것입니다. 그렇게 되면 TD Error가 오히려 11로 증가합니다. 따라서 첫 번째 State의 Value가 다시 증가하며, 이전보다 더 많은 수치인 12.1 정도가 증가합니다. 이후로는 역시 두 번째 State의 Value도 증가하며, 결국 $w$는 무한대로 발산합니다.&lt;/p&gt;

&lt;p&gt;이 원인을 알아보기 위해 두 State의 Update 순서를 자세히 알아보겠습니다. 먼저 두 State 사이의 Transition에 대한 TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t = R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w}) = 0 + \gamma 2 w_t - w_t = (2 \gamma - 1) w_t\]

&lt;p&gt;그리고 식 (11.2)에 의한 Off-policy Semi-gradient TD(0)의 Update를 사용하면,&lt;/p&gt;

\[w_{t+1} = w_t + \alpha \rho_t \delta_t \nabla \hat{v} (S_t, w_t) = w_t + \alpha \cdot 1 \cdot (2 \gamma - 1) w_t \cdot 1 = \left(1 + \alpha (2 \gamma - 1) \right) w_t\]

&lt;p&gt;이 예제에서 Importance Sampling Ratio $\rho_t$는 첫 번째 State에서 선택할 수 있는 Action이 1개뿐이기 때문에 1로 고정됩니다. (Target Policy와 Behavior Policy가 같을 수밖에 없으므로) 또한 이 Update 식에서 주목할 점은 $w_{t+1}$가 $w_t$에 $1 + \alpha (2 \gamma - 1)$을 곱한 값이라는 것입니다. 이 값이 1보다 크면 초기 값 $w$이 무엇인지에 따라 결국 양의 무한대나 음의 무한대로 발산하게 되므로, 시스템이 불안정할 수밖에 없습니다. 이 경우에는 만약 $\gamma$가 0.5보다 큰 경우에 발산합니다. 이것으로부터 알 수 있는 점은 Update의 안정성은 Step-size Parameter $\alpha$와는 무관하다는 것입니다. $\alpha$의 크기에 따라 발산하는 속도에 영향을 끼칠지는 몰라도, 결국 발산하는가 수렴하는가 자체에는 영향이 없습니다.&lt;/p&gt;

&lt;p&gt;이 예제에서 발산이 발생하는 근본적인 이유는 $w$가 다른 State로 Transition되지 않는 이상 같은 Transition이 반복해서 일어나기 때문입니다. 만약 선택할 수 있는 다른 Action이 있다면, Off-policy는 Target Policy가 하지 않을 Action을 Behavior Policy가 선택할 수 있기 때문에 이 문제를 회피할 수 있습니다. 이 경우 $\rho_t$는 0이 되고 Update가 발생하지 않습니다. 하지만 On-policy에서 $\rho_t$는 항상 1입니다. State $w$에서 State $2w$로 Transition이 일어날 때마다 $w$가 증가하면, State $2w$를 벗어다는 Transition 또한 있어야 합니다. 만약 그 Transition이 $2w$보다 높은 State가 아니라면 $w$를 감소시키고, 그 State 뒤에 더 높은 Value의 State가 와야 하며, 그렇지 않으면 다시 $w$가 감소합니다. 각각의 State는 더 높은 기대값을 생성해야만 이전 State가 발산하지 않습니다.&lt;/p&gt;

&lt;p&gt;다만 이 예제는 전체 MDP의 일부만 놓고 가정한 예시입니다. 이제 따져봐야 할 것은 예제와 같이 불안정하고 완전한 MDP 시스템이 실제로 존재할 수 있는지에 대한 여부입니다. 결론부터 말하자면 그런 시스템은 존재하며, &lt;span style=&quot;color:red&quot;&gt;Baird’s Counterexample&lt;/span&gt;이라는 이름으로 알려져 있습니다. 아래 그림과 같이 7개의 State와 2개의 Action이 있는 Episodic MDP가 있습니다. 점선으로 표시된 동작은 동일한 확률로 6개의 State 중 하나로 Action할 수 있다는 뜻이고, 실선으로 표시된 동작은 모두 맨 아래의 7번째 State로 이동합니다. Behavior Policy $b$는 확률 $\frac{6}{7}$로 점선 Action, 확률 $\frac{1}{7}$로 실선의 Action을 선택합니다. 이 확률 분포는 각 Episode의 시작 분포이기도 합니다. Target Policy $\pi$는 항상 실선 Action만 선택하여 7번째 State로만 이동하려고 합니다. Reward는 모든 Transition에서 0입니다. Discount Factor는 $\gamma = 0.99$로 정의됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각각의 State에 나타난 식은 Linear Parameter를 사용하여 State-Value를 추정한 것입니다. $w$ 아래에 있는 첨자는 Weight Vector $\mathbf{w} \in \mathbb{R}^8$의 몇 번째 요소인지 표시한 것입니다. 그러므로 맨 왼쪽의 State를 Feature Vector로 표현하면 $\mathbf{x} = (2,0,0,0,0,0,0,1)^{\sf T}$입니다. Reward는 모든 Transition에서 0으로 정의되어 있으므로 실제 Value Function은 모든 State $s$에 대해 $v_{\pi} = 0$입니다.  따라서 Weight Vector가 $\mathbf{w} = \mathbf{0}$에 수렴한다면 정확하게 근사가 가능합니다. (다만 이전 예제와 달리 구성 요소가 많기 때문에 이 외에도 더 많은 해법이 존재합니다) 또한 Feature Vector의 집합 $\{ \mathbf{x}(s) : s \in \mathcal{S} \}$는 Linear Independent 집합입니다. 이렇게 보았을 때 이 문제는 Linear Function Approximation에 적합한 것처럼 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 이 문제에 식 (11.2)와 같은 Semi-gradient TD(0)를 적용하면 위의 그림의 왼쪽과 같이 Weight가 무한대로 발산합니다. 심지어 그림의 오른쪽처럼 Expected Update를 이용한 Dynamic Programming을 사용해도 마찬가지로 발산합니다. 이 때 Weight Vector $\mathbf{w}$가 DP Target을 사용하여 Semi-gradient 방법으로 동시에 Update 되는 방식은 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{k+1} \doteq \mathbf{w}_k + \frac{\alpha}{|\mathcal{S}|} \sum_s \bigg( \mathbb{E}_{\pi} [ R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}_k) | S_t = s ] - \hat{v}(s, \mathbf{w}_k) \bigg) \nabla (s, \mathbf{w}_k) \tag{11.9}\]

&lt;p&gt;이 방법은 기존 DP Update와 마찬가지로 &lt;strong&gt;Randomness&lt;/strong&gt;와 &lt;strong&gt;Asynchrony&lt;/strong&gt;가 없습니다. 그럼에도 불구하고 Semi-gradient Function Approximation을 적용했을 때 시스템이 불안정합니다. 만약 Baird’s counterexample에서 DP Update의 Distribution를 Uniform Distribution에서 On-policy Distribution으로 변경하면 수렴이 식 (9.14)와 같이 Bound가 있는 해법으로 보장됩니다.&lt;/p&gt;

&lt;p&gt;이 반례가 신기한 점은 사용하는 TD나 DP가 가장 간단한 Bootstrapping 방법이고, 사용하는 Semi-gradient 방법이 가장 간단한 Function Approximation임에도 불구하고 발산한다는 점입니다. 이 예시를 통해 알 수 있는 점은 On-policy Distribution에 따라 Update가 수행되지 않으면 가장 간단한 Bootstrapping과 Function Approximation의 조합이라고 할지라도 불안정함을 나타냅니다.&lt;/p&gt;

&lt;p&gt;만약 Baird’s counterexample과 달리 Expected Return을 사용하여 Value Function을 Least-Square로 바꾼다고 가정하겠습니다. 이 방법은 Feature Vector의 집합 $\{ \mathbf{x}(s) : s \in \mathcal{S} \}$이 Linear Independent일 때 각 Iteration에서 정확한 근사가 가능하고 표준 Tabular DP로 방법이 축소되기 때문에 불안정성 문제를 해결할 수 있을 것이라고 기대할 수 있습니다. 하지만 불행하게도, 다음 예제에 나오듯이 이 방법으로도 안정성이 보장되지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 11.1) Tsitsiklis and Van Roy’s Counterexample&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예제는 각각의 단계에서 Least-Squares 해법을 찾은 경우에도 Linear Function Approximation이 DP에서 제대로 동작하지 않는 것을 보여줍니다. 위의 그림에서 나온 것처럼 $w$와 $2w$로 근사된 State가 있고, $2w$ State에서 $1-\epsilon$ 확률로 자기 자신으로 돌아오는 Action과 $\epsilon$ 확률로 마지막 State로 이동하는 Action이 있습니다. 모든 Transition에서 Reward는 0으로 정의되어 있기 때문에 Real Value는 $w = 0$일 경우 정확하게 추정이 가능합니다. 각 단계에서 1-step Return과 Estimated Value에 대한 $\overline{\text{VE}}$를 최소화하는 것으로 $w_{k+1}$를 정의하면 다음과 같이 전개할 수 있습니다.&lt;/p&gt;

\[\begin{align}
w_{k+1} &amp;amp;= \underset{w \in \mathbb{R}}{\operatorname{argmin}} \sum_{s \in \mathcal{S}} \bigg( \hat{v}(s, w) - \mathbb{E}_{\pi} [ R_{t+1} + \gamma \hat{v} (S_{t+1}, w_k) | S_t = s ] \bigg)^2 \\ \\
&amp;amp;= \underset{w \in \mathbb{R}}{\operatorname{argmin}} (w - \gamma 2 w_k)^2 + \Big( 2w - (1 - \epsilon) \gamma 2 w_k \Big)^2 \\ \\
&amp;amp;= \frac{6 - 4 \epsilon}{5} \gamma w_k \tag{11.10}
\end{align}\]

&lt;p&gt;즉, Sequence $\{ w_k \}$은 $\gamma &amp;gt; \frac{5}{6 - 4 \epsilon}$이고 $w_0 \ne 0$일 때 발산합니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;불안정성을 해결하기 위한 또 다른 방법은 특별한 방법을 사용해 함수를 근사하는 것입니다. 특히 관찰되는 Target에서 Extrapolate 하지 않는 Function Approximation의 경우 안정성이 보장됩니다. 이 방법은 &lt;span style=&quot;color:red&quot;&gt;Averagers&lt;/span&gt;라고 부르며, Nearest Neighbor나 Locally Weighted Regression이 포함되지만, 아쉽게도 이전에 다룬 Tile Coding이나 Artificial Neural Network 같은 방법은 포함되지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;the-deadly-triad&quot;&gt;The Deadly Triad&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 어떤 조건 때문에 지난 Section의 예제처럼 발산하는지에 대해 논의해보겠습니다. 이번 Section에서는 불안정성과 발산의 위험을 증가시키는 3가지 위험 요소인 &lt;span style=&quot;color:red&quot;&gt;The Deadly Triad&lt;/span&gt;를 소개하도록 하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Function Approximation&lt;/strong&gt; : 메모리 및 계산 자원보다 훨씬 큰 State Space에서 일반화하는 방법&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt; : Actual Reward 및 Complete Return 대신 기존 추정치를 포함하는 Update Target&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Off-policy Training&lt;/strong&gt; : Target Policy와 다른 Distribution에 의해 생성된 데이터로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;특이한 점으로, Control이나 Generalized Policy Iteration 자체는 이러한 위험을 초래하지 않습니다. 이 경우 분석하기 더 복잡하지만, 이 3가지 위험 요소 중 해당하는 것이 많을 수록 불안정성과 발산의 가능성이 늘어납니다. 또한 이 불안정성은 학습이나 Environment의 불확실성과도 상관이 없습니다. Environment가 완전히 알려진 Planning과 같은 Dynamic Programming에서도 발생하기 때문입니다.&lt;/p&gt;

&lt;p&gt;이 3가지 위험 요소 중 2가지 요소만 해당하는 경우 불안정성을 피할 수 있습니다. 그렇기 때문에 사용하는 방법에서 이 3가지 중 어떤 것을 포기할 수 있는지 하나하나 따져봐야합니다.&lt;/p&gt;

&lt;p&gt;가장 먼저 포기할 수 없는 요소는 &lt;strong&gt;Function Approximation&lt;/strong&gt;입니다. Function Approximation 없이는 매우 큰 State Space를 학습하기 어렵기 때문입니다. 대안으로 생각해볼 수 있는 State Aggregation이나 Nonparametric 방법은 데이터의 수가 많아질수록 시간 및 공간 복잡도가 너무 커집니다. LSTD와 같은 방법 또한 큰 문제에서 시간 복잡도로 인해 사용하기 어렵습니다.&lt;/p&gt;

&lt;p&gt;두 번째로 &lt;strong&gt;Bootstrapping&lt;/strong&gt;을 포기한다면 그 대신 계산 및 데이터 효율성을 포기해야 합니다. 둘 중에서는 계산 효율성의 손실이 더 치명적입니다. Monte Carlo와 같이 Bootstrapping을 사용하지 않는 방법은 예측을 수행하고 최종 Return을 얻는 사이에 발생하는 모든 것을 저장할 메모리가 필요하며, 모든 계산은 최종 Return을 얻은 후에야 완료됩니다. 이런 계산 문제의 비용은 주로 특수한 하드웨어에서 발생할 수 있습니다. 물론 데이터 효율성의 관점에서도 Bootstrapping을 사용하지 않았을 때의 손실이 적지 않습니다. 또한 일반적으로 Bootstrapping을 사용했을 때 학습 속도가 더 빠른 결과를 보여줍니다. 그렇기 때문에 필수는 아니지만, 가급적이면 Bootstrapping을 사용하는 것이 좋습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;strong&gt;Off-policy Training&lt;/strong&gt;이 있습니다. On-policy이 효율적인 경우가 많기도 했고, Model이 없는 강화학습의 경우 Q-learning 대신 Sarsa를 사용하면 됩니다. Off-policy의 장점은 Target Policy에 상관 없는 Action을 할 수 있다는 점입니다. 편리하긴 하지만, 필수는 아닙니다. 하지만 Off-policy는 이 책에서 언급하지 않은 강력한 지능형 Agent를 만드는데 필수적입니다.&lt;/p&gt;

&lt;h2 id=&quot;linear-value-function-geometry&quot;&gt;Linear Value-function Geometry&lt;/h2&gt;

&lt;p&gt;Off-policy 학습의 안정성 문제를 더 잘 이해하기 위해서는 Value Function Approximation에 대해 더 자세히 알아보는 것이 좋습니다. 먼저 모든 State-Value Function은 State에서 실수 집합으로 매핑되는 함수입니다. ($v : \mathcal{S} \to \mathbb{R}$) 대부분의 Value Function은 어떤 Policy에도 해당하지 않는데, 중요한 점은 대부분의 Function Approximation 방법은 State보다 매개변수가 훨씬 적기 때문에 Policy로 표현될 수 없다는 것입니다.&lt;/p&gt;

&lt;p&gt;State Space $\mathcal{S} = \{ s_1, s_2, \ldots, s_{\lvert \mathcal{S} \rvert} \}$에 대해 임의의 Value Function $v$를 가정합니다. 각 State에 대한 Value를 Vector로 표현하면 $\big[ v(s_1), v(s_2), \ldots, v(s_{\mathcal{S}}) \big]^{\sf T}$와 같이 State 수만큼 구성 요소가 있습니다. Function Approximation을 사용하려는 경우 Vector를 명시적으로 나타내기에는 구성 요소가 너무 많습니다만, 개념적으로 이러한 아이디어는 유용합니다.&lt;/p&gt;

&lt;p&gt;간단하게 3개의 State $\mathcal{S} = \{ s_1, s_2, s_3 \}$와 2개의 매개변수 $\mathbf{w} = (w_1, w_2)^{\sf T}$를 예시로 들어보겠습니다. 이 예시에서 모든 Value Function/Vector는 3차원 공간의 점으로 볼 수 있습니다. 매개변수는 2차원 부분공간에 대한 대체 좌표계를 제공합니다. 또한 모든 Weight Vector $\mathbf{w} = (w_1, w_2)^{\sf T}$는 2차원 부분공간의 한 점이므로 3가지 State 전부에게 값을 할당하는 완전한 Value Function $v_{\mathbf{w}}$이기도 합니다. 일반적인 Function Approximation을 사용하면 전체 공간과 표현 가능한 함수 사이의 관계가 복잡할 수 있으나, 이 경우에는 Linear Value Function Approximation을 사용하면 부분 공간은 아래 그림과 같은 간단한 평면으로 나옵니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예시에서 고정된 단일 Policy $\pi$에 대하여, Real Value Function $v_{\pi}$가 너무 복잡해 근사값으로 정확하게 표현되지 않는다고 가정해보겠습니다. 이것은 $v_{\pi}$가 부분공간에 있지 않다는 의미입니다. 따라서 $v_{\pi}$는 위의 그림과 같이 표현 가능한 평면 (부분공간) 밖에 있는 것으로 나타낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;$v_{\pi}$를 정확하게 표현할 수 없기 때문에 표현 가능한 Value Function 중에서 가장 가까운 것을 찾아야합니다. 이 문제를 해결하기 위해서는 먼저 가장 가깝다는 의미를 파악해야 합니다. 즉, 두 Value Function 사이의 거리 측정이 필요합니다. 두 개의 Value Function $v_1$과 $v_2$가 주어졌을 때, 두 Value Function 사이의 차이를 $v = v_1 - v_2$로 표현하겠습니다. $v$가 작다는 것은 두 Value Function이 서로 가깝다는 의미입니다. 하지만 이 Vector의 크기를 측정하는 것이 문제입니다. Section 9.2에서 논의한 바와 같이 일부 State가 더 자주 발생하거나, 집중해야할 필요가 있기 때문에 Euclidean Norm은 적절하지 않습니다. 따라서 Section 9.2에서 배운 Distribution $\mu : \mathcal{S} \to [0, 1]$을 사용합니다. 이 Distribution은 서로 다른 State를 정확하게 평가하는데 어느 정도 관심이 있는지 그 정도를 표현하는 척도입니다. 그 후, 다음과 같은 Norm을 사용하여 Value Function 사이의 거리를 정의합니다.&lt;/p&gt;

\[||v||_{\mu}^2 \doteq \sum_{s \in \mathcal{S}} \mu(s) v(s)^2 \tag{11.11}\]

&lt;p&gt;Section 9.2의 $\overline{\text{VE}}$는 이 Norm을 사용하여 $\overline{\text{VE}} (\mathbf{w}) = \lVert v_{\mathbf{w}} - v_{\pi} \rVert_{\mu}^2 $로 간단하게 작성할 수 있습니다. Value Function $v$를 표현 가능한 Value Function 부분 공간에서 가장 가까운 Value Function을 찾는 방법은 &lt;strong&gt;Projection&lt;/strong&gt;입니다. 앞으로 Projection은 연산자 $\Pi$로 표현하겠습니다. Projection 연산자 $\Pi$를 사용하여 Value Function $v$를 표현 가능한 Value Function 부분 공간으로 Projection하는 것은 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[\Pi \doteq v_{\mathbf{w}} \quad \text{where} \quad \mathbf{w} = \underset{\mathbf{w} \in \mathbb{R}^d}{\operatorname{argmin}} || v - v_{\mathbf{w}} ||_{\mu}^2 \tag{11.12}\]

&lt;p&gt;따라서 Real Value Function $v_{\pi}$에 가장 가까운 표현 가능한 Value Function은 위의 그림과 같이 Projection된 Value Function $\Pi v_{\pi}$입니다. 이 해법은 Monte Carlo Method에 의해 점근적으로 구할 수 있지만, 구하는 속도가 종종 매우 느릴 수 있습니다. 다음 단계로 넘어가기 전에, Projection 연산에 대해 더 자세히 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Projection Matrix&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Linear Function Approximation 방법에서 Projection 연산은 마찬가지로 Linear이며, 이것은 다음과 같이 $\lvert \mathcal{S} \rvert \times \lvert \mathcal{S} \rvert$ 행렬로 표현할 수 있습니다.&lt;/p&gt;

\[\Pi \doteq \mathbf{X} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \mathbf{X}^{\sf T} \mathbf{D} \tag{11.13}\]

&lt;p&gt;$\mathbf{D}$는 대각 요소에 $\mu(s)$가 있는 $\lvert \mathcal{S} \rvert \times \lvert \mathcal{S} \rvert$ 크기의 대각 행렬이고, $\mathbf{X}$는 각 행이 Feature Vector $\mathbf{x}(s)^{\sf T}$로 이루어진 $\lvert \mathcal{S} \rvert \times d$ 크기의 행렬입니다. 만약 식 (11.13)의 역행렬이 존재하지 않는다면, &lt;strong&gt;Pseudo Inverse&lt;/strong&gt;로 대체됩니다. Pseudo Inverse에 대해서는 다음 포스트를 참고해주시기 바랍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/ml/linear-models-1/&quot;&gt;[기계학습] 3. Linear Models I&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이를 토대로 Vector의 Square Norm을 표현하면 다음과 같습니다.&lt;/p&gt;

\[||v||_{\mu}^2 = v^{\sf T} \mathbf{D} v \tag{11.14}\]

&lt;p&gt;따라서 Approximate Linear Value Function은 아래와 같이 작성할 수 있습니다.&lt;/p&gt;

\[v_{\mathbf{w}} = \mathbf{X} \mathbf{w} \tag{11.15}\]

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;TD는 이와 다른 해법을 찾습니다. 그 이유를 알아보기 위해서, 먼저 Value Function $v_{\pi}$에 대한 Bellman Equation을 복습해보겠습니다.&lt;/p&gt;

\[v_{\pi} (s) = \sum_a \pi (a|s) \sum_{s&apos;, r} p(s&apos;, r | s, a) [r + \gamma v_{\pi} (s&apos;) ] \quad \text{for all } s \in \mathcal{S} \tag{11.16}\]

&lt;p&gt;Real Value Function $v_{\pi}$는 식 (11.16)을 정확하게 풀 수 있는 유일한 함수입니다. Real Value Function $v_{\pi}$를 Approximate Value Function $v_{\mathbf{w}}$로 대체한 후, 수정된 방정식에서 발생하는 우변과 좌변 사이의 차이를 Bellman Error라고 부릅니다. Bellman Error는 $v_{\mathbf{w}}$가 $v_{\pi}$에서 얼마나 멀리 떨어져 있는지를 측정하는 데 사용되며, State $s$에 대한 Bellman Error는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\bar{\delta}_{\mathbf{w}} (s) &amp;amp; \doteq \bigg( \sum_a \pi (a | s) \sum_{s&apos;, r} p(s&apos;, r|s, a) [r + \gamma v_{\mathbf{w}} (s&apos;)] \bigg) - v_{\mathbf{w}} (s) \tag{11.17} \\ \\
&amp;amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\mathbf{w}} (S_{t+1}) - v_{\mathbf{w}}(S_t) | S_t = s, A_t \sim \pi] \tag{11.18}
\end{align}\]

&lt;p&gt;식 (11.18)을 통해 Bellman Error는 TD Error (=식 11.3)의 기대값임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;모든 Bellman Error의 Vector $\bar{\delta}_{\mathbf{w}} \in \mathbb{R}^{\lvert \mathcal{S} \rvert}$는 &lt;span style=&quot;color:red&quot;&gt;Bellman Error Vector&lt;/span&gt;라고 합니다. 위의 그림에서 Bellman Error Vector는 BE로 표시되어 있습니다. Norm에서 이 Vector의 전체 크기는 Value Function의 전체적인 Error를 측정하며, &lt;span style=&quot;color:red&quot;&gt;Mean Square Bellman Error&lt;/span&gt;라고 부릅니다. 이 식은 다음과 같이 간단하게 표현할 수 있습니다.&lt;/p&gt;

\[\overline{\text{BE}} (\mathbf{w}) = || \bar{\delta}_{\mathbf{w}} ||_{\mu}^2 \tag{11.19}\]

&lt;p&gt;일반적으로 $\overline{\text{BE}}$를 0으로 줄이는 것은 불가능하지만, Linear Function Approximation의 경우 $\overline{\text{BE}}$가 최소화되는 고유한 $\mathbf{w}$가 있습니다. 이것은 위의 그림에서 $\min \overline{\text{BE}}$로 표현되어 있으며, 일반적으로 $\overline{\text{VE}}$를 최소화하는 지점(=$\Pi v_{\pi}$)과는 다릅니다. $\overline{\text{BE}}$를 최소화하는 방법은 다음 두 Section에서 더 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;Bellman Error Vector는 Bellman Operator $B_{\pi} : \mathbb{R}^{\lvert \mathcal{S} \rvert} \to \mathbb{R}^{\lvert \mathcal{S} \rvert}$를 Approximate Value Function에 적용한 결과로, 위의 그림에도 나와 있습니다. Bellman Operator는 State $s \in \mathcal{S}$와 Value Function $v : \mathcal{S} \to \mathbb{R}$에 대해 다음과 같이 정의됩니다.&lt;/p&gt;

\[(B_{\pi} v)(s) \doteq \sum_a \pi (a|s) \sum_{s&apos;, r} p(s&apos;, r | s, a) [r + \gamma v(s&apos;)] \tag{11.20}\]

&lt;p&gt;식 (11.20)을 이용하면 $v_{\mathbf{w}}$에 대한 Bellman Error Vector를 $\bar{\delta}_{\mathbf{w}} = B_{\pi} v_{\mathbf{w}} - v_{\mathbf{w}}$로 작성할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Bellman Operator를 표현 가능한 부분 공간의 Value Function에 적용하게 되면 일반적으로 위의 그림과 같이 부분 공간 외부에 새로운 Value Function을 생성합니다. Function Approximation을 사용하지 않는 Dynamic Programming에서 Bellman Operator는 그림 상단에 있는 회색 화살표와 같이 표현 가능한 공간 외부의 점에 반복적으로 적용됩니다. 결국 이 과정은 Real Value Function $v_{\pi}$에 수렴하게 됩니다.&lt;/p&gt;

\[v_{\pi} = B_{\pi} v_{\pi} \tag{11.21}\]

&lt;p&gt;이것은 식 (11.16)에서 $\pi$에 대한 Bellman Equation을 다르게 표현한 것입니다.&lt;/p&gt;

&lt;p&gt;하지만 Function Approximation을 사용하게 되면 부분 공간 외부에 있는 Value Function을 표현할 수 없습니다. 즉, Dynamic Programming처럼 그림 상단의 회색 화살표를 따라갈 수 없습니다. 첫 번째 Update 후에 Value Function이 표현할 수 있는 부분 공간에 Projection되어야 하기 때문입니다. 다음 Update는 부분 공간 내에서 시작되며, Value Function은 다시 Bellman Operator에 의해 부분 공간 외부로 나간 다음, 다시 Projection에 의해 부분 공간으로 매핑됩니다. 이 과정은 근사를 사용한 Dynamic Programming과 유사합니다.&lt;/p&gt;

&lt;p&gt;이 때 Bellman Error Vector를 표현 가능한 부분 공간으로 Projection한 $\Pi \bar{\delta}_{\mathbf{w}}$는 그림에서 PBE로 나타나 있습니다. Norm에서 이 Vector의 크기는 Approximate Value Function의 또 다른 Error 측정값입니다. 임의의 Approximate Value Function $v_{\mathbf{w}}$에 대해, Mean Square Projected Bellman Error는 $\overline{\text{PBE}}$라고 하며, 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\overline{\text{PBE}} (\mathbf{w}) = || \Pi \bar{\delta}_{\mathbf{w}}  ||_{\mu}^2 \tag{11.22}\]

&lt;p&gt;Linear Function Approximation을 사용하면 $\overline{\text{PBE}}$가 0인 Approximate Value Function이 (부분 공간 안에) 반드시 존재합니다. 이것은 Section 9.4에서 소개한 TD Fixed Point $\mathbf{w}_{\text{TD}}$와 같습니다. 이 점은 Semi-gradient TD 방법과 Off-policy 방법에서 안정적이지 않다고 배웠습니다. 게다가 그림에서 볼 수 있듯이 이 Value Function은 일반적으로 $\overline{\text{VE}}$나 $\overline{\text{BE}}$를 최소화하는 함수와 다릅니다. 이것이 수렴하는 것을 보장하는 방법은 Section 11.7과 11.8에서 더 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-in-the-bellman-error&quot;&gt;Gradient Descent in the Bellman Error&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 지난 Section에서 배운 Bellman Error를 토대로 Off-policy 학습의 안정성 문제에 대해 다시 다루겠습니다. 이번 장에서 궁극적으로 하고싶은 것은 Off-policy 기반의 Stochastic Gradient Descent (SGD) 입니다. SGD의 Update는 목적 함수의 음의 기울기와 같습니다. 이 방법은 항상 목표에 대해 내리막길 방향으로 이동하기 때문에 안정적인 수렴을 기대할 수 있습니다.&lt;/p&gt;

&lt;p&gt;SGD 방법은 On-policy와 Off-policy 학습 뿐만 아니라 일반 non-Linear (미분가능한) Function Approximation 방법에 대해서도 수렴하지만, Bootstrapping이 있는 Semi-gradient 방법보다 느린 경향이 있습니다. 대신 이번 장 앞부분에서 다루었듯이 Semi-gradient 방법은 Off-policy 학습과 non-Linear Function Approximation의 경우 발산할 수 있다는 문제가 있습니다. 하지만 SGD 방법은 그런 발산이 일어나지 않습니다.&lt;/p&gt;

&lt;p&gt;SGD는 발산하지 않는다는 매우 강력한 장점을 가졌기 때문에 많은 연구자들이 강화학습에서 SGD를 활용하고자 연구하였습니다. 강화학습에 SGD를 활용하는 첫 번째 단계는 최적화할 Error나 목적 함수를 선택하는 것입니다. 따라서 이전 Section에서 소개한 Bellman Error를 기반으로 이번 Section과 다음 Section에 걸쳐 가장 많이 쓰이는 목적 함수를 소개하고 그 한계를 알아볼 것입니다. 결론부터 말하자면 이 방법들은 좋은 접근 방식이기는 하나, 좋은 학습 알고리즘은 생성하지 못합니다. 어째서 이러한 결론이 나오지는지는 차근차근히 일아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;가장 먼저 Bellman Error 대신 TD Error를 먼저 다시 살펴보겠습니다. Discount가 포함된 1-step TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t = R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w})\]

&lt;p&gt;이 때 가능한 목적 함수는 다음과 같은 Mean Square TD Error로 볼 수 있습니다.&lt;/p&gt;

\[\begin{align}
\overline{\text{TDE}} (\mathbf{w}) &amp;amp;= \sum_{s \in \mathcal{S}} \mu(s) \mathbb{E} \left[ \delta_t^2 | S_t = s, A_t \sim \pi \right] \\ \\
&amp;amp;= \sum_{s \in \mathcal{S}} \mu(s) \mathbb{E} \left[ \rho_t \delta_t^2 | S_t = s, A_t \sim b \right] \\ \\
&amp;amp;= \mathbb{E}_b \left[ \rho_t \delta_t^2 \right] \quad ( \text{if } \mu \text{ is the distribution encountered under } b )
\end{align}\]

&lt;p&gt;위의 마지막 방정식이 바로 SGD에 필요한 형태입니다. Behavior Policy $b$를 토대로 Sampling할 수 있는 기대값이 목적이 됩니다. 따라서 표준 SGD 접근 방식에 따라 이 기대값의 Sample을 토대로 단계별 Update를 유도할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha \nabla (\rho_t \delta_t^2) \\ \\
&amp;amp;= \mathbf{w}_t - \alpha \rho_t \delta_t \nabla \delta_t \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \rho_t \delta_t \big( \nabla \hat{v} (S_t, \mathbf{w}_t) - \gamma \nabla \hat{v} (S_{t+1}, \mathbf{w}) \big) \tag{11.23}
\end{align}\]

&lt;p&gt;재밌는 점은 식 (11.23)에서 Discount Factor $\gamma$가 붙은 항만 제외하면 식 (11.2)의 Semi-gradient TD 알고리즘과 동일하다는 것입니다. 이 추가적인 항은 진정한 SGD 알고리즘으로 만드는 역할을 합니다. Update에 식 (11.23)을 사용하는 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;Naive Residual-gradient Algorithm&lt;/span&gt;이라고 부릅니다. (Baird, 1995) Naive Residual-gradient Algorithm은 강력하게 수렴하지만, 반드시 원하는 위치로 수렴하지는 않습니다. 다음 예제를 통해 이게 무슨 의미인지 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 11.2) A-split example, showing the naiveté of the naive residual-gradient algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래 그림과 같이 3개의 State로 이루어진 Episodic MRP를 가정해보겠습니다. Episode는 항상 State A에서 시작한 다음, 확률적으로 Split됩니다. 50% 확률로 B로 간 다음 1의 Reward를 받고 종료되는 분기와, 50% 확률로 C로 간 다음 0의 Reward를 받고 종료되는 분기가 있습니다. A에서 B를 가는 것과 C를 가는 것 자체는 모두 0의 Reward를 받게 설계되어 있습니다. 이것은 Episodic Task이기 때문에 $\gamma$를 1로 간주할 수 있습니다. 또한 $\rho_t$가 항상 1이 될 수 있게 On-policy라고 가정하고, Tabular Function Approximation을 사용하도록 하겠습니다. 문제 설정만 보면 굉장히 간단한 것처럼 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제에서 State A의 Value를 구해보면 50% 확률로 1의 Reward를 받고 50% 확률로 0의 Reward를 받기 때문에 A의 Value는 $1/2$이라고 쉽게 계산할 수 있습니다. 마찬가지로 State B의 Value는 1, State C의 Value는 0이 됩니다.&lt;/p&gt;

&lt;p&gt;그런데 만약 이 문제에 Naive Residual-gradient Algorithm을 사용하게 되면 다른 결과가 나옵니다. State B의 Value는 $3/4$로, State C의 Value는 $1/4$로 수렴합니다. 다행히 State A의 Value는 똑같이 $1/2$로 수렴합니다. 이 추정한 Value들은 실제로 $\overline{\text{TDE}}$를 최소화하는 값입니다.&lt;/p&gt;

&lt;p&gt;이 Value를 사용하여 $\overline{\text{TDE}}$를 계산해봅시다. 각 Episode의 첫 번째 Transition은 A의 Value $1/2$에서 B의 Value $3/4$로 변해 그 차이가 $1/4$이 되거나 C의 Value $1/4$로 변해 그 차이가 $-1/4$이 됩니다. 이 Transition 자체의 보상은 0이고 $\gamma = 1$ 이므로 첫 번째 Transition에서 TD Error의 제곱은 항상 $1/16$ 입니다. 두 번째 Transition 또한 B의 경우 $3/4 \to 1$, C의 경우 $1/4 \to 0$이므로 TD Error의 제곱은 마찬가지로 $1/16$ 입니다. 따라서 두 단계 모두 $\overline{\text{TDE}}$는 $1/16$ 임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이제 Real Value를 사용하여 $\overline{\text{TDE}}$를 계산해보겠습니다. 이 경우에는 첫 번째 Transition에서 B로 갈 경우 $1/2 \to 1$, C로 갈 경우 $1/2 \to 0$ 입니다. 두 경우 모두 TD Error의 제곱은 $1/4$ 입니다. 두 번째 Transition은 State의 Value와 Reward가 동일하기 때문에 둘 다 TD Error가 0입니다. 따라서 TD Error의 제곱은 첫 번째 Transition에서 $1/4$, 두 번째 Transition에서 0이 됩니다. 따라서 평균적으로 TD Error의 제곱은 $1/8$이므로 방금 전에 계산한 $1/16$보다 크기 때문에 $\overline{\text{TDE}}$ 관점에서 더 나쁜 해법이라고 볼 수 있습니다. 이 간단한 문제에서도 Real Value는 가장 작은 $\overline{\text{TDE}}$를 갖지 않는다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;A-split 예제에서는 Tabular 방법을 사용했기 때문에 State의 Real Value를 정확하게 표현할 수 있었지만, Naive Residual-gradient Algorithm은 State의 Value를 Real Value와 다르게 추정했고, 이 Value를 사용했을 때 Real Value보다 $\overline{\text{TDE}}$가 더 낮았습니다. 이것이 바로 지금까지 Mean Square TD Error를 최소화하는 것을 목적으로 하지 않았던 이유입니다.&lt;/p&gt;

&lt;p&gt;이전 장에서 배운 Mean Square Bellman Error $\overline{\text{BE}}$는 Real Value를 학습하면 모든 지점에서 Error가 0이었습니다. 따라서 Bellman Error를 최소화하는 알고리즘은 A-split 예제에서 문제가 없어야 합니다. 일반적으로 Bellman Error가 0이 될 것이라고 기대할 수는 없지만, Real Value Function을 찾는데 관련이 있기 때문입니다. State에 대한 Bellman Error는 해당 State에서의 평균 TD Error이기 때문에 Update 식을 유도해보겠습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha \nabla (\mathbb{E}_{\pi} [\delta_t]^2) \\ \\
&amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha \nabla (\mathbb{E}_{\pi} [\rho_t \delta_t]^2) \\ \\
&amp;amp;= \mathbf{w}_t - \alpha \mathbb{E}_b \left[ \rho_t \delta_t \right] \nabla \mathbb{E}_b \left[ \rho_t \delta_t \right] \\ \\
&amp;amp;= \mathbf{w}_t - \alpha \mathbb{E}_b \left[ \rho_t ( R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w})) \right] \mathbb{E}_b \left[ \rho_t \nabla \delta_t \right] \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \bigg[ \mathbb{E}_b \left[ \rho_t (R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w})) \right] - \hat{v}(S_t, \mathbf{w}) \bigg] \bigg[ \nabla \hat{v} (S_t, \mathbf{w}) - \gamma \mathbb{E}_b \left[ \rho_t \nabla \hat{v} (S_{t+1}, \mathbf{w}) \right] \bigg]
\end{align}\]

&lt;p&gt;이 Update와 이것을 Sampling하는 방법들을 &lt;span style=&quot;color:red&quot;&gt;Residual-gradient Algorithm&lt;/span&gt;이라고 합니다. 모든 기대값에서 단순한 Sample을 사용한 경우 위의 식은 식 (11.23)의 Naive Residual-gradient Algorithm과 거의 정확하게 감소합니다. 하지만 이 식이 식 (11.23)과 다른 점은 다음 State $S_{t+1}$과 함께 곱해지는 두 개의 평균값입니다. Bias되지 않는 Sample을 얻기 위해서는 다음 State에서 두 개의 독립적인 Sample이 필요하지만, Environment와의 상호작용에서 얻는 Sample은 한 개 뿐입니다. 여기서 구현의 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;Residual-gradient Algorithm을 구현하기 위한 두 가지 방법이 있습니다. 하나는 Environment가 Deterministic으로 주어진 경우입니다. Deterministic Environment라면 다음 State로 Transition하는 두 개의 Sample은 같을 수밖에 없기 때문입니다. 또 다른 방법은 $S_t$에서 다음 State인 $S_{t+1}$의 두 개의 독립된 Sample을 얻는 것입니다. 하나는 위의 식에서 첫 번째 평균에 대한 것이고, 다른 하나는 두 번째 평균에 대한 것입니다. Environment와 실제로 상호작용할 때 이렇게 2개의 Sample을 얻는 것은 불가능하지만, Simulated Environment라면 가능합니다. 이러한 경우에 Residual-gradient Algorithm은 Step-size Parameter가 일반적인 조건일 때 $\overline{\text{BE}}$의 최소값으로 수렴하는 것이 보장됩니다. 진짜 SGD 방법으로써 이 수렴은 강력하며, Linear 및 non-Linear Function Approximation 방법 모두에 적용이 가능합니다. Linear인 경우라면 수렴은 항상 $\overline{\text{BE}}$를 최소화하는 고유한 $\mathbf{w}$를 생성합니다.&lt;/p&gt;

&lt;p&gt;하지만 그럼에도 불구하고 Residual-gradient Algorithm의 문제점은 적어도 3가지가 남아 있습니다.&lt;/p&gt;

&lt;p&gt;첫 번째 문제는 Semi-gradient 방법보다 훨씬 느리다는 것입니다. 이를 해결하기 위한 아이디어로 초기에 Semi-gradient 방법을 사용하고, 나중에 Residual-gradient Algorithm으로 전환하여 수렴을 보장시키는 방법이 제안되었습니다. (Baird and Moore, 1999)&lt;/p&gt;

&lt;p&gt;두 번째 문제는 Naive Residual-gradient Algorithm처럼 몇몇 경우에 잘못된 값으로 수렴한다는 것입니다. 이 문제의 예시는 다음과 같이 A-split 문제의 변형에서 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 11.3) A-presplit example, a counterexample for the $\overline{\text{BE}}$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래 그림과 같은 3개의 State A, B, C로 이루어진 Episodic MRP가 있습니다. Episode는 동일한 확률로 A1 또는 A2에서 시작합니다. 그러나 State A1, A2는 Function Approximation을 수행할 때 A라는 동일한 State로 간주됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Function Approximation 방법의 매개변수는 3가지 구성요소를 갖고 있습니다. 하나는 State B의 Value, 하나는 State C의 Value, 나머지 하나는 State A (=A1과 A2)의 Value를 표현합니다. 초기 State를 제외하고 시스템은 Deterministic입니다. Episode가 A1에서 시작하면 0의 Reward를 받고 B로 Transition되고, 1의 Reward를 받은 다음 Episode가 종료됩니다. 마찬가지로 A2에서 시작하는 경우에는 0의 Reward를 받고 C로 Transition되고, 0의 Reward를 받고 Episode가 종료됩니다.&lt;/p&gt;

&lt;p&gt;외부적으로 보았을 때 이 예제는 A-split 예제와 동일한 것처럼 보입니다. A-split 예제와 마찬가지로 B와 C의 Real Value는 각각 1과 0이고, A는 $1/2$입니다. A-split 예제에서, Semi-gradient TD는 이 Real Value로 수렴하지만, Naive Residual-gradient Algorithm은 B와 C의 Value는 각각 $3/4$와 $1/4$로 수렴합니다. 모든 State Transition은 Deterministic이기 때문에 Residual-gradient Algorithm 또한 이 값으로 수렴합니다.&lt;/p&gt;

&lt;p&gt;따라서 Naive Residual-gradient Algorithm도 $\overline{\text{BE}}$를 최소화하는 해법이 됩니다. Deterministic 문제에서 Bellman Error와 TD Error는 동일하므로, $\overline{\text{BE}}$가 $\overline{\text{TDE}}$와 동일한 것입니다. 이 예제에서 $\overline{\text{BE}}$를 최적화하면 A-split 예제와 마찬가지로 잘못된 Value로 수렴함을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;세 번째 문제는 바로 다음 Section에서 설명합니다. 간단하게 설명하자면, $\overline{\text{BE}}$를 최소화하는 알고리즘에 문제가 있는 것이 아니라, $\overline{\text{BE}}$를 목표로 하는 것 자체에 문제가 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;the-bellman-error-is-not-learnable&quot;&gt;The Bellman Error is Not Learnable&lt;/h2&gt;

&lt;p&gt;이번 Section의 제목에 Learnable이 있습니다만, 이것은 일반적으로 기계학습에서 사용하는 Learnable과는 다른 개념입니다. 일반적인 Learnable 용어에 대해서는 다음 포스트를 참고해주시기 바랍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/ml/is-learning-feasible/&quot;&gt;[기계학습] 2. Is Learning Feasible?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 Section에서 Learnable은 &lt;strong&gt;무한한 양의 Sample을 갖고 있다고 해도 학습이 불가능하다는 의미&lt;/strong&gt;로 사용하겠습니다. 지난 Section에서 다루었던 $\overline{\text{BE}}$는 이러한 의미에서 학습할 수 없다는 것을 밝힐 것입니다.&lt;/p&gt;

&lt;p&gt;Learnable의 개념을 명확하게 하기 위해 몇 가지 간단한 예부터 시작하겠습니다. 다음과 같은 두 개의 Markov Reward Process (MRP)를 가정해봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 두 MRP에서 Edge는 모두 동일한 확률로 발생하는 것으로 가정하고, Edge에 있는 숫자는 그 Transition이 일어났을 때 받게 되는 Reward를 의미합니다. 모든 State는 단일 성분 Feature Vector $x = 1$과 $w$로 포함됩니다. 따라서 MRP에서 유일하게 변화하는 부분은 Reward의 순서입니다. 왼쪽 MRP는 계속 같은 State에 있으면서 각각 0.5의 확률로 0이나 2의 Reward를 받습니다. 오른쪽 MRP는 같은 확률로 다른 State로 움직이거나 같은 State에 있는데, Reward는 한 State에서는 무조건 0이고 다른 State에서는 무조건 2입니다. 두 MRP가 다르게 설계되어 있지만, Reward를 관찰하게 되면 두 MRP 모두 0, 2, 2, 0, 2, 0, 0, 0, … 과 같은 Sequence로 나오게 됩니다. 따라서 무한한 양의 데이터가 주어진다고 하더라도 이 Sequence이 주어졌을 때 두 MRP중 어느 곳에서 생성된 MRP인지 알 수 없습니다. 심지어, 데이터로부터 MRP의 State가 1개인지 2개인지, Stochastic인지 Deterministic인지조차 알 수 없습니다. 이런 것을 &lt;span style=&quot;color:red&quot;&gt;Not Learnable&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;이 예시는 식 (9.1)과 같은 $\overline{\text{VE}}$ 또한 학습할 수 없습니다. $\gamma = 0$이면 예시의 MRP에서 State의 Real Value는 왼쪽부터 1, 0, 2입니다. 만약 $w = 1$이라고 가정하면, $\overline{\text{VE}}$는 왼쪽 MRP에서 0이고, 오른쪽 MRP에서 0입니다. 두 문제에서 $\overline{\text{VE}}$는 다르지만 생성된 데이터의 Distribution이 동일하기 때문에 $\overline{\text{VE}}$를 학습할 수 없습니다.&lt;/p&gt;

&lt;p&gt;목표를 학습할 수 없기 때문에 $\overline{\text{VE}}$를 왜 사용하는건가라는 의문이 듭니다. 그런데 만약 $w = 1$이라는 해법을 구한다면 그 해법은 예시의 두 MRP에 대해 최적입니다. 즉, $\overline{\text{VE}}$는 학습할 수는 없지만 이를 최적화하는 매개변수는 학습이 가능합니다. 따라서 $\overline{\text{VE}}$는 여전히 사용 가능한 목표입니다.&lt;/p&gt;

&lt;p&gt;이것을 설명하기 위해 또 다른 목적 함수를 하나 정의하도록 하겠습니다. 항상 관찰할 수 있는 한 가지 Error는 각 시점에서의 Estimated Value와 그 시점의 Return 사이의 차이입니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Mean Square Return Error $\overline{\text{RE}}$&lt;/span&gt;로 정의하며, On-policy의 경우에는 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[\begin{align}
\overline{\text{RE}} (\mathbf{w}) &amp;amp;= \mathbb{E} \Big[ \big( G_t - \hat{v} (S_t, \mathbf{w}) \big)^2 \Big] \\ \\
&amp;amp;= \overline{\text{VE}} (\mathbf{w}) + \mathbb{E} \Big[ \big( G_t - v_{\pi} (S_t) \big)^2 \Big] \tag{11.24}
\end{align}\]

&lt;p&gt;식 (11.24)를 보면 $\overline{\text{RE}}$와 $\overline{\text{VE}}$는 매개변수 Vector $\mathbf{w}$에 의존하지 않는 항을 제외하고는 동일합니다. 따라서 두 목표는 동일한 최적의 매개변수 $\mathbf{w}$가 필요합니다. 이 둘의 관계는 아래 그림을 보시면 더 이해가 쉬울 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 다시 $\overline{\text{BE}}$로 돌아가보면, $\overline{\text{RE}}$는 MDP에 대한 지식을 토대로 계산하는 것이 가능하지만, 데이터에서 학습할 수 없다는 점에서 $\overline{\text{VE}}$와 유사합니다. 하지만 $\overline{\text{BE}}$가 $\overline{\text{VE}}$와 다른 점은 최적화하는 해법을 학습할 수 없다는 점입니다. 바로 아래의 반례를 통해 그것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 11.4) Counterexample to the learnability of the Bellman error&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이번에는 앞에서 다루었던 예시보다 약간 더 복잡한 MRP가 필요합니다. 반례에 사용할 두 개의 MRP는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 왼쪽 MRP에는 서로 구분되는 두 개의 State가 있습니다. 하지만 오른쪽 MRP는 세 가지 State가 있는데, 그 중 B와 B’은 동일한 State로 판단되기 때문에 동일한 Value로 추정해야 합니다. 구체적으로 매개변수 Vector $\mathbf{w}$는 2개의 성분이 있으며, State A의 Value는 첫 번째 성분에 의해 표현되고, B와 B’의 Value는 두 번째 성분에 의해 표현됩니다. 두 번째 MRP는 세 State 모두 동일한 시간이 소요되도록 설계되었으므로 모든 State에 대해 $\mu(s) = 1/3$임을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;역시 이전 예제와 마찬가지로 관찰 가능한 데이터의 분포는 두 MRP가 동일합니다. 두 MRP 모두 State A 다음에는 0의 Reward를 얻으며 State B로 이동하는 것을 알 수 있습니다. State B는 State A로 Transition될 때 1의 Reward를 받고, 그 이외에는 모두 -1의 Reward를 받습니다. 두 MRP는 통계적으로 다른 정보도 동일합니다. 예를 들어, State B가 $k$번 나타날 확률은 둘 다 $2^{-k}$입니다.&lt;/p&gt;

&lt;p&gt;만약 $\mathbf{w} = \mathbf{0}$이라고 가정해 보겠습니다. 첫 번째 MRP에서 이것은 정확한 해법이고 $\overline{\text{BE}}$는 0입니다. 두 번째 MRP에서 이 해법은 $\overline{\text{BE}} = \mu(B) 1 + \mu(B’)1 = 2/3$이 되도록 B와 B’ 모두 Error의 제곱을 1로 생성합니다. 동일한 데이터 분포를 생성하는 두 MRP에서 $\overline{\text{BE}}$가 다르므로, $\overline{\text{BE}}$는 학습할 수 없습니다.&lt;/p&gt;

&lt;p&gt;게다가 $\mathbf{w}$를 최소화하는 값은 두 MRP가 다르게 나타납니다. 첫 번째 MRP의 경우 $\mathbf{w} = \mathbf{0}$은 $\gamma$에 대해 $\overline{\text{BE}}$를 최소화합니다. 그런데 두 번째 MRP에서는 $\gamma \to 1$이 될 때 $\mathbf{w} = (-1/2, 0)^{\sf T}$이 됩니다. 따라서 $\overline{\text{BE}}$를 최소화하는 해법은 데이터만으로 추정할 수 없습니다. 데이터에 드러난 것 이상으로 MRP에 대한 추가적인 지식이 필요합니다. 따라서 $\overline{\text{BE}}$를 학습의 목적으로 추구하는 것은 불가능합니다.&lt;/p&gt;

&lt;p&gt;여기서 놀라운 점은 두 번째 MRP에서 State A의 $\overline{\text{BE}}$ 최소화 값이 0과 상당히 차이난다는 것입니다. State A는 Transition할 때 무조건 0의 Reward를 얻고, 거의 0의 Value를 갖는 State로 Transition합니다. 따라서 $v_{\mathbf{w}}(A)$는 0이어야 한다는 뜻입니다. 그런데 그렇게 나오지 않은 이유는 $v_{\mathbf{w}}(A)$를 음수로 만들었을 때 State B에서 A에 도달할 때 Error가 감소하기 때문입니다. 이 Deterministic Transition의 Reward는 1이며, 이것은 State B가 State A보다 더 큰 값을 가져야함을 의미합니다. State B의 Value가 0에 가깝기 때문에 State A의 Value는 -1에 가까워집니다. 따라서 $\overline{\text{BE}}$의 최소화 값은 State A를 $-1/2$로 추정함으로써 State A를 떠날 때와 돌아올 때의 Error의 차이를 줄이는 것입니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;서로 다른 두 개의 MRP는 동일한 데이터를 생성하지만 최소화하는 매개변수 Vector가 서로 다릅니다. 이 때 최적의 매개변수 Vector가 데이터에 대한 함수가 아니기 때문에 데이터에서 학습할 수 없다는 것이 증명됩니다. 지금까지 고려한 다른 Bootstrapping 목표인 $\overline{\text{PBE}}$와 $\overline{\text{TDE}}$는 학습 가능한 데이터에서 결정될 수 있으며, 일반적으로 $\overline{\text{BE}}$를 최소화함으로써 최적의 해법을 구할 수 있습니다. 다시 위의 그림으로 돌아가보면 이것이 요약되어 있습니다.&lt;/p&gt;

&lt;p&gt;어쨌든 $\overline{\text{BE}}$는 학습이 불가능합니다. 즉, Feature Vector나 관찰 가능한 데이터를 토대로 추정할 수 없습니다. 따라서 $\overline{\text{BE}}$는 Model에 기반한 환경으로 제한될 수밖에 없다는 결론을 낼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;gradient-td-methods&quot;&gt;Gradient-TD Methods&lt;/h2&gt;

&lt;p&gt;이제 $\overline{\text{PBE}}$를 최소화하기 위한 SGD 방법을 논의하겠습니다. 진짜 SGD 방법으로써 Gradient TD 방법은 Off-policy 학습 및 non-Linear Function Approximation에서도 강력한 수렴을 보장합니다. 만약 Linear Function Approximation이라면 $\overline{\text{PBE}}$가 0인 TD Fixed-point $\mathbf{w}_{\text{TD}}$가 항상 존재합니다. 이 해법은 Section 9.8에서 다룬 LSTD로 구할 수 있지만, 매개변수의 수 $d$에 대해 $O(d^2)$의 시간 복잡도를 갖습니다. 여기서는 이 방법 대신 계산이 복잡하더라도 $O(d)$의 시간 복잡도를 갖는 방법인 Gradient-TD에 대해 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;먼저 목적인 $\overline{\text{PBE}}$의 식 (11.22)를 행렬식으로 표현해보겠습니다.&lt;/p&gt;

\[\begin{align}
\overline{\text{PBE}} (\mathbf{w}) &amp;amp;= || \Pi \bar{\delta}_{\mathbf{w}} ||_{\mu}^2 \\ \\
&amp;amp;= (\Pi \bar{\delta}_{\mathbf{w}})^{\sf T} \mathbf{D} \Pi \bar{\delta}_{\mathbf{w}} \tag{from (11.14)} \\ \\
&amp;amp;= \bar{\delta}_{\mathbf{w}}^{\sf T} \Pi^{\sf T} \mathbf{D} \Pi \bar{\delta}_{\mathbf{w}}
\end{align}\]

&lt;p&gt;위 식을 식 (11.13)과 항등식 $\Pi^{\sf T} \mathbf{D} \Pi = \mathbf{D} \mathbf{X} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \mathbf{X}^{\sf T} \mathbf{D}$를 이용하면 다음과 같이 수정할 수 있습니다.&lt;/p&gt;

\[\begin{align}
&amp;amp;= \bar{\delta}_{\mathbf{w}}^{\sf T} \mathbf{D} \mathbf{X} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \tag{11.25} \\ \\
&amp;amp;= \big( \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \big)^{\sf T} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \big( \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \big) \tag{11.26}
\end{align}\]

&lt;p&gt;식 (11.26)을 $\mathbf{w}$에 대해 미분하면 Gradient는 다음과 같습니다.&lt;/p&gt;

\[\nabla \overline{\text{PBE}} (\mathbf{w}) = 2 \nabla \left[ \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \right]^{\sf T} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \big( \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \big)\]

&lt;p&gt;이것을 SGD 방법으로 바꾸기 위해서 이 값을 기대값으로 갖는 모든 시간 단계에서 무언가를 Sampling해야 합니다. Behavior Policy에 따라 방문한 State의 Distribution을 $\mu$라고 했을 때, 위의 식에서 3개의 항(여기서는 괄호로 구분된 부분) 모두를 이 Distribution에서 기대값으로 쓸 수 있습니다. 예를 들어서, 마지막 항 $\mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}}$은 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

\[\mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} = \sum_s \mu(s) \mathbf{x} \bar{\delta}_{\mathbf{w}} (s) = \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right]\]

&lt;p&gt;이것은 Semi-gradient TD(0)의 Update 식 (11.2)의 기대값에 불과합니다. $\nabla \overline{\text{PBE}} (\mathbf{w})$의 첫 번째 항은 이 식에 Transpose를 붙이고 미분한 값이므로, 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\nabla \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right]^{\sf T} &amp;amp;= \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \\ \\
&amp;amp;= \mathbb{E} \left[ \rho_t \nabla (R_{t+1} + \gamma \mathbf{w}^{\sf T} \mathbf{w}_{t+1} - \mathbf{w}^{\sf T} \mathbf{x}_t )^{\sf T} \mathbf{x}_t^{\sf T} \right] \\ \\
&amp;amp;= \mathbb{E} \left[ \rho_t ( \gamma \mathbf{x}_{t+1} - \mathbf{x}_t ) \mathbf{x}_t^{\sf T} \right]
\end{align}\]

&lt;p&gt;여기서 $\delta_t$는 Episodic Task라고 가정합니다. 마지막으로, 가운데 항은 다음과 같이 Feature Vector의 Outer Product 기대값으로 간단하게 표현이 가능합니다.&lt;/p&gt;

\[\mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} = \sum_s \mu (s) \mathbf{x} (s) \mathbf{x} (s)^{\sf T} = \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]\]

&lt;p&gt;이렇게 기대값으로 표현한 식을 이용하여 $\overline{\text{PBE}}$의 Gradient 식 $\nabla \overline{\text{PBE}} (\mathbf{w})$을 다시 작성하면 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\nabla \overline{\text{PBE}} (\mathbf{w}) = 2 \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \tag{11.27}\]

&lt;p&gt;식 (11.27)을 보면 첫 항과 마지막 항이 Feature Vector $\mathbf{x}_{t+1}$에 의존하기 때문에, 단순히 Sampling한 데이터의 기대값으로 Residual-gradient Algorithm을 적용하면 편향된 추정값이 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;다른 방법을 생각해보면, 3개의 기대값을 각각 개별적으로 추정한 다음 결합하여 Bias되지 않은 추정치를 생성할 수도 있습니다. 괜찮은 방법이기는 하지만, $d \times d$ 행렬인 처음 두 개 항의 기대값을 계산하고, 두 번째 항의 역행렬을 계산하기 위해서는 많은 계산 자원이 필요합니다. Section 9.8에서와 같이 계산량을 줄이는 방법이 있긴 하지만, 그럼에도 불구하고 여전히 시간 복잡도는 $O(d^2)$입니다.&lt;/p&gt;

&lt;p&gt;또 다른 방법으로는 일부 추정값을 별도로 저장한 다음 결합하는 것을 생각해 볼 수 있습니다. 이번 Section에서 다루는 Gradient-TD가 이 방법을 사용하는데, 식 (11.27)에서 두 번째 항과 세 번째 항을 곱한 결과를 추정합니다. 두 번째 항이 $d \times d$ 행렬이고, 세 번째 항이 $d \times 1$ Vector이기 때문에 곱셈의 결과는 $d \times 1$ 크기가 됩니다. 이 Vector를 다음과 같이 $\mathbf{v}$로 표현합니다.&lt;/p&gt;

\[\mathbf{v} \approx \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \tag{11.28}\]

&lt;p&gt;식 (11.28)과 같은 형태는 Linear Supervised Learning에서 자주 보던 형태입니다. Feature에서 $\rho_t \delta_t$를 근사하기 위한 Linear Least Square 문제의 해법입니다. 이것은 다음과 같이 Importance Sampling Ratio가 포함된 식 $(\mathbf{v}^{\sf T} \mathbf{x}_t - \rho_t \delta_t)^2$의 Expected Squared Error를 최소화하는 Vector $\mathbf{v}$를 점진적으로 계산하는 표준 SGD 방법으로 구할 수 있으며, 이 방법을 &lt;span style=&quot;color:red&quot;&gt;Least Mean Square (LMS)&lt;/span&gt;라고 합니다.&lt;/p&gt;

\[\mathbf{v}_{t+1} \doteq \mathbf{v}_t + \beta \rho_t \big( \delta_t - \mathbf{v}_t^{\sf T} \mathbf{x}_t \big) \mathbf{x}_t\]

&lt;p&gt;위의 식에서 $\beta &amp;gt; 0$은 또 다른 Step-size Parameter입니다. 이런 Update 방법을 사용하면 식 (11.28)을 $O(d)$의 시간 복잡도로 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게 식 (11.28)에서 $\mathbf{v}_t$를 계산하고 나면 식 (11.27)을 기반으로 하는 SGD 방법을 사용하여 매개변수 $\mathbf{w}_t$를 Update할 수 있습니다. 이 과정을 정리하면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha \nabla \overline{\text{PBE}} (\mathbf{w}_t) \tag{the general SGD rule} \\ \\
&amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha 2 \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \tag{from (11.27)} \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \tag{11.29} \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbf{v}_t \tag{based on (11.28)} \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \rho_t (\mathbf{x}_t - \gamma \mathbf{x}_{t+1}) \mathbf{x}_t^{\sf T} \mathbf{v}_t \tag{sampling}
\end{align}\]

&lt;p&gt;위와 같은 Update 식을 사용한 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;GTD2&lt;/span&gt;라고 합니다. 만약 마지막의 Inner Product $\mathbf{x}_t^{\sf T} \mathbf{v}_t$가 먼저 수행되면 전체 알고리즘은 $O(d)$의 시간 복잡도로 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;머리를 더 쓴다면, 식 (11.29) 부분에서 $\mathbf{v}_t$로 대체하기 전에 몇 가지 단계를 더 거쳐 약간 더 나은 알고리즘으로 만들 수도 있습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t + \alpha \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \big( \mathbb{E} \left[ \rho_t \mathbf{x}_t \mathbf{x}_t^{\sf T} \right] - \gamma \mathbb{E} \left[ \rho_t \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \right] \big) \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \big( \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right] - \gamma \mathbb{E} \left[ \rho_t \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \right] \big) \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \big( \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] - \gamma \mathbb{E} \left[ \rho_t \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \right] \big) \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \big( \mathbb{E} \left[ \rho_t delta_t \mathbf{x}_t \right] - \gamma \mathbb{E} \left[ \rho_t \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \right] \mathbf{v}_t \big) \tag{based on (11.28)} \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \rho_t \big( \delta_t \mathbf{x}_t - \gamma \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \mathbf{v}_t \big) \tag{sampling}
\end{align}\]

&lt;p&gt;마찬가지로, 마지막 곱셈 부분인 $(\mathbf{x}_t^{\sf T} \mathbf{v}_t)$가 먼저 수행되면 시간 복잡도는 $O(d)$가 됩니다. 이 알고리즘은 &lt;span style=&quot;color:red&quot;&gt;TD(0) with Gradient Correction (TDC)&lt;/span&gt; 또는 &lt;span style=&quot;color:red&quot;&gt;GTD(0)&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 Baird’s Counterexample에 대한 Sample과 TDC의 추정 결과를 보여줍니다. 의도했던 대로 $\overline{\text{PBE}}$는 0으로 감소하지만 매개변수 Vector $\mathbf{w}$의 구성 요소들은 0에 접근하지 않습니다. 그래프를 보면 최적의 해법인 $\hat{v} = 0$과는 거리가 멀며, 다른 최적의 해법이라도 $\mathbf{w}$가 $(1, 1, 1, 1, 1, 1, 4, -2)^{\sf T}$에 비례해야 하는데, 1000번의 반복 후에도 그것에 가깝게 수렴하지 않습니다. 사실 실제로 최적의 해법으로 수렴을 하고 있긴 하지만, $\overline{\text{PBE}}$가 이미 0에 너무 가깝기 때문에 수렴 속도가 너무 느릴 뿐입니다.&lt;/p&gt;

&lt;p&gt;GTD2와 TDC는 $\mathbf{w}$를 학습하는 프로세스(1차 프로세스)와 $\mathbf{v}$를 학습하는 프로세스(2차 프로세스)로 나눌 수 있습니다. 1차 프로세스는 2차 프로세스에 의존하는 반면, 2차 프로세스는 독립적으로 진행됩니다. 이렇게 비대칭적으로 의존하는 성질을 &lt;span style=&quot;color:red&quot;&gt;Cascade&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Cascade에서는 보통 2차 프로세스가 더 빠르게 진행되므로 항상 1차 프로세스를 지원할 준비가 되어 있고, 정확한 값에 점근적이라고 가정합니다. Cascade에 대한 수렴 증명은 보통 이렇게 명시적으로 가정합니다. 이런 증명 방법을 &lt;span style=&quot;color:red&quot;&gt;Two-time-scale 증명&lt;/span&gt;이라고 합니다. &lt;strong&gt;Fast time scale&lt;/strong&gt;은 2차 프로세스를 의미하며, &lt;strong&gt;Slower time scale&lt;/strong&gt;은 1차 프로세스를 의미합니다. 만약 1차 프로세스의 Step-size Parameter를 $\alpha$, 2차 프로세스의 Step-size Parameter를 $\beta$라고 한다면 수렴에 대한 증명은 보통 $\beta \to 0$과 $\frac{\alpha}{\beta} \to 0$이라는 조건이 필요합니다.&lt;/p&gt;

&lt;p&gt;Gradient-TD 방법은 현재 가장 널리 사용되는 안정적인 Off-policy 방법입니다. 이에 대한 여러 연구가 많이 이루어졌는데, 대표적으로 Semi-gradient TD와 Gradient-TD 방법을 융합한 Hybrid-TD 방법이 있습니다. (Hackman, 2012; White and White, 2016) Hybrid-TD 방법은 Target Policy와 Behavior Policy가 동일한 State에서는 Semi-gradient TD 처럼 동작하고, 그렇지 않은 State에서는 Gradient-TD 처럼 동작하는 방법입니다. 마지막으로, 이 외에도 Gradient TD 방법의 아이디어는 Proximal 방법이나 Control 방법에 융합한 연구도 있습니다. (Mahadevan et al., 2014; Du et al., 2017)&lt;/p&gt;

&lt;h2 id=&quot;emphatic-td-methods&quot;&gt;Emphatic-TD Methods&lt;/h2&gt;

&lt;p&gt;이제 두 번째 아이디어로 넘어가서, 특별한 Distribution에 의존하지 않는 새로운 Gradient 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;Section 9.4에서 Linear Semi-gradient TD 방법은 On-policy Distribution에서 훈련될 때 효율적이고 안정적임을 배웠습니다. 이것은 식 (9.11)의 행렬 $\mathbf{A}$가 Positive Definiteness인 것, On-policy의 State Distribution $\mu_{\pi}$, Target Policy 하에 State Transition 확률 $p(s \mid s, a)$과 관련이 있었습니다. Off-policy에서는 Importance Sampling을 사용하여 State Transition에 대한 Weight를 부여하여 Target Policy를 학습하였으나, State Distribution은 여전히 Behavior Policy로부터 나왔었습니다.&lt;/p&gt;

&lt;p&gt;이것을 해결하는 방법은 일부 State를 강조하고, 나머지 State를 덜 강조하여 Update Distribution을 On-policy Distribution으로 만드는 것입니다. 그렇게 하면 안정성과 수렴성이 뒤따를 것이기 때문입니다. 이것이 이번 Section에서 배울 Emphatic-TD 방법의 핵심 아이디어입니다.&lt;/p&gt;

&lt;p&gt;사실 On-policy Distribution는 여러 개가 있고, 그 중 어떤 것이든 안정성을 보장하는데 충분하기 때문에 &lt;strong&gt;On-policy Distribution&lt;/strong&gt;이라는 개념은 맞지 않습니다.&lt;/p&gt;

&lt;p&gt;Discount가 없는 Episodic Task를 가정해봅시다. 여기서 Episode가 종료되는 방식은 Transition Probability에 의해 결정되지만, Episode가 시작되는 방법은 여러 방법이 있을 수 있습니다. 그러나 Episode가 시작되고, 만약 모든 State Transition이 Target Policy에 의해 일어난다면 State Distribution은 On-policy Distribution에 의한 결과라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;그리고 Episode가 어디에서 시작하는지에 따라 방문하는 State의 수가 달라질 것이라고 예상할 수 있습니다. 만약 Episode가 Terminal State에 가까운 곳에서 시작한다면 종료 전까지 몇 개의 State만 방문하고 끝나겠지만, Terminal State에서 먼 곳에서 시작한다면 많은 State를 방문할 수 있기 때문입니다. 두 경우 모두 On-policy Distribution이며, Semi-gradient 방법으로 안정적인 학습을 보장할 수 있습니다. 하지만 프로세스가 시작되면, On-policy Distribution은 Episode가 종료될 때까지 방문하는 모든 State를 Update 하는 것과 같은 결과를 갖습니다.&lt;/p&gt;

&lt;p&gt;만약 Discount가 있는 경우 이러한 목적을 위해 부분적으로, 또는 확률적으로 Episode가 종료하게 만들 수 있습니다. 예를 들어, Discount Factor를 $\gamma = 0.9$로 설정한다면 모든 시간 단계에서 $0.1$의 확률로 Episode가 종료되고 Transition된 State에서 새로운 Episode로 시작한다는 것으로 볼 수 있습니다. 즉, Discount를 사용한 문제는 모든 시간 단계에서 $1 - \gamma$ 확률로 Episode가 종료되고 다시 시작되는 문제나 마찬가지입니다. Discount를 이러한 관점으로 보는 것은 &lt;span style=&quot;color:red&quot;&gt;Pseudo Termination&lt;/span&gt;에 대한 일반적인 개념의 한 예시입니다. Pseudo Termination에서 Episode의 종료는 State Transition의 순서에 영향을 미치지 않지만, 학습 과정과 학습의 양에는 영향을 미친다는 의미입니다. 이러한 종류의 Pseudo Termination은 Off-policy 학습에 중요합니다. 왜냐하면 원하는 방식으로 Episode를 다시 시작할 수 있기 때문입니다. Pseudo Termination으로 인해 On-policy Distribution을 토대로 방문한 State를 유지할 필요성이 줄어듭니다. 즉, 만약 새 State를 다시 시작한 Episode로 간주하지 않으면 Discounting을 통해 빠르게 On-policy Distribution이 제한됩니다.&lt;/p&gt;

&lt;p&gt;Episodic State-Value를 학습하기 위한 1-step Emphatic-TD 알고리즘은 다음과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\delta_t &amp;amp;= R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w}) \\ \\
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t + \alpha M_t \rho_t \delta_t \nabla \hat{v} (S_t, \mathbf{w}_t) \\ \\
M_t &amp;amp;= \gamma \rho_{t-1} M_{t-1} + I_t
\end{align}\]

&lt;p&gt;위 식에서 Interest $I_t$는 임의의 초기값을 갖고, Emphasis $M_t$는 $M_{-1} = 0$으로 초기화됩니다. 아래 그림은 Baird’s counterexample에서 Parameter Vector의 구성 요소 값이 어떻게 변하는지 나타내고 있습니다. (이 그래프는 모든 시간 단계 $t$에 대해서 $I_t = 1$로 설정되어 있다고 가정했습니다) 그래프 중간에서 약간의 출렁임이 보이긴 하지만, 결국 모든 값은 특정 값으로 수렴하며 $\overline{\text{VE}}$는 0에 가까워집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 Trajectory는 Transition나 Reward에 대한 Sampling으로 인한 Variance 없이, Parameter Vector Trajectory의 기대값을 반복적으로 계산해서 얻습니다. 안타깝게도 Emphatic-TD 알고리즘을 직접 적용한다면 Baird’s counterexample에서 Variance가 너무 크기 때문에 계산 실험에서 일관된 결과를 얻는 것이 불가능합니다. 알고리즘은 이 문제에 대해 이론적으로 최적의 해법으로 수렴하지만, 실제로는 그렇지 않습니다. 다음 Section에서는 이런 알고리즘의 Variance를 어떻게 줄일 수 있을지에 대해 논의합니다.&lt;/p&gt;

&lt;h2 id=&quot;reducing-variance&quot;&gt;Reducing Variance&lt;/h2&gt;

&lt;p&gt;Off-policy 학습은 On-policy 학습보다 본질적으로 Variance가 더 클 수밖에 없습니다. 왜냐하면 Policy와 덜 관련있는 데이터를 토대로 학습할수록, Policy의 Value를 그만큼 덜 학습하기 때문입니다. 극단적인 경우를 가정한다면 데이터를 토대로 아무것도 학습하지 못할 수도 있습니다. 결국, Target Policy와 Behavior Policy가 어느 정도는 관련이 있어야만 유사한 State를 방문하고, 유사한 Action을 취하면서 Off-policy 학습을 진행할 수 있습니다.&lt;/p&gt;

&lt;p&gt;다른 한편으로는, 방문한 State와 선택한 Action이 상당히 겹치지만 동일하지는 않은 유사한 Policy가 많이 있습니다. Off-policy 학습의 존재 이유는 이러한 많은 Policy에 일반화를 가능하게 하는 것입니다. 문제는 주어진 경험을 어떻게 최대한 활용하는지에 대한 것입니다. Step-size Parameter가 올바르게 설정된 경우 Expected Value를 안정적으로 구할 몇 가지 방법이 있으므로, 이제는 Estimated Value의 Variance를 줄여보겠습니다. 이에 대한 많은 아이디어가 있지만, 여기서는 그 중 일부만 소개합니다.&lt;/p&gt;

&lt;p&gt;먼저 Importance Sampling을 기반으로 하는 Off-policy 방법에서 Variance를 제어하는 것이 중요한 이유를 논의해보겠습니다. 지금까지 알아본 Importance Sampling에는 종종 Policy Ratio의 곱셈이 포함됩니다. 이 떄의 Ratio는 항상 식 (5.13)과 같은 예상치이지만, 매우 높거나 0에 가까울 정도로 낮을 수도 있습니다. 연속적인 Ratio들은 서로 상관 관계가 없기 때문에 곱셈의 기대값이 항상 1이지만, Variance는 매우 높을 수 있습니다. 이 비율은 SGD 방법에서의 Step-size를 곱하기 때문에 Variance가 크다는 것은 Step-size가 크게 달라지는 단계를 수행한다는 의미가 됩니다. 이것은 때때로 매우 큰 Step-size를 가질 수 있기 때문에 SGD에 문제가 생길 수 있습니다. SGD 방법은 여러 단계의 평균에 의존하기 때문에 단일 Sample에서 크게 이동하면 신뢰할 수 없기 때문입니다.&lt;/p&gt;

&lt;p&gt;이를 해결하는 방법으로 가장 먼저 떠올릴 수 있는 것은 이 문제를 방지할 수 있을 정도로 Step-size Parameter를 작게 설정하는 것입니다. 문제는 이렇게 할 경우 학습 속도가 매우 느려질 수 있다는 것입니다. 따라서 문제가 생길 만한 몇몇 Sample에서만 Step-size Parameter를 조절하는 방법이 연구되었습니다. 대표적으로 매개변수 Vector에 따라 Step-size Parameter를 적응적으로 설정하는 방법이 있습니다. (Jacobs, 1988; Sutton, 1992b, c)&lt;/p&gt;

&lt;p&gt;또는 5장에서 Weighted Importance Sampling이 일반적인 Importance Sampling보다 Variance Update가 낮을 때 훨씬 더 잘 작동한다는 사실을 배웠습니다. 하지만 Weighted Importance Sampling을 Function Approximation에 적용하는 것은 어려운 일이며, $O(d)$의 시간 복잡도에서만 대략적으로 수행할 수 있습니다. (Mahmood and Sutton, 2015)&lt;/p&gt;

&lt;p&gt;Section 7.5에서 배운 Tree-backup 알고리즘은 Importance Sampling을 사용하지 않고 일부 Off-policy 학습을 수행할 수 있었습니다. 이 아이디어는 Munos, Stepleton, Harutyunyan 및 Bellemare(2016)와 Mahmood, Yu 및 Sutton(2017)에 의해 보다 안정적이고 효율적인 Off-policy Policy에 확장되었습니다.&lt;/p&gt;

&lt;p&gt;마지막으로, Target Policy가 부분적으로 Behavior Policy에 의해 결정되도록 만들어서 Importance Sampling Ratio가 비교적 크지 않도록 만드는 방법이 있습니다. (Precup et al., 2006)&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Off-policy 방법은 On-policy와는 다르게 Policy이 Target Policy와 Behavior Policy로 분리되어 Value Function을 학습합니다. Tabular Q-learning을 배웠을 때 Off-policy 학습은 쉬운 것처럼 보였고, Expected Sarsa 및 Tree-backup 알고리즘으로 자연스럽게 일반화까지 해냈습니다. 그러나 이번 장에서 보았듯이 이 아이디어를 Function Approximation, 특히 Linear Function Approximation로 확장하는 것은 생각보다 어려운 일이며, 강화학습에 대한 심도 깊은 이해를 필요로 합니다.&lt;/p&gt;

&lt;p&gt;Off-policy 알고리즘을 찾는 이유 중 하나는 Exploration과 Exploitation 사이의 Trade-off를 처리하는 데 유연성을 제공하기 위해서입니다. 또 다른 이유는 학습으로부터 Action을 분리하여 Target Policy에 얶매이지 않기 위해서입니다. TD 학습은 동시에 여러 작업을 해결하기 위해 하나의 경험을 사용하여 여러가지를 병렬로 학습할 수 있는 가능성을 보여줍니다. 모든 경우에 그런 것은 아니지만, 특별한 경우에는 원하는 만큼 효율적으로 이 작업을 수행할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 장에서는 Off-policy 학습에서 Function Approximation을 위한 해결 방법을 두 부분으로 나누었습니다. 첫 번째 부분에서는 Behavior Policy에 대한 학습 목표를 수정하여 Tabular에서 고안된 기술로 간단하게 처리했습니다. 다만 이 방법은 Update의 Variance를 증가시켜 학습 속도를 느리게 만드는 단점이 있었습니다. Off-policy 학습에서 높은 Variance는 아직까지도 완전히 해결되지 않은 부분이며, 앞으로도 많은 연구자들의 목표로 남을 것입니다.&lt;/p&gt;

&lt;p&gt;두 번째 부분에서는 Off-policy 학습에서 Bootstrapping을 포함한 Semi-gradient TD 방법의 불안정성을 다루었습니다. Function Approximation, Off-policy 학습, Bootstrapping이라는 3가지 위험 요소가 있고, 이 3가지 요소 중 최소한 한 개 이상을 포기해야 불안정성을 제거할 수 있습니다. 이번 장에서 다룬 알고리즘 중 가장 널리 쓰이는 방법은 Bellman Error를 토대로 진짜 SGD 방법을 수행하는 것입니다.&lt;/p&gt;

&lt;p&gt;그러나 이어지는 분석을 통해 이 방법은 많은 경우에 매력적인 목표가 아니며, 결국 학습 알고리즘으로 완성하는 것이 불가능하다는 결론이 나왔습니다. 또 다른 접근 방식인 &lt;strong&gt;Gradient-TD&lt;/strong&gt; 방법은 Projected Bellman Error를 토대로 SGD를 수행합니다. $\overline{\text{PBE}}$의 Gradient는 $O(d)$의 시간 복잡도로 학습할 수 있지만, 두 번째 Step-size와 두 번째 매개변수 Vector가 필요했습니다. 최신 방법으로 &lt;strong&gt;Emphatic-TD&lt;/strong&gt; 방법은 Update에 Weight를 부여하여 일부 State를 강조하는 방식으로 On-policy 학습처럼 만들었습니다. 이로 인해 계산적으로 간단한 Semi-gradient 방법을 사용할 수 있게끔 유도하였습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 정리해보면, Off-policy 학습은 아직도 새로운 영역이며, 불안정합니다. 어떤 방법이 가장 좋을지, 또는 적절할지 명확하지 않습니다. 이번 장에서 소개한 방법이 과연 현실적이며, 필요한 방법인지, 소개한 방법 외에 더 좋은 방법이 있는 것은 아닌지와 같은 연구가 현재도 진행중이기 때문입니다.&lt;/p&gt;

&lt;p&gt;11장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이 책은 5장 이후로 Generalized Policy Iteration (GPI)에서 내재된 Exploitation과 Exploration 사이의 Trade-off를 처리하는 방법으로 On-policy와 Off-policy를 사용했습니다. 9장과 10장에서는 On-policy의 경우를 Function Approximation로 처리했으며, 이번 장에서는 Off-policy에서의 Function Approximation을 다룰 예정입니다. Off-policy 방법을 Function Approximation로 확장하는 것은 On-policy의 경우에서와 다른 점도 많고 어려운 점도 많습니다.</summary></entry><entry><title type="html">On-policy Control with Approximation</title><link href="http://localhost:4000/rl/on-policy-control-with-approximation/" rel="alternate" type="text/html" title="On-policy Control with Approximation" /><published>2022-05-20T00:00:00+09:00</published><updated>2022-05-20T00:00:00+09:00</updated><id>http://localhost:4000/rl/on-policy-control-with-approximation</id><content type="html" xml:base="http://localhost:4000/rl/on-policy-control-with-approximation/">&lt;p&gt;지난 장에서 근사를 이용한 Value Function Approximation에 대해 알아보았습니다. 이번 장에서는 매개변수를 사용하여 Action-Value Function $\hat{q}(s, a, \mathbf{w}) \approx q_* (s, a)$를 근사하는 Control 문제를 다루겠습니다. (Weight Vector $\mathbf{w} \in \mathbb{E}^d$는 유한 차원 Vector입니다) 이번 장에서는 먼저 On-policy에만 집중하고, Off-policy의 문제는 다음 장에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;가장 먼저 지난 장에서 다룬 Semi-gradient TD(0)를 확장한 Semi-gradient Sarsa 알고리즘을 다룹니다. Episodic Task에서는 이 확장이 간단하지만, Continuing Task에서는 Optimal Policy를 찾기 위해 Discounting을 다시 사용해야 합니다. 신기한 점은, 진짜 Function Approximation을 얻게 되면 Discounting을 버리고 새로운 Differential Value Function을 사용하여 Control 문제를 새로운 Average-reward 식으로 바꿔야 합니다.&lt;/p&gt;

&lt;p&gt;Episodic Task에서는 먼저 지난 장에서 다룬 함수 근사를 State-Value에서 Action-Value로 확장합니다. 그런 다음 $\epsilon$-greedy를 사용한 Action 선택을 통해 On-policy GPI의 일반적인 패턴을 따라 Control하도록 확장합니다. 그 후 Mountain Car 예제에서 $n$-step Linear Sarsa가 어떤 성능을 보이는지 결과를 확인하고, Continuing Task로 넘어가는 순서로 진행하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;episodic-semi-gradient-control&quot;&gt;Episodic Semi-gradient Control&lt;/h2&gt;

&lt;p&gt;먼저 지난 장에서 배운 Semi-gradient Prediction을 Action-Value로 확장해보겠습니다. 이 경우 Weight Vector $\mathbf{w}$를 매개변수로 사용하여 Action-Value Function으로 표현한다면 $\hat{q} \approx q_{\pi}$가 됩니다. $S_t \mapsto U_t$ 형태의 무작위 Training Data를 고려하기 전에, $S_t, A_t \mapsto U_t$ 형태의 데이터를 먼저 살펴보겠습니다. Update의 Target인 $U_t$는 Monte Carlo의 Return $G_t$나 $n$-step Sarsa의 Return 식 (7.4)와 같이 일반적인 Backed-up 값을 포함하여 $q_{\pi} (S_t, A_t)$의 근사값이 될 수 있습니다. Action-Value 예측을 위한 일반적인 Gradient Descent Update는 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ U_t - \hat{q} (S_t, A_t, \mathbf{w}_t) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{10.1}\]

&lt;p&gt;이것을 1-step Sarsa 방법에 맞게 수정하면 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ R_{t+1} + \gamma \hat{q}(S_{t+1, A_{t+1}, \mathbf{w}_t}) - \hat{q} (S_t, A_t, \mathbf{w}_t) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{10.2}\]

&lt;p&gt;이 Update 식을 사용한 Control 방법을 &lt;span style=&quot;color:red&quot;&gt;Episodic Semi-gradient 1-step Sarsa&lt;/span&gt;라고 부릅니다. 고정된 Policy의 경우 이 방법은 TD(0)와 동일한 방식으로 수렴하며, 식 (9.14)와 동일한 Error Bound를 가집니다.&lt;/p&gt;

&lt;p&gt;Control 방법을 제대로 구축하기 위해서는 이러한 Action-Value Approximation을 Policy Improvement 및 Action 선택을 위한 방법들과 융합해야 합니다. 하지만 Continuous Action이나 수가 매우 많은 Discrete Action이 주어졌을 때 사용할 수 있는 적절한 기술은 현재도 연구가 이루어질 정도로 아직 명확한 방법이 없습니다. 다행히 Action 집합이 이산적이고 그 수가 적절하다면 이전 장에서 소개한 방법을 도입할 수 있습니다. 즉, 다음 State $S_{t+1}$에서 사용 가능한 Action에 대해 $\hat{q}(S_{t+1}, a, \mathbf{w}_t)$를 계산하면 Greedy Action인 $A_{t+1}^* = \underset{a}{\operatorname{argmax}} \hat{q} (S_{t+1}, a, \mathbf{w}_t)$를 구할 수 있습니다. 그 후 $\epsilon$-greedy와 같은 Softmax 방법을 통해 Policy Improvement를 수행할 수 있습니다. 이 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 10.1) Mountain Car Task&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래 그림의 왼쪽 위를 보시면 자동차가 가파른 산길을 통과하려고 합니다. 그런데 이 자동차는 동력이 부족하기 때문에 단순히 엔진의 힘으로는 Goal 지점에 도달할 수 없고, 유일한 해결책은 반대방향으로 먼저 자동차를 후진한 다음 가속도와 관성을 이용해 Goal 지점에 도달하는 것입니다. 이 예제는 목표에 도달하기 위해 오히려 목표에서 멀어져야 하는 Control 문제입니다. 많은 기존의 Control 방법들은 명시적으로 이러한 문제의 해결 방법을 지원하지 않는 한 해결하기 쉽지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mountain Car 문제에서 Reward는 자동차가 Goal 위치를 지나갈 때까지(=Episode가 종료될 때까지) 모든 시간 단계에서 -1로 정의되어 있습니다. 자동차가 선택할 수 있는 Action은 3개로, 전진(+1), 후진(-1) 및 정지(0)입니다. 자동차는 단순한 물리 법칙에 따라 움직이며, 자동차의 위치 $x_t$ 및 속도 $\dot{x}_t$는 다음과 같이 Update됩니다.&lt;/p&gt;

\[\begin{align}
x_{t+1} &amp;amp; \doteq \text{bound} \left[ x_t + \dot{x}_{t+1} \right] \\ \\
\dot{x}_{t+1} &amp;amp; \doteq \text{bound} \left[ \dot{x}_t + 0.001 A_t - 0.0025 \cos (3x_t) \right]
\end{align}\]

&lt;p&gt;위 식에서 $\text{bound}$ 연산은 위치와 속도가 $-1.2 \le x_{t+1} \le 0.5$ 및 $-0.07 \le \dot{x}_{t+1} \le 0.07$ 구간을 벗어나지 않게 만드는 연산입니다. 또한, $x_{t+1}$이 왼쪽 경계에 도달하면 $\dot{x}_{t+1}$는 0으로 재설정됩니다. 만약 오른쪽 경계에 도달하면 Goal 지점에 도달한 것이니 Episode가 종료됩니다. 각각의 Episode는 임의의 위치 $x_t \in \left[ -0.6, -0.4 \right)$에서 속도 0으로 시작합니다. 위치와 속도는 모두 Continuous State이므로 이를 Binary Feature로 변환하기 위해 Tile Coding을 사용하였습니다. Tile Coding에 의해 생성된 Feature Vector $\mathbf{x} (s, a)$는 Action-Value Function를 근사화하기 위해 다음과 같이 Weight Vector와 선형으로 결합되었습니다.&lt;/p&gt;

\[\hat{q} (s, a, \mathbf{w}) \doteq \mathbf{w}^{\sf T} \mathbf{x} (s, a) = \sum_{i=1}^d w_i \cdot x_i (s, a) \tag{10.3}\]

&lt;p&gt;위의 그림은 이 함수 근사로 Mountain Car 문제를 해결하는 방법을 학습하는 동안 발생하는 일을 나타내고 있습니다. 그래프는 단일 실행에서 학습된 Value Function의 부호를 뒤집은 결과입니다. 초기의 Action Value는 모두 0이었습니다. 모든 Action에 대한 Reward가 음수이기 때문에 Action Value를 낙관적으로 본 것이며, 이것을 통해 $\epsilon = 0$일지라도 탐색이 일어나도록 유도한 것입니다. 그림 중간에 428단계에서는 한 Episode도 완료되지 않았지만 자동차는 State 공간에서 궤도를 따라 앞뒤로 움직였습니다. 자주 방문하는 모든 State는 그렇지 않은 State보다 더 나쁘게 평가되고, 실제 Reward 또한 예상했던 것보다 나빴기 때문입니다. 이는 해법을 찾을 때까지 자동차가 있던 곳에서 계속 멀어져 새로운 State를 탐색하도록 유도합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그래프는 다양한 Step-size에 대해 Mountain Car 문제에서 Semi-gradient Sarsa의 학습 곡선을 보여줍니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;semi-gradient-n-step-sarsa&quot;&gt;Semi-gradient $n$-step Sarsa&lt;/h2&gt;

&lt;p&gt;Semi-gradient Sarsa의 $n$-step 버전의 Return만 구하면 식 (10.1)의 Update 식을 그대로 사용하여 계산할 수 있습니다. $n$-step 버전의 Return은 식 (7.4)를 함수 근사 형식으로 일반화하면 다음과 같이 변형할 수 있습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots +  \gamma^{n-1} R_{t+n} + \gamma^n \hat{q} (S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \quad t+n &amp;lt; T \tag{10.4}\]

&lt;p&gt;식 (7.4)와 마찬가지로 $t + n \ge T$라면 $G_{t:t+n} \doteq G_t$입니다. $n$-step Update는 식 (10.1)과 크게 다르지 않습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G_{t:t+n} - \hat{q} (S_t, A_t, \mathbf{w}_t) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_t), \quad 0 \le t &amp;lt; T \tag{10.5}\]

&lt;p&gt;Semi-gradient $n$-step Sarsa의 전체 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;7장에서 언급한 것과 마찬가지로 $n$이 1보다 크면서 중간 단계만큼 Bootstrapping 하는 경우 성능이 가장 좋습니다. 아래 그림은 $n=8$이 $n=1$보다 더 빨리 학습하는 것을 보여줍니다. 또한 그 아래 그림은 이 문제에서 매개변수 $\alpha$와 $n$이 학습에 어떠한 영향을 끼치는지 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;average-reward-a-new-problem-setting-for-continuing-tasks&quot;&gt;Average Reward: A New Problem Setting for Continuing Tasks&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 Continuing Task로 넘어가겠습니다. Continuing Task는 Episodic Task와 다르게 종료되지 않고 영원히 계속되는 문제이므로 Return을 다른 방법으로 계산합니다. 대표적으로 Average Reward 방법이 있습니다. Average Reward 방법에서 Policy $\pi$의 우수함은 Average Rate of Reward, 또는 단순히 Average Reward로 정의되며, 다음과 같이 $r(\pi)$로 표현되는 Policy를 따릅니다.&lt;/p&gt;

\[\begin{align}
r(\pi) &amp;amp; \doteq \lim_{h \to \infty} \frac{1}{h} \sum_{t=1}^h \mathbb{E} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \tag{10.6} \\ \\
&amp;amp;= \lim_{t \to \infty} \mathbb{E} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \tag{10.7} \\ \\
&amp;amp;= \sum_s \mu_{\pi} (s) \sum_a \pi (a|s) \sum_{s&apos;, r} p(s&apos;, r | s, a) r
\end{align}\]

&lt;p&gt;위 식에서의 기대값은 초기 State $S_0$와 Policy $\pi$를 따르는 Action $A_0, A_1, \ldots, A_{t-1}$에 따라 정해집니다. 두 번째 및 세 번째 식은 Steady-state Distribution $\mu_{\pi} (s) \doteq \underset{t \to \infty}{\operatorname{lim}} \text{Pr} \{ S_t = s \mid A_{0:t-1} \sim \pi \}$가 존재하고 $S_0$에 독립적일 때 성립합니다. 즉, MDP가 &lt;strong&gt;Ergodic&lt;/strong&gt; 할 때 성립합니다. Ergodic MDP에서 시작 State와 Agent가 초기에 내린 선택은 일시적인 효과만 있습니다. 장기적으로 어떤 State에 있을지는 Policy와 MDP의 Transition Probability에만 영향을 받습니다. Ergodicity는 충분조건이지만, 식 (10.6)에서 극한의 존재를 보장할 필요는 없습니다.&lt;/p&gt;

&lt;p&gt;Discounting이 없는 Continuing Task의 경우 Optimality의 종류에 따라 약간의 차이가 있습니다. 하지만 대부분 실용적인 목적을 위해서는 시간 단계당 Average Reward, 즉 $r(\pi)$에 맞춰 Policy를 따르는 것이 적절합니다. 이것은 식 (10.7)에서 보여주는 $\pi$의 Average Reward, 또는 Reward Rate입니다. 여기서는 $r(\pi)$의 최대값을 달성하는 모든 Policy를 Optimal Policy로 간주하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Steady-state Distribution $\mu_{\pi}$는 Policy $\pi$에 따라 Action을 선택하면 동일한 Distribution을 유지하는 특수한 Distribution입니다. 수학적으로는 다음과 같이 표현합니다.&lt;/p&gt;

\[\sum_s \mu_{\pi} (s) \sum_a \pi(a|s) p(s&apos;|s, a) = \mu_{\pi} (s&apos;) \tag{10.8}\]

&lt;p&gt;Average Reward 방법에서 Return은 Reward와 Average Reward 간의 차이로 정의됩니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \cdots \tag{10.9}\]

&lt;p&gt;식 (10.9)와 같은 Return을 &lt;span style=&quot;color:red&quot;&gt;Differential Return&lt;/span&gt;이라고 하며, 이것을 사용한 Value Function을 &lt;span style=&quot;color:red&quot;&gt;Differential Value Function&lt;/span&gt;이라고 합니다. 기존 Value Function이 Discounted Return으로 정의된 것처럼 Differential Value Function 또한 새로운 Return의 관점에서 정의됩니다. 따라서 기존과 마찬가지로 Differential Value Function에서도 $v_{\pi} (s) \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right]$, $q_{\pi} (s, a) \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s, A_t = a \right]$와 같은 동일한 표기법을 사용할 것입니다. Differential Value Function에서도 Bellman Equation이 있는데, 이것은 이전에 다룬 것과 약간 다릅니다. 간단하게 설명하면, $\gamma$를 모두 제거하고 모든 Reward를 Reward와 실제 Average Reward 간의 차이로 대체합니다.&lt;/p&gt;

\[\begin{align}
v_{\pi} (s) &amp;amp;= \sum_a \pi(a|s) \sum_{r, s&apos;} p(s&apos;,r|s, a) \left[ r - r(\pi) + v_{\pi} (s&apos;) \right] \\ \\
q_{\pi} (s, a) &amp;amp;= \sum_{r, s&apos;} p (s&apos;, r | s, a) \left[ r - r(\pi) + \sum_{a&apos;} \pi (a&apos;|s&apos;) q_{\pi} (s&apos;, a&apos;) \right] \\ \\
v_* (s) &amp;amp;= \max_a \sum_{r, s&apos;} p(s&apos;, r|s, a) \left[ r - \max_{\pi} r(\pi) + v_* (s&apos;) \right] \\ \\
q_* (s, a) &amp;amp;= \sum_{r, s&apos;} p(s&apos;, r | s, a) \left[ r - \max_{\pi} r(\pi) + \max_{a&apos;} q_* (s&apos;, a&apos;) \right]
\end{align}\]

&lt;p&gt;마찬가지로 Differential TD Error 또한 다음과 같이 변경됩니다.&lt;/p&gt;

\[\begin{align}
\delta_t &amp;amp; \doteq R_{t+1} - \bar{R}_t + \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{10.10} \\ \\
\delta_t &amp;amp; \doteq R_{t+1} - \bar{R}_t + \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \tag{10.11}
\end{align}\]

&lt;p&gt;여기서 $\bar{R}_t$는 시간 단계 $t$에서 Average Reward $r(\pi)$를 추정한 값입니다. 이렇게 변형된 식을 알고리즘에 대입하기만 하면 이론적으로 큰 문제 없이 Average Reward에 대한 방법으로 바꿀 수 있습니다. 예를 들어, Semi-gradient Sarsa의 Average Reward 버전은 식 (10.2)를 다음과 같이 변경하면 됩니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w} + \alpha \delta_t \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{10.12}\]

&lt;p&gt;식 (10.12)에서 $\delta_t$는 식 (10.11)과 같습니다. Differential Semi-gradient Sarsa 알고리즘의 완전한 Pseudocode는 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 알고리즘의 한 가지 문제점은 Differential Value로 수렴하지 않고 Differential Value에 임의의 Offset을 더한 값으로 수렴된다는 것입니다. 위에서 언급한 Bellman Equation과 TD Error는 모든 값이 같은 양만큼 변화해도 영향을 받지 않습니다. 따라서 Offset은 실제로 중요하지 않을 수 있지만, 이 Offset을 제거하기 위한 연구는 지금도 이루어지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 10.2) An Access-Control Queuing Task&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이 예제는 10개의 서버 집합에 대해 접근 제어와 관련된 결정 문제입니다. 이용자는 4가지 종류의 우선순위로 분류된 작업을 단일 Queue에 보냅니다. 서버에 대한 접근 권한이 부여되면 이용자는 우선 순위에 따라 서버에 1, 2, 4 또는 8의 비용을 지불하는데, 우선 순위가 높은 이용자는 더 많은 비용을 지불합니다. 각 시간 단계에서 Queue 맨 앞에 있는 이용자는 Accept (서버 중 하나에 할당)되거나 Reject (대기열에서 0의 Reward를 받고 제거) 됩니다. 어떤 결과가 나오든 다음 이용자의 순서로 넘어갑니다. Queue는 절때 비지 않고, Queue에 있는 이용자의 우선 순위는 무작위로 분포되어 있다고 가정합니다. 물론 여유있는 서버가 없다면 이용자의 요청은 모두 거부됩니다. 사용 중인 서버는 각각의 시간 단계에서 확률 $p = 0.06$에 따라 다시 여유 State로 돌아옵니다. 이용자의 도착 및 출발 통계는 알 수 없다고 가정하겠습니다. 이 문제의 목적은 이용자의 우선 순위와 여유 State의 서버 수에 따라 요청을 Accept할지, Reject 할지를 결정하는 것입니다. 즉, 장기적인 Reward를 극대화하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;교재에서는 이 문제를 Tabular 방식으로 해결하였습니다. State 간의 일반화는 없지만, 일반적인 함수 근사를 사용할 수는 있습니다. 따라서 각 State의 쌍(여유 State의 서버 수와 맨 앞에 있는 이용자의 우선 순위)과 Action(Accept 또는 Reject)에 대해 Differential Action-Value Function을 추정합니다. 위의 그림은 $\alpha = 0.01$, $\beta = 0.01$, $\epsilon = 0.1$로 설정한 Differential Semi-gradient Sarsa 해법을 보여줍니다. 초기의 Action-Value와 $\bar{R}$은 0으로 설정하였습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;deprecating-the-discounted-setting&quot;&gt;Deprecating the Discounted Setting&lt;/h2&gt;

&lt;p&gt;Continuing, Discounted Task에서는 각 State의 Return을 개별적으로 구분할 수 있는 Tabular에서 매우 유용했습니다. 하지만 근사를 사용하는 경우에는 Tabular에서 생각하지 못했던 여러 문제점이 발생하게 됩니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 시작이나 끝이 없고 State를 제대로 식별하지 못하는 무한 Return을 가정해봅시다. 각 State는 Feature Vector로만 표시할 수 있기 때문에 State를 구별하는 데 거의 도움이 되지 않습니다. 어떤 경우에는 모든 Feature Vector가 동일할 수도 있습니다. 따라서 Reward에 대한 순서만 주어지고 성능은 이러한 순서로만 평가될 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하는 방법 중 하나는 장기간에 걸쳐 Reward를 평균화하는 것입니다. 이것이 이전 Section에서 다룬 Average Reward의 아이디어입니다. 만약 Discount를 사용하고 싶다면 각 시간 단계에 대해 Discount된 Return을 측정할 수 있는데, 어떤 Return은 크고 어떤 Return은 작을 수 있으므로 충분히 긴 시간 단계에 걸쳐 평균을 내야 합니다. 그런데 간단히 생각해보면 결국 Discounted Return의 평균이 Reward의 평균에 비례한다는 것을 알 수 있습니다. 수학적으로 따져봐도 Policy $\pi$에 대해 Discounted Return의 평균은 $r(\pi) / ( 1 - \gamma)$입니다. 즉, 사실상 $r(\pi)$나 마찬가지입니다. 따라서 Discounted Return의 평균에서 Policy의 Ordering은 Average Reward에서와 동일하므로, Discount Factor $\gamma$는 문제에 영향을 끼치지 않습니다. Discount Factor는 $0 \le \gamma &amp;lt; 1$이기 때문입니다.&lt;/p&gt;

&lt;p&gt;이것을 엄밀하게 증명하기 전에, 먼저 직관적으로 간단하게 설명하겠습니다. (State를 제대로 식별하지 못하기 때문에) 각각의 시간 단계가 다른 모든 시간 단계와 동일하다고 가정해보겠습니다. Discount를 사용한 시간 단계 $t$의 Reward는 시간 단계 $t-1$의 Return에서 Discount를 사용하지 않은채 나오고, 시간 단계 $t-2$의 Return에서 1번 Discount 되며, 같은 이치로 시간 단계 $t-1000$의 Return에서 999번 Discount 됩니다. 따라서 시간 단계 $t$에서 Reward의 Weight는 $1 + \gamma + \gamma^2 + \gamma^3 + \cdots = 1/(1-\gamma)$가 됩니다. 모든 State가 동일하기 때문에 모든 State에 대해 이 Weight가 곱해지므로 Reward의 평균은 $r(\pi) / ( 1 - \gamma)$가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Futility of Discounting in Continuing Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이 부분에서는 Policy $\pi$에 따른 Discounted Value의 합이 어떻게 Average Reward 방법과 동일한지 수학적으로 식을 전개하도록 하겠습니다. Discounted Value Function을 $v_{\pi}^{\gamma}$라고 하면,&lt;/p&gt;

\[\begin{align}
J(\pi) &amp;amp;= \sum_s \mu_{\pi} (s) v_{\pi}^{\gamma} (s) \\ \\
&amp;amp;= \sum_s \mu_{\pi}(s) \sum_a \pi (a|s) \sum_{s&apos;} \sum_{r} p (s&apos;, r | s, a) \left[ r + \gamma v_{\pi}^{\gamma} (s&apos;) \right] \tag{Bellman Eq.} \\ \\
&amp;amp;= r(\pi) + \sum_s \mu_{\pi} (s) \sum_a \pi (a|s) \sum_{s&apos;} \sum_r p (s&apos;, r | s, a) \gamma v_{\pi}^{\gamma} (s&apos;) \tag{from (10.7)} \\ \\
&amp;amp;= r(\pi) + \gamma \sum_{&apos;s} v_{\pi}^{\gamma} (s&apos;) \sum_s \mu_{\pi} (s) \sum_a \pi (a|s) p (s&apos; | s, a) \tag{from (3.4)} \\ \\
&amp;amp;= r(\pi) + \gamma \sum_{s&apos;} v_{\pi}^{\gamma} (s&apos;) \mu_{\pi} (s&apos;) \tag{from (10.8)} \\ \\
&amp;amp;= r(\pi) + \gamma J(\pi) \\ \\
&amp;amp;= r(\pi) + \gamma r(\pi) + \gamma^2 J(\pi) \\ \\
&amp;amp;= r(\pi) + \gamma r(\pi) + \gamma^2 r(\pi) + \gamma^3 r(\pi)  + \cdots \\ \\
&amp;amp;= \frac{1}{1 - \gamma} r(\pi)
\end{align}\]

&lt;p&gt;즉, Discounted Value Function은 Discount되지 않은 Average Reward와 동일한 Policy를 생성하므로, Discount Factor $\gamma$는 영향을 끼치지 않는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;이 증명은 결국 On-policy Distribution에서 Discounted Value를 최적화하는 것과 Undiscounted Average Reward를 최적화 하는 것이 동일하다는 것을 의미합니다. $\gamma$가 실제로 어떤 값이든 상관이 없습니다. 즉, 이것은 함수 근사의 Control 문제에서 Discount가 아무런 역할을 하지 않는다는 것을 보여줍니다. 그럼에도 불구하고 Discount를 사용할 수 없는 것은 아닙니다. Discount Factor $\gamma$를 문제의 매개변수에서 해법의 매개변수로 바꾸면 됩니다. 하지만 그렇게 할 시 함수 근사를 사용하는 Discounting 알고리즘은 On-policy Distribution에서 Discounted Value를 최적화하지 않으므로 Average Reward를 최적화한다고 보장할 수 없습니다.&lt;/p&gt;

&lt;p&gt;Discount를 사용한 Control이 어려운 이유는 함수 근사를 사용하게 되면 Policy Improvement Theorem를 사용할 수 없기 때문입니다. 즉, State의 Discounted Value를 개선하기 위해 Policy를 변경하면 전체 Policy를 개선할 수 있다는 정리가 통용되지 않습니다. 이 정리는 지금까지의 강화학습 Control의 핵심 이론이었으나, 함수 근사를 사용함으로써 이 정리를 사용하는 것을 포기했기 때문입니다.&lt;/p&gt;

&lt;p&gt;사실 Discount를 사용한 Control 뿐만 아니라 전체 Episodic Task와 Average Reward에서도 Policy Improvement Theorem은 통하지 않습니다. 함수 근사를 사용하는 모든 방법은 Policy Improvement Theorem을 사용할 수 없기 때문입니다. 추후 13장에서 매개변수화된 Policy를 토대로 Policy Improvement Theorem과 유사한 정리인 Policy Gradient Theorem을 대안으로 사용할 예정입니다. 하지만 Action-Value를 학습하는 방법들은 Local Improvement에 대한 보장조차 없다고 밝혀졌습니다. (Perkins and Precup, 2003) 예를 들어, $\epsilon$-greedy를 사용하는 Policy는 때때로 비효율적인 Policy를 초래할 수 있기 때문입니다. (Gordon, 1996) 이것은 현재도 이론적인 논쟁과 연구가 이루어지고 있는 부분입니다.&lt;/p&gt;

&lt;h2 id=&quot;differential-semi-gradient-n-step-sarsa&quot;&gt;Differential Semi-gradient $n$-step Sarsa&lt;/h2&gt;

&lt;p&gt;Average Reward를 $n$-step Bootstrapping으로 일반화하려면 TD Error의 $n$-step 버전이 필요합니다. 함수 근사를 사용하여 식 (7.4)의 $n$-step Return을 Differential 형태로 일반화하면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} - \bar{R}_{t+n-1} + \cdots + R_{t+n} - \bar{R}_{t+n-1} + \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}) \tag{10.14}\]

&lt;p&gt;$\bar{R}$은 $r(\pi)$의 추정값이고, $n \ge 1$, $t + n &amp;lt; T$입니다. 만약 $t + n \ge T$라면 $G_{t:t+n} \doteq G_t$ 입니다. $n$-step TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t \doteq G_{t:t+n} - \hat{q}(S_t, A_t, \mathbf{w}) \tag{10.15}\]

&lt;p&gt;Update는 Semi-gradient Sarsa의 식 (10.12)를 그대로 사용합니다. &lt;span style=&quot;color:red&quot;&gt;Differential Semi-Gradient $n$-step Sarsa&lt;/span&gt;의 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서는 지난 장에서 소개한 Parameterized Function Approximation 및 Semi-gradient Descent의 개념을 On-policy Control로 확장했습니다. Episodic Task에서는 확장하는 것이 크게 어렵지 않았으나, Continuing Task에서는 &lt;strong&gt;Average Reward&lt;/strong&gt;라는 새로운 개념을 도입했습니다. 왜냐하면 Discount를 사용한 식은 Function Approximation을 사용할 경우 Control에 있어 여러 이론적 문제가 발생했기 때문입니다. 따라서 Average Reward $r(\pi)$를 사용하여 &lt;strong&gt;Differential Value Function&lt;/strong&gt;과 &lt;strong&gt;Differential TD-Error&lt;/strong&gt;를 새롭게 정의하였습니다.&lt;/p&gt;

&lt;p&gt;다음 장에서는 Function Approximation을 사용한 Off-policy의 Control 방법에 대해 다루겠습니다.&lt;/p&gt;

&lt;p&gt;10장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">지난 장에서 근사를 이용한 Value Function Approximation에 대해 알아보았습니다. 이번 장에서는 매개변수를 사용하여 Action-Value Function $\hat{q}(s, a, \mathbf{w}) \approx q_* (s, a)$를 근사하는 Control 문제를 다루겠습니다. (Weight Vector $\mathbf{w} \in \mathbb{E}^d$는 유한 차원 Vector입니다) 이번 장에서는 먼저 On-policy에만 집중하고, Off-policy의 문제는 다음 장에서 다룰 예정입니다.</summary></entry></feed>