<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-19T20:15:03+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">KEEPMIND</title><subtitle>A place I record so that I don&apos;t forget.</subtitle><author><name>Joonsu Ryu</name></author><entry><title type="html">Policy Gradient Methods</title><link href="http://localhost:4000/studies/policy-gradient-methods/" rel="alternate" type="text/html" title="Policy Gradient Methods" /><published>2022-10-04T00:00:00+09:00</published><updated>2022-10-04T00:00:00+09:00</updated><id>http://localhost:4000/studies/policy-gradient-methods</id><content type="html" xml:base="http://localhost:4000/studies/policy-gradient-methods/">&lt;p&gt;이번 장은 드디어 마지막 장인 &lt;span style=&quot;color:red&quot;&gt;Policy Gradient&lt;/span&gt;입니다. 이번 장에서는 지금까지 이 교재에서 다룬 방법들과는 다르게, Policy 자체를 매개변수화하는 방법을 알아보겠습니다. 지금까지의 방법들은 Estimated Action-Value를 기반으로 Action을 선택했기 때문에 Action-Value를 추정하는 것이 중요했습니다. 하지만 이번 장에서 배울 새로운 방법인 Policy Gradient는 Action을 선택하는 데 Value Function을 사용하지 않습니다. 이번 장에서 사용할 새로운 표기는 Policy에 대한 매개변수 벡터인 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$입니다. 따라서 Policy는 이제 매개변수 $\boldsymbol{\theta}$를 포함하여 $\pi (a \mid s, \boldsymbol{\theta}) = Pr \{ A_t = a \mid S_t = s, \boldsymbol{\theta}_t = \boldsymbol{\theta} \}$로 표현합니다. 이것은 시간 $t$에서 State가 $s$이고 매개변수가 $\boldsymbol{\theta}$일 때 Action $a$를 선택할 확률로 정의됩니다. 만약 학습 알고리즘 안에서 Value Function에 대한 추정을 포함하는 경우, 이전과 마찬가지로 여전히 Weight Vector $\mathbf{w} \in \mathbb{R}^d$를 포함하여 $\hat{v}(s, \mathbf{w})$로 표현합니다.&lt;/p&gt;

&lt;p&gt;새로 정의하는 Policy 매개변수 $\boldsymbol{\theta}$를 학습하기 위해서는 스칼라 성능 측정 함수인 $J(\boldsymbol{\theta})$를 기반으로 합니다. 당연히 성능을 최대화하는 것이 목적이기 때문에, $J$의 Gradient를 상승시키기 위해 $\boldsymbol{\theta}$의 값을 조절합니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \widehat{\nabla J (\boldsymbol{\theta}_t)} \tag{13.1}\]

&lt;p&gt;식 (13.1)에서 $\widehat{\nabla J (\boldsymbol{\theta}_t)} \in \mathbb{R}^{d’}$는 매개변수 $\boldsymbol{\theta}_t$에 대해 성능을 나타내는 Gradient에 가까운 확률적 추정치입니다. 이러한 과정을 따르는 모든 방법은 Approximate Value Function을 학습하는지에 대한 여부에 상관 없는 Policy Gradient Method라고 합니다. 만약 Policy와 Value Function에 대한 근사값을 모두 학습한다면 Actor-Critic Method라고 합니다. Actor는 Policy를 학습하는 것을 의미하며, Critic은 Value Function을 학습하는 것을 말합니다. 이번 장은 Section 10.3과 마찬가지로 먼저 매개변수된 Policy 하에서의 State에 대한 Value로 성능이 정의되는 Episodic Task를 다룬 후, 성능이 Average Reward로 정의되는 Continuing Task를 다룰 예정입니다. 결국 마지막에는, 두 경우 모두에 대해 매우 유사한 용어를 사용하여 알고리즘을 표현할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;policy-approximation-and-its-advantages&quot;&gt;Policy Approximation and its Advantages&lt;/h2&gt;

&lt;p&gt;Policy Gradient Method에서 $\pi (a \mid s, \boldsymbol{\theta})$가 매개변수 $\boldsymbol{\theta}$에 대해 미분할 수 있고 모든 State $s \in \mathcal{S}$와 모든 Action $a \in \mathcal{A}(s)$, 그리고 매개변수 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$에 대해 유한하다면 Policy는 어떤 방식으로든 매개변수화할 수 있습니다. 실제로, 탐색을 보장하기 위해서는 Policy가 절대 Deterministic이 아니어야 합니다. (즉, 모든 $s, a, \boldsymbol{\theta}$에 대해 $\pi (a \mid s, \boldsymbol{\theta}) \in (0, 1)$) 이번 Section에서는 이산적인 Action Space에서의 가장 일반적인 매개변수화 방법을 소개하고, 그것이 Action-Value 방법에 비해 어떤 장점이 있는지 논의하겠습니다. Policy에 기반한 방법은 (추후 Section 13.7에서 설명하는 것처럼) 연속적인 Action Space가 주어졌을 때 유용한 방법이기도 합니다.&lt;/p&gt;

&lt;p&gt;만약 Action Space가 이산적이고 너무 크지 않다면, 일반적으로 떠올릴 수 있는 방법은 각 State-Action 쌍에 대해 매개변수화하는 Numerical Preference $h(s, a, \boldsymbol{\theta})$를 생성하는 것입니다. 각 State에서 Preference가 가장 높은 Action을 선택할 확률이 높게 만드는 것인데, 대표적인 방법으로 다음과 같은 &lt;span style=&quot;color:red&quot;&gt;Exponential Soft-max Distribution&lt;/span&gt;이 있습니다.&lt;/p&gt;

\[\pi (a|s, \boldsymbol{\theta}) \doteq \frac{e^{h(s, a, \boldsymbol{\theta})}}{\sum_b e^{h(s, a, \boldsymbol{\theta})}} \tag{13.2}\]

&lt;p&gt;식 (13.2)에서 $e \approx 2.71828$는 자연 로그의 밑입니다. 여기서 분모는 각 State의 Action을 선택할 확률의 합이 1이 되도록 설정한 것입니다. 이러한 Policy 매개변수화를 &lt;span style=&quot;color:red&quot;&gt;Soft-max in Action Preference&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Action Preference 설정 자체는 임의로 매개변수화할 수 있습니다. 예를 들어, Deep Artificial Neural Network (ANN)로 계산할 수도 있습니다. 여기서 $\boldsymbol{\theta}$는 네트워크의 모든 Connection Weight의 벡터로 구성됩니다. 또는, 간단하게 다음과 같이 선형으로 나타낼 수도 있습니다.&lt;/p&gt;

\[h(s, a, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\sf T} \mathbf{x} (s,a) \tag{13.3}\]

&lt;p&gt;식 (13.3)에서 $\mathbf{x}  (s, a) \in \mathbb(R)^{d’}$는 Section 9.5에서 설명했던 Feature Vector입니다.&lt;/p&gt;

&lt;p&gt;Action Preference의 Soft-max에 따라 Policy를 매개변수화하는 것의 장점 중 하나는 Approximate Policy가 Deterministic Policy에 점점 가까워진다는 것입니다. 지금까지 많이 사용했던 $\epsilon$-greedy의 경우 항상 무작위 Action을 선택할 확률이 존재합니다. 물론 Action-Value를 기반으로 Soft-max Distribution에 따라 Action을 선택할 수도 있지만, 이것만으로는 Policy가 Deterministic Policy에 가까워질 수 없습니다. 대신 Action-Value의 추정치는 그에 해당하는 Real Value값으로 수렴할 뿐이며, 이것은 0과 1이 아닌 특정 확률로 수렴합니다. Soft-max Distribution에 Temperature 매개변수가 포함된 경우 Temperature는 Deterministic에 접근하기 위해 시간이 지남에 따라 감소할 수 있지만, 실제 Action-Value에 대한 사전 지식 없이는 어느 정도 감소하게 할지, 또는 초기 Temperature를 어떻게 설정할 것인가에 대한 문제가 있기 때문입니다. 하지만 Action Preference는 이러한 특정한 값에 가까워지지 않기 때문에 다릅니다. 이것은 오직 Optimal Stochastic Policy를 유도하는데 주력할 뿐입니다. Optimal Policy가 Deterministic이라면, Optimal Action에 대한 Action Preference는 모든 다른 Action보다 무한히 높아집니다.&lt;/p&gt;

&lt;p&gt;Action Preference의 Soft-max에 따라 Policy를 매개변수화하는 것의 두 번째 장점으로는 임의의 확률로 Action을 선택할 수 있다는 것입니다. 특정한 Function Approximation이 있는 문제에서 Optimal Approximate Policy는 Stochastic일 수도 있습니다. 예를 들어, 불완전한 정보를 가진 카드 게임에서의 최적의 플레이는 포커와 같이 임의의 확률로 블러핑을 하는 것입니다. Action-Value 방법은 이런 경우 Stochastic Optimal Policy를 찾는 방법이 없지만, Policy Gradient Method는 다음 예제와 같이 이것이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 13.1) Short corridor with switched actions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프에 삽입된 작은 Gridworld 문제가 있습니다. Reward는 각 단계당 -1로 설정되어 있습니다. Episode는 항상 S에서 시작하고, G에 도착하면 종료됩니다. 맨 오른쪽 State를 제외한 나머지 State에서는 각각 오른쪽/왼쪽으로 이동하는 2가지 Action이 있습니다. (단, S에서 왼쪽으로 가는 Action은 움직이지 않는 것으로 대체합니다) 이 문제에서 재밌는 점은 왼쪽에서 두 번째 State의 경우, Action에 따른 결과가 반전된다는 것입니다. 즉, 왼쪽을 선택하면 오른쪽으로, 오른쪽을 선택하면 왼쪽으로 움직입니다.&lt;/p&gt;

&lt;p&gt;Function Approximation의 경우 모든 State가 동일하게 간주되므로 해결하기 어렵습니다. 예를 들어 모든 $s$에 대해 $\mathbf{x}(s, \text{right}) = [1, 0]^{\sf T}$, $\mathbf{x}(s, \text{left}) = [0, 1]^{\sf T}$로 정의하면, $\epsilon$-greedy를 사용한 Action-Value 방법은 크게 2가지 Policy만을 생성할 수 있습니다. 하나는 모든 단계에서 높은 확률로 오른쪽을 선택하고 $1 - \epsilon / 2$의 확률로 왼쪽을 선택하는 것이고, 다른 하나는 그 반대를 선택하는 것입니다. 만약 $\epsilon = 0.1$이라면 이 2개의 Policy는 위의 그래프처럼 시작 State에서 -44와 -82의 기대 Value를 각각 얻습니다. 만약 Stochastic Policy를 사용할 수 있다면 훨씬 더 나은 성능을 보일 수 있습니다. 가장 좋은 확률은 오른쪽을 약 0.59의 확률로 선택하는 것이며, 이 때의 Value는 약 -11.6이 됩니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Policy에 대한 매개변수화가 Action-Value 매개변수화에 비해 가질 수 있는 가장 간단한 장점은 Policy가 더 간단한 함수로 근사화할 수 있다는 것입니다. 하지만 이것은 Policy와 Action-Value Function의 복잡성에 따라 다릅니다. 문제에 따라 Action-Value Function이 더 간단하게 근사화할 수도 있기 때문입니다. 다행히 일반적인 경우에는 Policy 기반 방법이 더 빠르게 학습하고 우수한 Policy를 생성할 수 있다는 것이 증명되었습니다. (참고 : Şimşek, Algorta, Kothiyal, 2016)&lt;/p&gt;

&lt;p&gt;마지막으로, Policy를 매개변수하는 것은 때때로 원하는 Policy의 형태에 대한 사전 지식을 강화학습 시스템에 전달하는 좋은 방법이 될 수도 있습니다. 이것은 Policy 기반 학습 방법을 사용하는 가장 큰 이유 중 하나입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-policy-gradient-theorem&quot;&gt;The Policy Gradient Theorem&lt;/h2&gt;

&lt;p&gt;이전 Section에서 설명한 Policy 매개변수화의 장점 외에도 중요한 이론적인 이점이 있습니다. 지속적인 Policy 매개변수화를 사용한 Action 확률은 학습된 매개변수에 대한 함수로 쉽게 표현되는 반면, $\epsilon$-greedy에서의 Action 확률은 추정된 Action-Value의 작은 변화에도 극적으로 변할 수 있습니다. 이로 인해 Policy Gradient Method는 Action-Value 방법보다 더 강력한 수렴을 보장할 수 있습니다. 특히, 매개변수에 대한 Policy 의존성의 연속성이 식 (13.1)에서 Policy Gradient Method가 Gradient Ascent를 사용할 수 있게 보장합니다.&lt;/p&gt;

&lt;p&gt;Episodic Task와 Continuing Task에서 성능 측정 함수 $J(\boldsymbol{\theta})$를 다르게 정의하므로 어느 정도 다르게 취급해야 합니다. 하지만 일단 여기서는 두 가지 경우를 통합하고, 중요한 이론적 결과를 단일 방정식으로 설명할 수 있도록 표기법을 정리할 것입니다.&lt;/p&gt;

&lt;p&gt;이 Section에서는 Episode의 시작 State에 대한 Value를 성능 측정으로 정의하는 Episodic Task를 다루겠습니다. 모든 Episode가 무작위가 아닌 특정한 State $s_0$에서 시작한다고 가정함으로써, 일반성을 잃지 않고 표기법을 단순화할 수 있습니다. 그러면 Episodic Task의 성능을 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}} (s_0) \tag{13.4}\]

&lt;p&gt;식 (13.4)에서 $v_{\pi_{\boldsymbol{\theta}}}$는 $\boldsymbol{\theta}$로 인해 정의된 Policy인 $\pi_{\boldsymbol{\theta}}$를 따를 때의 Real Value Function입니다. 여기에서는 일단 완전성을 위해 알고리즘에서 Discounting을 포함하지만, Episodic Task에서는 Discounting이 없다고 가정하겠습니다. (즉, $\gamma = 1$)&lt;/p&gt;

&lt;p&gt;Function Approximation을 사용하면 Policy Improvement를 보장하는 방식으로 Policy 매개변수를 업데이트하는 것이 어려울 수 있습니다. 문제는 성능이 Action의 선택과, 그러한 선택을 만든 State Distribution 모두에 의존하는데, 이 두 가지 모두 Policy 매개변수에 의해 영향을 받는다는 것입니다. State가 주어졌을 때 Action에 대한 Policy 매개변수의 효과와 Reward에 대한 영향은 매개변수화에 대한 지식으로부터 간단한 방법으로 계산할 수 있습니다. 그러나 State Distribution에 대한 Policy의 영향은 Environment의 함수이기 때문에 일반적으로 알 수 없습니다. 그렇다면 Gradient가 State Distribution에서 알 수 없는 Policy 변경에 의존할 때 Policy 매개변수에 대한 성능의 Gradient를 어떻게 추정해야 할까요?&lt;/p&gt;

&lt;p&gt;다행히 Policy 매개변수에 대한 성능의 Gradient를 분석할 수 있는 &lt;span style=&quot;color:red&quot;&gt;Policy Gradient Theorem&lt;/span&gt;이라는 이론적 해결 방법이 있습니다. 이로 인해 식 (13.1)의 Gradient Ascent를 추정할 수 있으며, State Distribution에 대한 미분을 포함하지 않습니다. Episodic Task의 경우 Policy Gradient Theorem은 다음과 같이 적용할 수 있습니다.&lt;/p&gt;

\[\nabla J(\boldsymbol{\theta}) \propto \sum_s \mu (s) \sum_a q_{\pi} (s, a) \nabla \pi (a | s, \boldsymbol{\theta}) \tag{13.5}\]

&lt;p&gt;이 식에서의 Gradient는 $\boldsymbol{\theta}$의 각 원소에 대한 편미분의 열 벡터이고, $\pi$는 매개변수 벡터 $\boldsymbol{\theta}$에 해당하는 Policy를 의미합니다. 기호 $\propto$는 비례를 의미합니다. Episodic Task의 경우 비례 상수는 Episode의 평균 길이이고, Continuing Task의 경우 1이 됩니다. (즉, 이 때는 등식이 됩니다) Distribution $\mu$는 9장과 10장에서 다루었던 $\pi$에 대한 On-policy Distribution 입니다. Episodic Task에서 Policy Gradient Theorem은 다음과 같이 증명할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of the Policy Gradient Theorem for Episodic Case&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;간단한 미적분학과 식의 변형으로 Policy Gradient Theorem을 증명할 수 있습니다. 표기법을 단순하게 유지하기 위해, 모든 경우에 $\pi$가 $\boldsymbol{\theta}$에 대한 함수이고, 모든 Gradient도 $\boldsymbol{\theta}$에 대해 표현할 수 있다는 것을 가정하겠습니다. 먼저 State-Value Function의 Gradient는 다음과 같이 Action-Value Function의 관점에서 나타낼 수 있습니다.&lt;/p&gt;

\[\begin{align}
\nabla v_{\pi} (s) &amp;amp;= \nabla \left[ \sum_a \pi (a|s) q_{\pi} (s, a) \right], \quad \text{for all } s \in \mathcal{S} \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi}(s, a) + \pi (a|s) \nabla q_{\pi} (s, a) \right] \tag{product rule of calculus} \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi} (s,a) + \pi(a|s) \nabla \sum_{s&apos;, r} p (s&apos;, r|s, a) (r + v_{\pi} (s&apos;)) \right] \\ \\
&amp;amp;= \sum_a \left[ \nabla \pi (a|s) q_{\pi} (s,a) + \pi (a|s) \sum_{s&apos;} p(s&apos; |s, a) \nabla v_{\pi} (s&apos;) \right] \tag{Equation 3.4} \\ \\
&amp;amp;= \sum_a \Bigg[ \nabla \pi (a|s) q_{\pi}(s,a) + \pi(a|s) \sum_{s&apos;} p(s&apos; |s, a) \\ \\
&amp;amp; \qquad \sum_{a&apos;} [\nabla \pi (a&apos;|s&apos;) q_{\pi} (s&apos;, a&apos;) + \pi (a&apos;|s&apos;) \sum_{s&apos;&apos;} p (s&apos;&apos; | s&apos;, a&apos;) \nabla v_{\pi} (s&apos;&apos;)] \Bigg] \tag{unrolling} \\ \\
&amp;amp;= \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \text{Pr} (s \to x, k, \pi) \sum_{a} \nabla \pi (a|s) q_{\pi} (x, a)
\end{align}\]

&lt;p&gt;Unrolling 부분은 식이 너무 길어서 2줄로 나누었습니다. 위 식에서 $\text{Pr} \left( s \to x, k, \pi \right)$는 Policy $\pi$에 따라 $k$단계 후에 State $s$에서 State $x$로 전환될 확률입니다. 이것을 이용하여 $\nabla J(\boldsymbol{\theta})$의 식을 변형하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp;= \nabla v_{\pi} (s_0) \\ \\
&amp;amp;= \sum_s \left( \sum_{k=0}^{\infty} \text{Pr} (s_0 \to s, k, \pi) \right) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \\ \\
&amp;amp;= \sum_a \eta (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Equation 9.2} \\ \\
&amp;amp;= \sum_{s&apos;} \eta (s&apos;) \sum_s \frac{\eta (s)}{\sum_{s&apos;} \eta(s&apos;)} \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \\ \\
&amp;amp;= \sum_{s&apos;} \eta (s&apos;) \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Equation 9.3} \\ \\
&amp;amp;\propto \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s,a) \tag{Q.E.D.}
\end{align}\]

&lt;h2 id=&quot;reinforce-monte-carlo-policy-gradient&quot;&gt;REINFORCE: Monte Carlo Policy Gradient&lt;/h2&gt;

&lt;p&gt;이제 Policy Gradient를 사용한 첫 번째 학습 알고리즘을 만들 준비가 되었습니다. 학습의 기본 전략은 이전에 다룬 식 (13.1)의 Stochastic Gradient Ascent를 기반으로 합니다. 이것을 수행하기 위해서는 Sample Gradient의 기대값이 매개변수의 함수로써 성능 측정의 실제 Gradient에 비례하도록 Sample을 얻는 방법이 필요합니다. Sample의 Gradient는 원래의 Gradient에 비례하기만 하면 되는데, 왜냐하면 비례 상수는 Step-size Parameter인 $\alpha$와 통합하여 취급할 수도 있고, 임의로 설정할 수도 있기 때문입니다. Policy Gradient Theorem은 Gradient에 비례하는 정확한 표현을 제공하므로, 필요한 것은 기대값이 이 표현식과 같거나 가까운 Sampling 방법입니다. Policy Gradient Theorem의 오른쪽 항은 Target Policy $\pi$ 하에서 State가 얼마나 자주 발생하는지에 따라 Weight가 부여된 State의 합입니다. 즉, Policy $\pi$를 따를 경우, 이러한 비율로 State가 발생합니다. 이것을 식으로 표현하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp; \propto \sum_s \mu (s) \sum_a q_{\pi} (s, a) \nabla \pi (a | s, \boldsymbol{\theta}) \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ \sum_a q_{\pi} (S_t, a) \nabla \pi (a|S_t, \boldsymbol{\theta}) \right] \tag{13.6}
\end{align}\]

&lt;p&gt;또한 식 (13.1)의 Stochastic Gradient Ascent 알고리즘을 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha \sum_a \hat{q} (S_t, a, \mathbf{w}) \nabla \pi (a | S_t, \boldsymbol{\theta}) \tag{13.7}\]

&lt;p&gt;식 (13.7)에서 $\hat{q}$는 $q_{\pi}$에 대한 학습된 근사치입니다. 업데이트가 모든 Action을 포함하기 때문에 이 알고리즘은 &lt;span style=&quot;color:red&quot;&gt;All-actions&lt;/span&gt; 방법이라고 불리며, 이 방법 중 대표적인 것으로 시간 $t$에서 실제로 취한 Action인 $A_t$에 대한 업데이트가 포함된 고전적인 &lt;span style=&quot;color:red&quot;&gt;REINFORCE&lt;/span&gt; 알고리즘입니다. (Willams, 1992 참고)&lt;/p&gt;

&lt;p&gt;REINFORCE의 완전한 알고리즘을 유도하기 위해서는 식 (13.6)가 $S_t$를 포함한 것과 마찬가지로 $A_t$를 포함시켜야 합니다. 즉, 확률 변수의 가능한 값에 대한 합을 Policy $\pi$ 하의 기대값으로 대체하여 Sampling합니다. 식 (13.6)은 Action에 대한 적절한 합을 포함하지만, 각 항은 $\pi (a \mid S_t, \boldsymbol{\theta})$에 Weight가 부여되지 않습니다. 그래서 우리는 전체적인 식의 관계가 변경되지 않도록 $\pi (a \mid S_t, \boldsymbol{\theta})$으로 나누어 식을 수정합니다. 이 과정을 식 (13.6)에 이어서 작성하면,&lt;/p&gt;

\[\begin{align}
\nabla J(\boldsymbol{\theta}) &amp;amp; \propto \mathbb{E}_{\pi} \left[ \sum_a \pi (a|S_t, \boldsymbol{\theta}) q_{\pi} (S_t, a) \frac{\nabla \pi (a | S_t, \boldsymbol{\theta})}{\pi (a | S_t, \boldsymbol{\theta})} \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ q_{\pi} (S_t, A_t) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \right] \qquad \text{(replacing } a \text{ by the sample } A_t \sim \pi \text{)} \\ \\
&amp;amp;=\mathbb{E}_{\pi} \left[ G_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \right] \qquad \text{(because } \mathbb{E}_{\pi} [G_t | S_t, A_t] = q_{\pi} (S_t, A_t) \text{)}
\end{align}\]

&lt;p&gt;위 식에서 $G_t$는 일반적인 Return입니다. 마지막 식의 결과는 기대값의 Gradient에 비례하는 각 시간 단계에서의 Sampling할 수 있는 양입니다. 이 Sample을 사용하여 식 (13.1)의 Stochastic Gradient Ascent 알고리즘을 인스턴스화하면 REINFORCE 업데이트 식이 유도됩니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha G_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \tag{13.8}\]

&lt;p&gt;식 (13.8)과 같은 업데이트 식은 직관적입니다. 각 시간별 증가분은 Return $G_t$와 Action을 취할 확률의 Gradient를 그 확률로 나눈 값에 비례합니다. 그 벡터의 방향은 미래에 State $S_t$를 방문할 때 Action $A_t$를 반복할 확률을 가장 많이 증가시키는 매개변수 공간의 방향입니다. 즉, 업데이트는 Return $G_t$에 비례하고, Action 확률에 반비례하는 방향으로 매개변수 벡터를 증가시킵니다. Return에 비례하는 것은 가장 높은 Reward를 얻을 수 있는 Action을 선호하는 방향으로 매개변수를 움직인다는 의미가 있습니다. Action 확률에 비례하는 것은 자주 선택되는 Action을 선호한다는 뜻입니다. 자주 선택되는 Action은 가장 높은 Reward를 내지 못하더라도 유리한 선택이 될 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;REINFORCE는 Episode가 끝날 때까지 미래의 모든 Reward를 포함하는 시간 $t$부터 완전한 Return을 사용합니다. 이런 의미에서 REINFORCE는 Monte Carlo 알고리즘이라고 볼 수 있으며, Episode가 끝난 후 모든 업데이트가 수행되는 Episode Task에만 잘 정의됩니다. REINFORCE 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Pseudocode에서 마지막 줄은 REINFORCE 업데이트인 식 (13.8)과 다른 점이 있습니다. 먼저 $\nabla \ln x = \frac{\nabla x}{x}$라는 것을 이용하여, $\frac{\nabla \pi (A_t \mid S_t, \boldsymbol{\theta}_t)}{\pi (A_t \mid S_t, \boldsymbol{\theta}_t)}$를  $\nabla \ln \pi (A_t \mid S_t, \boldsymbol{\theta}_t)$로 바꾸었습니다. 이 벡터의 이름은 문헌에 따라 다르지만, 여기서는 간단하게 &lt;span style=&quot;color:red&quot;&gt;Eligibility Vector&lt;/span&gt;라고 명칭하겠습니다. 이 부분이 알고리즘에서 Policy 매개변수화가 나타나는 유일한 부분입니다.&lt;/p&gt;

&lt;p&gt;마지막 줄에서의 또 다른 차이점으로는 $\gamma^t$의 유무입니다. 식 (13.8)을 논할 때는 Discounting을 가정하지 않았기 때문에(즉, $\gamma = 1$) 이것을 생략하였지만, 위의 Pseudocode는 일반적인 경우에 대한 알고리즘이기 때문에 포함되었습니다.&lt;/p&gt;

&lt;p&gt;다음의 그래프는 Example 13.1에서 REINFORCE의 성능을 나타내고 있습니다. $\alpha$에 값에 따라 성능의 차이가 크기 때문에, 좋은 $\alpha$를 정하는 것이 중요하다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;REINFORCE는 Stochastic Gradient Method로 인해 좋은 이론적 수렴 특성을 가지고 있습니다. Episode에 대한 예상 업데이트는 성능에 대한 Gradient와 같은 방향입니다. 이것은 충분히 작은 $\alpha$에 대해 예상되는 성능의 개선과, $\alpha$가 감소하는 일반적인 확률적 근사 조건 하에 Local Optimum에 수렴하는 것을 보장합니다. 그러나 Monte Carlo Method인 REINFORCE는 Variance가 커서 학습이 느려질 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;reinforce-with-baseline&quot;&gt;REINFORCE with Baseline&lt;/h2&gt;

&lt;p&gt;Policy Gradient Theorem을 나타내는 식 (13.5)는 상대적인 Action-Value를 비교하는데 사용되는 임의의 baseline $b(s)$를 포함하여 일반화할 수 있습니다.&lt;/p&gt;

\[\nabla J(\boldsymbol{\theta}) \propto \sum_s \mu (s) \sum_a \left( q_{\pi} (s, a) - b(s) \right) \nabla (a | s, \boldsymbol{\theta}) \tag{13.10}\]

&lt;p&gt;Baseline은 $a$에 의해 변하지 않는 한, 확률 변수를 포함한 어떤 함수든 될 수 있습니다. 다음과 같이 Baseline은 Gradient를 취했을 때 0이 되기 때문입니다.&lt;/p&gt;

\[\sum_a b(s) \nabla \pi (a|s, \boldsymbol{\theta}) = b(s) \nabla \sum_a \pi (a|s, \boldsymbol{\theta}) = b(s) \nabla 1 = 0\]

&lt;p&gt;식 (13.10)과 같이 Baseline이 있는 Policy Gradient Theorem을 사용하여 이전 Section에서와 유사하게 업데이트 규칙을 유도할 수 있습니다. 다음은 Baseline을 포함한 새로운 버전의 REINFORCE 업데이트 식입니다.&lt;/p&gt;

\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_t + \alpha \left( G_t - b(S_t) \right) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta})}{\pi (A_t | S_t, \boldsymbol{\theta})} \tag{13.11}\]

&lt;p&gt;Baseline은 0으로 균일할 수 있으므로 위의 식 (13.11)은 REINFORCE의 엄격한 일반화입니다. 일반적으로 Baseline은 업데이트의 Expected Value를 변경하지 않지만, Variance에 큰 영향을 줄 수 있습니다. 예를 들어, Section 2.8에서와 유사한 Baseline은 Gradient Bandit Algorithm의 Variance를 크게 줄일 수 있습니다. Variance가 줄어든다는 것은 그만큼 학습 속도가 빨라진다는 의미입니다.&lt;/p&gt;

&lt;p&gt;Bandit Algorithm에서 Baseline은 숫자(=평균 Reward)에 불과했지만, MDP의 경우 Baseline은 State에 따라 달라져야 합니다. 어떤 State에서는 모든 Action이 높은 Value를 가질 수 있기 때문에, 더 높은 Value와 덜 높은 Value의 Action을 구별하기 위해 높은 Baseline이 필요합니다. 물론 반대로 모든 Action이 낮은 Action에서는 낮은 Baseline을 통해 Action의 상대적 Value를 구별해야 합니다.&lt;/p&gt;

&lt;p&gt;Baseline에 대한 자연스러운 선택 중 하나는 State-Value의 추정값인 $\hat{v} (S_t, \mathbf{w})$ 입니다. 여기서 $\mathbf{w} \in \mathbb{R}^d$는 이전 장에서 제시된 방법 중 하나로 학습된 Weight Vector입니다. REINFORCE는 Policy 매개변수 $\boldsymbol{\theta}$를 학습하기 위한 Monte Carlo Method이기 때문에 Monte Carlo Method를 사용하여 State-Value의 Weight인 $\mathbf{w}$를 학습할 수도 있습니다. 학습된 State-Value Function을 Baseline으로 사용하는 REINFORCE의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 알고리즘에는 두 개의 Step-size Parameter인 $\alpha^{\boldsymbol{\theta}}$와 $\alpha^{\mathbf{w}}$가 있습니다. 이 중 $\alpha^{\mathbf{w}}$를 선택하는 것은 비교적 쉽습니다. 예를 들어, Section 9.6에서와 같이 Linear의 경우 $\alpha^{\mathbf{w}} = 0.1 / \mathbb{E} [ \lVert \nabla \hat{v} (S_t, \mathbf{w}) \rVert^2_{\mu}]$라는 경험적인 법칙이 있었습니다. 하지만 $\alpha^{\boldsymbol{\theta}}$는 Reward의 범위와 Policy 매개변수화에 따라 결정해야하기 때문에 명확한 방법이 존재하지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 예제 13.1에서 Baseline이 있는 경우와 없는 경우 REINFORCE의 성능을 비교합니다. 이 비교에 사용된 State-Value Function의 추정값은 $\hat{v} (s, \mathbf{w}) = w$입니다. 즉, $\mathbf{w}$는 단일 요소 $w$로 구성되어 있습니다. 그래프를 보시면, 수렴되는 값은 두 방법이 차이가 없지만, Baseline을 사용하는 방법이 더 빠르게 수렴함을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;actorcritic-methods&quot;&gt;Actor–Critic Methods&lt;/h2&gt;

&lt;p&gt;Baseline이 있는 REINFORCE에서 학습된 State-Value Function은 각 State Transition에서 첫 번째 State의 Value를 추정합니다. 이 추정값은 후속 Return에 대한 Baseline을 설정하지만, Action이 Tranistion되기 전에 이루어지므로 해당 Action을 평가하는데 사용할 수 없습니다.&lt;/p&gt;

&lt;p&gt;반면, Actor-Critic Method에서는 State-Value Function이 두 번째 State에도 적용됩니다. 두 번째 State의 Estimated Value는 Discount되어 Reward에 추가될 때, 1-step Return $G_{t:t+1}$을 포함하며, 이것은 실제 Return에 대한 올바른 추정이므로 Action을 평가할 수 있습니다. 이전에 배운 TD Learning에서도 보았듯이, 1-step Return은 Bias를 감안하더라도 Variance 및 계산 적합성 측면에서 실제 Return보다 우수합니다. 또한 7장 및 12장에서와 같이 $n$-step Return 및 Eligibility Trace를 사용하여 Bias의 범위를 유연하게 조정할 수도 있습니다. 이와 같이 State-Value Function을 사용하여 Action을 평가할 때, 이를 &lt;span style=&quot;color:red&quot;&gt;Critic&lt;/span&gt;이라고 하며, 전체 Policy Gradient Method를 &lt;span style=&quot;color:red&quot;&gt;Actor-Critic Method&lt;/span&gt;라고 합니다. Gradient 추정값의 Bias는 Bootstrapping으로 인한 것이 아니기 때문에 Critic이 Monte Carlo Method로 학습하더라도 Actor는 Bias될 것입니다.&lt;/p&gt;

&lt;p&gt;먼저 1-step Actor-Critic Method를 소개하겠습니다. 1-step 방법의 장점은 완전한 On-line 및 Incremental 방식임에도 Eligibility Trace의 복잡함을 피할 수 있다는 것입니다. 1-step Actor-Critic Method는 REINFORCE 방법인 식 (13.11)의 전체 Return을 다음과 같이 1-step Return으로 대체합니다. (단, 이때 Baseline은 학습된 State-Value Function을 사용합니다)&lt;/p&gt;

\[\begin{align}
\boldsymbol{\theta}_{t+1} &amp;amp; \doteq \boldsymbol{\theta}_t + \alpha \Big( G_{t:t+1} - \hat{v} (S_t, \mathbf{w}) \Big) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.12} \\ \\
&amp;amp;= \boldsymbol{\theta}_t + \alpha \Big( R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w}) \Big) \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.13} \\ \\
&amp;amp;= \boldsymbol{\theta}_t + \alpha \delta_t \frac{\nabla \pi (A_t | S_t, \boldsymbol{\theta}_t)}{\pi (A_t | S_t, \boldsymbol{\theta}_t)} \tag{13.14}
\end{align}\]

&lt;p&gt;이 업데이트와 짝을 이루는 State-Value Function의 학습 방법은 Semi-gradient TD(0)입니다. 이것을 반영한 전체 알고리즘의 Pseudocode는 다음과 같습니다. 이 알고리즘은 State, Action, 그리고 Reward가 발생하는 즉시 처리되는 완전한 On-line Incremental 알고리즘이라는 점에 유의하시기 바랍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 1-step 방법의 Forward View에 대한 일반화 및 $n$-step으로의 일반화는 간단합니다. 이 때, 식 (13.12)의 1-step Return은 각각 $G_{t:t+n}$ 또는 $G^{\lambda}_t$로 대체됩니다. $\lambda$-return 알고리즘의 Backward View도 12장의 패턴을 따라 Actor와 Critic에 대해 별도의 Eligibility Trace를 사용하여 처리합니다. 이것을 반영한 전체 알고리즘의 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;policy-gradient-for-continuing-problems&quot;&gt;Policy Gradient for Continuing Problems&lt;/h2&gt;

&lt;p&gt;Section 10.3에서와 같이, Continuing Task의 성능은 다음과 같이 시간 단계에서 Average Reward로 정의해야 합니다.&lt;/p&gt;

\[\begin{align}
J(\boldsymbol{\theta}) &amp;amp; \doteq r(\pi) \doteq \lim_{h \to \infty} \frac{1}{h} \sum_{t=1}^h \mathbb{E} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \tag{13.15} \\ \\
&amp;amp;= \lim_{t \to \infty} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \\ \\
&amp;amp;\sum_s \mu (s) \sum_a \pi (a | s) \sum_{s&apos;, r} p (s&apos;, r | s, a) r
\end{align}\]

&lt;p&gt;위 식에서 $\mu$는 Policy $\pi$ 하에서의 Steady-state Distribution, $\mu (s) \doteq \underset{t \to \infty}{\operatorname{lim}} \text{Pr} \{ S_t = s \mid A_{0:t} \sim \pi \}$는 State $S_0$와 독립적으로 존재한다고 가정합니다. 이것은 $\pi$에 따라 Action을 선택하는 경우, 동일한 Distribution를 유지하는 특별한 Distribution임을 유의하시기 바랍니다.&lt;/p&gt;

\[\sum_s \mu (s) \sum_a \pi (a|s, \boldsymbol{\theta}) p(s&apos;|s,a) = \mu (s&apos;), \quad \text{for all } s&apos; \in \mathcal{S} \tag{13.16}\]

&lt;p&gt;(Backward View) Continuing Task에서 Actor-Critic 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 Continuing Task에서 Value Function은 각각 $v_{\pi} (s) \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right]$ (State-Value)와 $q_{\pi} (s, a) \doteq \mathbb{E} \left[ G_t \mid S_t = s, A_t = a \right]$ (Action-Value)로 정의됩니다. 이 때, Return $G_t$는 다음과 같습니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \cdots \tag{13.17}\]

&lt;p&gt;이렇게 정의가 바꾸더라도 식 (13.5) Policy Gradient Theorem은 유효합니다. Continuing Task에서 Policy Gradient Theorem에 대한 증명은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of the Policy Gradient Theorem for Continuing Case&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Episodic Task에서와 마찬가지로 먼저 모든 경우에 $\pi$가 $\boldsymbol{\theta}$에 대한 함수이고, Gradient가 $\boldsymbol{\theta}$로 표현될 수 있다는 가정이 필요합니다. 식 (13.15)에 의하여 Continuing Task에서 $J(\boldsymbol{\theta}) = r(\pi)$이고, $v_{\pi}$와 $q_{\pi}$는 식 (13.17)의 Return으로 구성되어 있습니다. 따라서 State-Value Function의 Gradient는 임의의 State $s \in \mathcal{S}$에 대해 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\nabla v_{\pi} (s) &amp;amp;= \nabla \left[ \sum_a \pi (a | s) q_{\pi} (s, a) \right], \quad \text{for all } s \in \mathcal{S} \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \nabla q_{\pi} (s, a) \Big] \tag{product rule of calculus} \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \nabla \sum_{s&apos; ,r} p(s&apos;, r|s, a) \big(r - r(\boldsymbol{\theta}) + v_{\pi} (s&apos;) \big) \Big] \\ \\
&amp;amp;= \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \big[ - \nabla r(\boldsymbol{\theta}) + \sum_{s&apos;} p (s&apos; |s, a) \nabla v_{\pi} (s&apos;) \big] \Big] \end{align}\]

&lt;p&gt;이 식을 정리하면 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\nabla r(\boldsymbol{\theta}) = \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \sum_{s&apos;} p(s&apos; |s, a) \nabla v_{\pi} (s&apos;) \Big] - \nabla v_{\pi} (s)\]

&lt;p&gt;위 식의 좌변은 $J(\boldsymbol{\theta})$로 표기할 수 있으며, State $s$에 의존하지 않습니다. 따라서 우변 역시 State $s$에 의존하지 않으므로 식의 변경 없이 Weight $\mu (s)$를 붙여 합산할 수 있습니다. ($\because \sum_s \mu (s) = 1$)&lt;/p&gt;

\[\begin{align}
J(\boldsymbol{\theta}) &amp;amp;= \sum_s \mu (s) \Bigg( \sum_a \Big[ \nabla \pi (a|s) q_{\pi} (s, a) + \pi (a|s) \sum_{s&apos;} p(s&apos; | s, a) \nabla v_{\pi} (s&apos;) \Big] - \nabla v_{\pi} (s) \Bigg) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_s \mu (s) \sum_a \pi (a|s) \sum_{s&apos;} p(s&apos; | s, a) \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_{s&apos;} \underbrace{\sum_s \mu (s) \sum_a \pi (a|s) p (s&apos; | s, a)}_{\mu (s&apos;) \text{( 13.16)}} \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) + \sum_{s&apos;} \mu (s&apos;) \nabla v_{\pi} (s&apos;) - \sum_s \mu (s) \nabla v_{\pi} (s) \\ \\
&amp;amp;= \sum_s \mu (s) \sum_a \nabla \pi (a|s) q_{\pi} (s, a) \tag{Q.E.D.}
\end{align}\]

&lt;h2 id=&quot;policy-parameterization-for-continuous-actions&quot;&gt;Policy Parameterization for Continuous Actions&lt;/h2&gt;

&lt;p&gt;Policy에 기반한 방법은 Action Space가 매우 큰 경우(=Action의 수가 매우 많은 경우)나 Action의 수가 무한한 연속적인 Space에서도 효과적인 해법을 제공합니다. 이런 경우, Action 각각에 대해 학습된 확률을 계산하는 대신 확률 분포의 통계를 학습합니다. 예를 들어, Action 집합은 Normal Distribution(=Gaussian Distribution)에서 선택된 Action을 포함하는 실수 집합일 수 있습니다.&lt;/p&gt;

&lt;p&gt;아시다시피, Normal Distribution에 대한 Probability Density Function은 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[p(x) \doteq \frac{1}{\sigma \sqrt{2 \pi}} \exp \left( - \frac{(x - \mu)^2}{2 \sigma^2} \right) \tag{13.18}\]

&lt;p&gt;Probability Density Function에서 $\mu$는 Normal Distribution의 평균이고, $\sigma$는 표준편차입니다. 이 때 $p(x)$는 $x$가 일어날 확률이 아니라 $x$에서의 Probability Density입니다. 즉, 1보다 클 수도 있습니다. 합이 1이 되어야 하는 부분은 $p(x)$와 $x$축 사이의 총 넓이입니다. 일반적으로 $x$의 값을 범위로 정하여 적분을 취하면 해당 범위 내에 $x$가 존재할 확률을 구할 수 있습니다. Probability Density Function에서 평균과 표준편차의 값에 따라 나타낸 그래프는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/13. Policy Gradient Methods/RL 13-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Probability Density Function을 기반으로 Policy 매개변수화를 생성하기 위해서는, Policy를 State에 따라 달라지는 매개변수 함수 근사가 제공하는 평균 및 표준편차와 함께 실수 값 Scalar Action을 Normal Probability Density로 정의하면 됩니다. 즉,&lt;/p&gt;

\[\pi (a|s, \boldsymbol{\theta}) \doteq \frac{1}{\sigma(s, \boldsymbol{\theta}) \sqrt{2 \pi}} \exp \left( - \frac{(a - \mu (s, \boldsymbol{\theta}))^2}{2 \sigma (s, \boldsymbol{\theta})^2} \right) \tag{13.19}\]

&lt;p&gt;식 (13.19)에서 $\mu : \mathcal{S} \times \mathbb{R}^{d’} \to \mathbb{R}$과 $\sigma : \mathcal{S} \times \mathbb{R}^{d’} \to \mathbb{R}^+$는 매개변수화된 함수 근사입니다.&lt;/p&gt;

&lt;p&gt;위 식을 완성하기 위해서는 두 개의 함수 근사에 대한 형태를 정의해야 합니다. 이를 위해 Policy의 매개변수 벡터를 $\boldsymbol{\theta} = [\boldsymbol{\theta}_{\mu}, \boldsymbol{\theta}_{\sigma}]^{\sf T}$와 같이 두 부분으로 나눕니다. 이 두 부분은 각각 평균과 표준편차에 대한 근사를 의미합니다. 이 중 평균은 Linear Function으로 근사할 수 있습니다. 표준편차의 경우에는 항상 양수여야하는 조건이 있기 때문에 Linear Function의 지수 형태로 근사할 수 있습니다. 이를 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\mu (s, \boldsymbol{\theta}) \doteq \boldsymbol{\theta}_{\mu}^{\sf T} \mathbf{x}_{\mu} (s) \quad \text{and} \quad \sigma (s, \boldsymbol{\theta}) \doteq \exp \left( \boldsymbol{\theta}_{\sigma}^{\sf T} \mathbf{x}_{\sigma} (s) \right) \tag{13.20}\]

&lt;p&gt;식 (13.20)에서 $\mathbf{x}_{\mu} (s)$와 $\mathbf{x}_{\sigma} (s)$는 Section 9.5에서 설명된 방법 중 하나로 구성되는 Feature Vector입니다. 이러한 정의를 사용하면 이번 장에서 배웠던 알고리즘 중 하나를 적용하여 실수값으로 정의된 Action을 선택하는 것을 학습할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장을 배우기 전까지는 Action-Value를 학습한 다음, Action을 선택하는데 사용하는 방법(Action-Value Method)에 초점을 맞췄습니다. 반면에 이번 장에서는 Action-Value의 추정값을 참조하지 않고 Action을 선택할 수 있도록 매개변수화된 Policy를 학습하는 방법을 고려하였습니다. 특히, Policy 매개변수에 대한 성능의 Gradient를 추정하는 방법인 &lt;strong&gt;Policy Gradient Method&lt;/strong&gt;를 고려하였습니다.&lt;/p&gt;

&lt;p&gt;Policy 매개변수를 학습하는 방법에는 많은 이점이 있습니다. 먼저, Action을 취할 특정한 확률을 학습할 수 있습니다. 이 방법들은 적절한 수준의 Exploration을 수행하고 Deterministic Policy에 점근적으로 접근할 수 있습니다. 또한 연속적인 Action Space도 다룰 수 있습니다. 이것은 Policy 기반 방법에서는 쉽지만 $\epsilon$-greedy나 일반적인 Action-Value 방법으로는 어렵거나, 불가능합니다. 또한 일부 문제에서는 Value Function을 매개변수로 표현하는 것보다, Policy를 매개변수로 표현하는 것이 더 간단합니다. 이러한 문제들은 매개변수화된 Policy 방법에 더 적합합니다.&lt;/p&gt;

&lt;p&gt;매개변수화된 Policy 방법은 &lt;strong&gt;Policy Gradient Theorem&lt;/strong&gt;을 기반으로 한 중요한 이론적 장점이 있습니다. 이 정리는 State Distribution에 대한 정보를 포함하지 않는 Policy 매개변수에 의해 성능이 어떻게 영향을 받는지에 대한 정확한 공식을 제공합니다. 이 정리는 모든 Policy Gradient Method에 대한 이론적인 토대를 마련합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;REINFORCE&lt;/strong&gt;는 Policy Gradient Theorem을 직접적으로 따르는 방법입니다. 또한 State-Value Function을 Baseline으로 추가하면 Bias를 피하면서 REINFORCE의 Variance를 줄일 수 있습니다. State-Value Function이 Policy의 Action 선택을 평가하거나 비판하는데 사용되는 경우, Value Function을 &lt;strong&gt;Critic&lt;/strong&gt;이라고 하고 Policy를 &lt;strong&gt;Actor&lt;/strong&gt;라고 합니다. 이 방법은 &lt;strong&gt;Actor-Critic Method&lt;/strong&gt;이라고 합니다. Critic은 Actor의 Gradient 추정에 Bias를 추가하지만, Bootstrapping TD 방법이 종종 Monte Carlo Method보다 Variance가 낮다는 측면에서 우수하기 때문에 때때로 더 우수합니다.&lt;/p&gt;

&lt;p&gt;정리하자면, Policy Gradient Method는 Action-Value 방법과는 상당히 다른 장점과 단점을 지닙니다. 현재 이 분야는 아직 활발하게 연구되는 주제이기 때문에, 앞으로 더 흥미로운 결과가 나오는 것을 기대하고 있습니다.&lt;/p&gt;

&lt;p&gt;이로써 길고 길었던 강화학습 포스팅이 얼추 마무리가 되었습니다. 당분간 새로운 이론적인 포스트를 작성하기 보다는, 쉬면서 작성했던 포스트를 검토하여 부족했던 부분을 수정하거나 추가하도록 하겠습니다. 지금까지 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장은 드디어 마지막 장인 Policy Gradient입니다. 이번 장에서는 지금까지 이 교재에서 다룬 방법들과는 다르게, Policy 자체를 매개변수화하는 방법을 알아보겠습니다. 지금까지의 방법들은 Estimated Action-Value를 기반으로 Action을 선택했기 때문에 Action-Value를 추정하는 것이 중요했습니다. 하지만 이번 장에서 배울 새로운 방법인 Policy Gradient는 Action을 선택하는 데 Value Function을 사용하지 않습니다. 이번 장에서 사용할 새로운 표기는 Policy에 대한 매개변수 벡터인 $\boldsymbol{\theta} \in \mathbb{R}^{d’}$입니다. 따라서 Policy는 이제 매개변수 $\boldsymbol{\theta}$를 포함하여 $\pi (a \mid s, \boldsymbol{\theta}) = Pr \{ A_t = a \mid S_t = s, \boldsymbol{\theta}_t = \boldsymbol{\theta} \}$로 표현합니다. 이것은 시간 $t$에서 State가 $s$이고 매개변수가 $\boldsymbol{\theta}$일 때 Action $a$를 선택할 확률로 정의됩니다. 만약 학습 알고리즘 안에서 Value Function에 대한 추정을 포함하는 경우, 이전과 마찬가지로 여전히 Weight Vector $\mathbf{w} \in \mathbb{R}^d$를 포함하여 $\hat{v}(s, \mathbf{w})$로 표현합니다.</summary></entry><entry><title type="html">Eligibility Trace</title><link href="http://localhost:4000/studies/eligibility-traces/" rel="alternate" type="text/html" title="Eligibility Trace" /><published>2022-09-22T00:00:00+09:00</published><updated>2022-09-22T00:00:00+09:00</updated><id>http://localhost:4000/studies/eligibility-traces</id><content type="html" xml:base="http://localhost:4000/studies/eligibility-traces/">&lt;p&gt;이번 장에서 새로 배우는 &lt;span style=&quot;color:red&quot;&gt;Eligibility Trace&lt;/span&gt;는 강화학습의 기본 메커니즘 중 하나입니다. 예를 들어, TD($\lambda$)에서 $\lambda$는 Eligibility Trace를 사용한다는 것을 의미합니다. Q-learning과 Sarsa를 포함한 대부분의 TD 방법은 Eligibility Trace와 결합하여 보다 효율적으로 학습할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace는 TD와 Monte Carlo Method를 통합하여 일반화하는 방법입니다. TD 방법을 Eligibility Trace를 사용하여 일반화하면 $\lambda = 1$일 때 Monte Carlo Method처럼 동작하며, $\lambda = 0$일 때 1-step TD로 동작합니다. 이로 인해 Eligibility Trace는 온라인으로 Monte Carlo Method을 구현할 수 있고, Episode가 없는 Continuing Problem에 대한 학습 방법을 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace의 메커니즘을 간단히 설명하자면, Eligibility Trace의 Short-term Memory Vector $\mathbf{z}_t \in \mathbb{R}^d$는 Long-term Weight Vector $\mathbf{w}_t \in \mathbb{R}^d$와 평행합니다. $\mathbf{w}_t$를 통해 함수를 추정할 때, $\mathbf{z}_t$의 구성 요소와 충돌한 후, $\mathbf{z}_t$는 사라지기 시작합니다. 이 &lt;strong&gt;Trace&lt;/strong&gt;가 0으로 감소하기 전에 0이 아닌 TD Error가 발생하면, $\mathbf{w}_t$의 해당 구성 요소에서 학습이 일어납니다. 이 때 $\lambda \in \left[ 0, 1 \right]$는 Trace가 얼마나 빨리 0으로 감소하는 지 나타내는 Trace-decay Parameter입니다.&lt;/p&gt;

&lt;p&gt;그런데 우리는 이미 7장에서 Monte Carlo와 1-step TD를 조율한 $n$-step TD를 배웠습니다. 하지만 Eligibility Trace는 $n$-step TD에 비해 계산적으로 이점이 있습니다. $n$-step TD는 마지막 $n$개의 Feature Vector를 저장했지만, Eligibility Trace는 1개의 Trace Vector만 필요합니다. 또한 $n$-step TD에서의 학습은 Episode가 끝나기 전까지 지연되는 방식이지만, Eligibility Trace는 지속적이고 균일하게 학습이 일어납니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 통해 학습 알고리즘은 계산상의 이점을 위해 때때로 다른 방법으로 구현될 수도 있다는 것을 보여줍니다. 기존 방법을 예로 들자면, Monte Carlo Method와 $n$-step TD는 Episode의 마지막부터 Episode의 처음까지 학습하거나, $n$-step만큼 학습하였습니다. 이것을 &lt;strong&gt;Forward View&lt;/strong&gt;라고 하는데, Forward View는 막상 알고리즘을 수행할 때 바로 사용할 수 없는 미래의 요소에 의존하기 때문에 구현하는 것이 상당히 복잡합니다. 그러나 Eligibility Trace는 알고리즘이 수행하는 순서와 거의 동일한 Update를 구현할 수 있습니다. 이것을 Backward View라고 합니다. 이번 장에서 이것에 대해 조금 더 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;이번 장 역시 이전 장들과 마찬가지로, State-Value의 Prediction에 대한 개념을 먼저 다룬 다음에, Action-Value 및 Control 문제로 확장합니다. 또한 마찬가지로 On-policy 학습을 먼저, Off-policy 학습을 나중에 다룰 예정입니다. Function Approximation은 Linear Function Approximation에 중점을 둘 것이며, Tabular 방법과 State Aggregation 경우에도 적용할 수 있다는 것을 보일 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;the-lambda-return&quot;&gt;The $\lambda$-Return&lt;/h2&gt;

&lt;p&gt;먼저 7장에서 배운 $n$-step Return을 복습해봅시다. 식 (7.1)에서 $n$-step Return은 처음 $n$개의 Discounted Reward와 방문한 State의 Estimated Value의 합으로 정의했습니다. 이 식을 매개변수를 사용한 Function Approximation 식으로 수정하면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{v} \left( S_{t+n}, \mathbf{w}_{t+n-1} \right), \quad 0 \le t \le T-n \tag{12.1}\]

&lt;p&gt;식 (12.1)에서 $\hat{v} \left( s, \mathbf{w} \right)$는 Weight Vector $\mathbf{w}$가 주어졌을 때 State $s$의 근사값이고, $T$는 Episode가 종료되는 시간입니다.&lt;/p&gt;

&lt;p&gt;또한 이러한 Update는 $n$-step Return 뿐만 아니라 다른 모든 $n$에 대한 $n$-step Return의 평균에 대해서도 유효합니다. 예를 들자면, 2-step Return의 절반과 4-step Return의 절반의 합으로 구성된 $\frac{1}{2} G_{t:t+2} + \frac{1}{2} G_{t:t+4}$와 같은 식에 대해서도 Update를 수행할 수 있다는 것입니다. 이렇게 각 Return의 Weight가 양수이면서 합이 1인 조건 하에서는 모든 $n$-step Return이 이런 방식으로 평균을 낼 수 있습니다. 심지어 항의 개수가 무한해도 말입니다. 이러한 평균화 기법을 이용하면 새로운 알고리즘을 개발할 수 있습니다. 예를 들어, TD나 Monte Carlo Method를 연결하기 위해 1-step 및 무한 단계 Return을 평균화하는 식으로 말입니다. 이와 같이 간단하게 구성 요소를 Update 하는 평균화 기법을 &lt;span style=&quot;color:red&quot;&gt;Compound Update&lt;/span&gt; 라고 합니다. 이에 대한 Backup Diagram은 Update 식에 따라 달라지는데, 방금 다룬 2-step Return의 절반과 4-step Return의 절반을 합친 식에 대한 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Backup Diagram에서 볼 수 있듯이, Compound Update의 Update를 수행하기 위해서는 가장 긴 구성 요소의 Update가 완료되어야 수행할 수 있습니다. 예를 들어, 위의 Backup Diagram에서 가장 긴 구성 요소는 4-step Return이므로, 시간 $t$에서의 추정치는 시간 $t+4$에 도달해야만 추정이 가능합니다. 이러한 문제로 인해 Update에 지연이 발생할 수 있으므로, 일반적으로는 가장 긴 구성 요소의 길이를 제한하는 방식으로 해결합니다.&lt;/p&gt;

&lt;p&gt;$n$-step update를 평균화하는 방법 중 대표적으로 &lt;span style=&quot;color:red&quot;&gt;TD($\lambda$)&lt;/span&gt;가 있습니다. 이 방법은 모든 $n$-step update에 대해 $\lambda^{n-1}$의 Weight를 부여한 평균화 방법입니다. 그리고 이 Weight의 합을 1로 만들기 위해 맨 앞에 $1 - \lambda$를 곱해줍니다. 이것을 식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+n} \tag{12.2}\]

&lt;p&gt;식 (12.2)의 Update 식을 &lt;span style=&quot;color:red&quot;&gt;$\lambda$-Return&lt;/span&gt; 이라고 합니다. $\lambda$-Return의 Backup Diagram은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\lambda$-Return에서는 각 항의 계수가 다르기 때문에 Weight 또한 항 마다 다릅니다. 예를 들어, 1-step Return의 계수는 $(1 - \lambda)$이지만, 2-step Return은 $(1 - \lambda) \lambda$의 계수를 갖습니다. 이렇게 시간 $t$를 기준으로 멀어질수록 $\lambda$를 곱하기 때문에 Weight가 낮아집니다. 이것을 그림으로 표현하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 보시다피시 마지막 항의 계수만 다른 항과 표현 방식이 다르기 때문에, 식 (12.2)를 다음과 같이 마지막 항만 분리하여 표현할 수도 있습니다.&lt;/p&gt;

\[G_t^{\lambda} = (1 - \lambda) \sum_{n=1}^{T-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{T-t-1}G_t \tag{12.3}\]

&lt;p&gt;식 (12.3)에서 $\lambda = 1$인 경우를 따져봅시다. $(1 - \lambda)$ 항이 0이 되므로 2번째 항만 살아남아 Monte Carlo Return이 됩니다. 반대로 $\lambda = 0$인 경우라면 2번째 항이 사라짐은 물론, 1번째 항의 첫 번째 Return을 제외하고 모두 0이 되기 때문에 1-step Return이 됩니다. 따라서 $\lambda = 0$인 경우라면 $\lambda$-Return은 1-step TD 방법이 된다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이제 $\lambda$-Return을 기반으로 만든 첫 번째 학습 알고리즘인 &lt;span style=&quot;color:red&quot;&gt;Off-line $\lambda$-Return Algorithm&lt;/span&gt;을 소개하겠습니다. &lt;strong&gt;Off-line&lt;/strong&gt; Algorithm이므로 Episode가 수행되는 동안에는 Weight Vector가 변경되지 않습니다. 그 후 Episode가 끝날 때 $\lambda$-Return을 Target으로 사용하여 일반적인 Semi-gradient Rule에 따라 Off-line Update의 전체 과정을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G_t^{\lambda} - \hat{v} (S_t, \mathbf{w}) \right] \nabla \hat{v} (S_t, \mathbf{w}), \quad t = 0, \ldots, T - 1 \tag{12.4}\]

&lt;p&gt;$\lambda$-Return은 7장에서 배웠던 $n$-step bootstrapping 방법과 다른 방법으로 Monte Carlo와 1-step TD 사이를 조절할 수 있는 대안을 제시합니다. 이에 대한 예시로 교재에서는 Random Walk Example을 제시하는데, 이 예제를 제가 7장에서 소개하지 않았습니다. 일단 여기에서 비교 내용만 설명하고, 추후 7장에 이 예제를 추가하겠습니다.&lt;/p&gt;

&lt;p&gt;아래 그림은 19개의 State를 가진 Random Walk Example에서의 $\lambda$-Return Algorithm과 $n$-step TD 방법 비교 그래프입니다. 두 방법 모두 처음 10개의 Episode에 대한 평균을 나타내며 그래프의 세로축은 Root Mean Square Error를 의미하기 때문에 낮을 수록 좋습니다. 그래프를 보면 두 방법 모두 성능이 비슷하며, 중간 정도의 $\lambda$와 $n$일 때 최적의 성능을 보임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 지금까지 취한 접근 방식은 학습 알고리즘에 대한 Theoretical View, 혹은 Forward View라고 부를 수 있습니다. 방문하는 각 State에 대해 향후 얻을 수 있는 모든 Reward에 대한 기대값과 이를 결합하는 최선의 방법을 결정하기 때문입니다. 아래 그림과 같이 Update를 결정하기 위해 각 State에서 기다리면서 State의 흐름을 타고 있다고 볼 수 있습니다. 한 State를 Update 한 후, 다음 State로 이동한 후에는 이 작업을 반복할 필요가 없습니다. 반면에 미래 State는 이전의 유리한 지점에서 한 번씩 반복적으로 처리됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tdlambda&quot;&gt;TD($\lambda$)&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;TD($\lambda$)&lt;/span&gt;는 강화학습에서 가장 오래되고 널리 사용되는 알고리즘 중 하나입니다. TD($\lambda$)는 Eligibility Trace를 사용하여 Forward View와 Backward View 사이의 형식적인 관계를 나타내는 최초의 알고리즘입니다. Forward View는 이론적인 면에서, Backward View는 계산적인 면에서 각각 이점이 있습니다. 이번 Section에서는 TD($\lambda$)가 이전 Section에서 배운 Off-line $\lambda$-Return Algorithm에 근접함을 경험적으로 보여줄 것입니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)는 Off-line Return Algorithm을 세 가지 방식으로 개선합니다. 첫째, Episode가 끝날 때 뿐만 아니라 Episode의 모든 단계에서 Weight Vector를 Update함으로써 추정치를 더 빠르게 계산합니다. 둘째, Episode가 끝낼 때 한번에 계산하지 않고, 시간에 따라 계산이 균등하게 분산됩니다. 셋째, Episodic Task 뿐만 아니라 Continuing Task에도 적용할 수 있습니다. 이번 Section에서는 Function Approximation을 사용하여 TD($\lambda$)의 Semi-gradient 버전을 제시합니다.&lt;/p&gt;

&lt;p&gt;Function Approximation에서 Eligibility Trace는 Weight Vector $\mathbf{w}_t$와 동일한 수의 구성 요소를 갖는 Vector $\mathbf{z}_t \in \mathbb{R}^d$입니다. Weight Vector는 시스템의 전체 수명 동안 누적되는 Long-term Memory이지만, Eligibility Trace는 일반적으로 Episode의 길이보다 짧은 시간 동안만 지속되는 Short-term Memory입니다. Eligibility Trace의 결과는 Weight Vector에 영향을 미치고 Weight Vector가 추정한 값을 결정함으로써 학습 과정에 도움이 됩니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)에서 Eligibility Trace의 Vector는 Episode 시작 시 Zero Vector로 초기화되고, 각 시간 단계에서 이전 Vector의 $\gamma \lambda$만큼 감소한 후, Value Function의 Gradient만큼 증가합니다. 이것을 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_{-1} &amp;amp; \doteq \mathbf{0} \\ \\
\mathbf{z}_t &amp;amp; \doteq  \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{v} (S_t, \mathbf{w}_t), \quad 0 \le t \le T \tag{12.5}
\end{align}\]

&lt;p&gt;식 (12.5)에서 $\gamma$는 Discount Factor이고, $\lambda$는 이전 Section에서 소개한 매개변수인데, 앞으로 이것을 Trace-decay 매개변수라고 부르겠습니다. Eligibility Trace는 Weight Vector의 어떤 구성 요소가 최근 State에 대한 평가에 긍정적으로/부정적으로 기여했는지 &lt;strong&gt;Trace&lt;/strong&gt;합니다. 여기서 &lt;strong&gt;최근&lt;/strong&gt;은 $\gamma \lambda$로 정의됩니다. Trace는 학습 이벤트가 발생할 경우 그것에 의해 변경이 일어날 수 있는 Weight Vector에서 각 구성 요소들의 &lt;strong&gt;Eligibility&lt;/strong&gt;를 나타냅니다. 여기서 우리가 우려할 수 있는 학습 이벤트는 매 순간의 1-step TD Error입니다. State-Value Prediction에 대한 TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t \doteq R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v} (S_t, \mathbf{w}_t) \tag{12.6}\]

&lt;p&gt;TD($\lambda$)에서 Weight Vector는 Scalar TD Error 및 Vector에 대한 Eligibility Trace에 비례하여 각 단계에서 다음과 같이 Update됩니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t \tag{12.7}\]

&lt;p&gt;Semi-gradient TD($\lambda$)의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TD($\lambda$)는 시간적으로 Backward View라고 볼 수 있습니다. 매 순간 현재의 TD Error를 확인하고, 그 State가 당시 Eligibility Trace에 얼마나 기여했는지에 따라 각각의 이전 State에 거꾸로 반영합니다. State가 미래에 다시 발생할 때를 대비하여 아래 그림과 같이 State의 흐름과 TD Error를 계산하고 식 (12.7)에 의해 얻은 Update를 이용하여 과거의 Value를 변경합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것을 조금 더 잘 이해하기 위해서는 $\lambda$에 값에 따라 어떻게 달라지는지 생각해보면 됩니다. 만약 $\lambda = 0$인 경우라면 식 (12.5)에 의해 시간 $t$ 에서의 Trace는 정확히 State $S_t$에서 Value의 Gradient와 같습니다. 따라서 이 때의 TD($\lambda$) Update인 식 (12.7)은 9장에서 배운 1-step TD Update와 동일합니다. 이것이 그 당시 1-step TD Update를 TD(0)로도 불렀던 이유입니다. 위의 그림을 토대로 설명하자면, TD(0)는 현재 State를 기준으로 한 단계 이전의 State에 대한 TD Error로만 Update하는 경우입니다. 하지만 만약 $\lambda &amp;lt; 1$ 조건 하에 $\lambda$의 값이 증가한다면 더 많은 이전 State들이 Update되는데, 그림에서 볼 수 있듯이 시간적으로 멀리 떨어진 State일수록 Eligibility Trace가 더 작기 때문에 덜 Update됩니다. 이것을 &lt;strong&gt;초기 State는 TD Error에 대해 더 적은 Credit을 받았다&lt;/strong&gt;라고 표현하기도 합니다.&lt;/p&gt;

&lt;p&gt;만약 $\lambda = 1$인 경우라면, 이전 State에 부여된 Credit은 단계당 $\gamma$만큼 떨어집니다. 예를 들어, TD Error $\delta_t$는 Discount 되지 않은 $R_{t+1}$를 포함합니다. 그리고 이전의 $k$ 단계에 대한 Return을 계산할 때는 Reward에 $\gamma^k$ 만큼의 Discount가 곱해지는데, 이것은 점점 감소하는 Eligibility Trace가 됩니다. 만약 $\lambda = 1$이고 $\gamma = 1$일 때는 시간적으로 아무리 떨어져 있더라도 Eligibility Trace가 소멸되지 않습니다. 이 경우에는 Discount가 없는 Episodic Task에 대한 Monte Carlo Method처럼 작동합니다. $\lambda = 1$인 경우 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;TD(1)&lt;/span&gt;으로도 부릅니다.&lt;/p&gt;

&lt;p&gt;TD(1)은 기존의 Monte Carlo Method를 더 일반적으로 구현한 방법입니다. 기존의 Monte Carlo Method는 Episodic Task에 한정되었지만, TD(1)은 Discounted Continuing Task에도 적용할 수 있습니다. 또한 TD(1)은 점진적으로, 온라인으로 수행할 수도 있습니다. Monte Carlo Method는 Episode가 끝날 때까지 아무것도 학습하지 못한다는 단점이 있지만, TD(1)는 Episode가 끝나지 않은 상황에서도 그 일부분을 $n$-step TD 방식으로 학습할 수 있다는 장점이 있습니다. 예를 들어, 만약 Episode 중 비정상적으로 좋거나 나쁜 일이 발생하면 TD(1)에 기반한 Control은 즉시 이전까지의 내용을 학습하고 Episode를 변경할 수 있습니다.&lt;/p&gt;

&lt;p&gt;TD($\lambda$)가 Off-line $\lambda$-Return Algorithm을 근사하는데 얼마나 성능이 좋은지 알아보기 위해 또 다시 19개의 State를 가진 Random Walk Example을 놓고 비교해보겠습니다. 아래 그림을 보시면 그래프의 모양 자체는 차이가 있지만, $\lambda$의 값이 최적인 State에서는 거의 동일한 성능을 보임을 알 수 있습니다. 다만 $\lambda$가 최적보다 크게 선택되는 상황을 보면 TD($\lambda$)는 Off-line $\lambda$-Return Algorithm보다 성능이 더 나쁘다는 것을 알 수 있습니다. 일반적으로 최적의 State를 제외하고는 $\lambda$를 사용하지 않기 때문에 큰 문제는 아닙니다만, TD($\lambda$)가 &lt;strong&gt;더 불안정하다&lt;/strong&gt;라고는 말할 수 있을 정도의 단점은 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Linear TD($\lambda$)는 On-policy인 경우 조건 식 (2.7)에 따라 Step-size Parameter가 시간에 따라 감소한다면 수렴합니다. Section 9.4에서 다룬 바와 같이 수렴한다는 것은 Weight Vector의 최소 오차가 $\lambda$에 따른다는 것을 의미합니다. 식 (9.14)에서 배운 오차 한계식은 $\lambda$에 의해 일반화될 수 있습니다. 만약 Discounted Continuing Task라면, 다음과 같습니다.&lt;/p&gt;

\[\overline{\text{VE}}(\mathbf{w}_{\text{TD}}) \le \frac{1 - \gamma \lambda}{1 - \gamma} \min_{\mathbf{w}} \overline{\text{VE}}(\mathbf{w}) \tag{12.8}\]

&lt;p&gt;즉, 점근적인 오차는 가능한 최소 오차의 $\frac{1 - \gamma \lambda}{1 - \gamma}$배를 넘지 않는 다는 뜻입니다. $\lambda$가 1에 가까워질수록 최소 오차에 가까워집니다. 이렇게 보면 $\lambda$를 1에 가깝게 잡는 것이 좋아보이지만, 실제로는 가장 좋지 않은 선택이 될 가능성이 높은데, 그 이유는 나중에 밝혀집니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-truncated-lambda-return-methods&quot;&gt;$n$-step Truncated $\lambda$-Return Methods&lt;/h2&gt;

&lt;p&gt;Off-line $\lambda$-Return Algorithm은 중요하지만, Episode가 끝날 때까지 알 수 없는 $\lambda$-Return을 이용하기 때문에 효용이 제한적입니다. (식 12.2 참고) Continuing Task의 경우, $\lambda$-Return은 기술적으로 계산할 수 없기 때문에 임의적으로 큰 $n$에 대해 $n$-step Return에 의존합니다. 하지만 시간적으로 멀리 떨어진 Reward일수록 $\gamma \lambda$만큼의 비율로 계속 비중이 줄어들기 때문에, 이 경우 근사를 하기 위해서는 일정 Step 마다 구간을 나누는 것이 좋습니다. $n$-step Return은 누락된 Reward를 추정된 값으로 대체하는 개념을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;Episode를 일정 구간인 $h$만큼 자르는 경우, 시간 $t$에 대한 Truncated $\lambda$-Return 식은 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[G_{t:h}^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{h-t-1}G_{t:h}, \quad 0 \le t &amp;lt; h \le T \tag{12.9}\]

&lt;p&gt;이 식은 $h$의 역할이 식 (12.3)에서 $T$의 역할과 동일함을 알 수 있습니다. 또 다른 차이점을 굳이 찾자면 두 번째 항의 $G_t$ 대신 $G_{t:h}$로 변경된 것 정도가 있고, 그 외에는 식 (12.3)과 동일합니다.&lt;/p&gt;

&lt;p&gt;Truncated $\lambda$-Return은 7장에서의 $n$-step 방법과 유사한 $n$-step Return Algorithm을 즉시 생성하는 방식으로 구성됩니다. 이 때의 Update는 $n$-step 만큼 지연되고 처음 $n$-step만 고려되었지만, 이제는 모든 $k$-step ($1 \le k \le n$) Return이 포함됩니다. State-Value의 경우 이 알고리즘과 같은 종류를 &lt;span style=&quot;color:red&quot;&gt;Truncated TD($\lambda$)&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;TTD($\lambda$)&lt;/span&gt;라고 부릅니다. 아래 그림의 복합적인 Backup Diagram은 가장 긴 구성 요소에 대한 Update가 항상 Episode의 끝까지 진행되는 것이 아니라 최대 $n$-step이라는 점을 명시하고 있습니다. 그 점을 제외한다면 이전에 보여드린 $\lambda$-Return의 Backup Diagram과 유사합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TTD($\lambda$)에 대한 식은 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \left[ G_{t:t+n}^{\lambda} - \hat{v} (S_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{v} (S_t, \mathbf{w}_{t+n-1}), \quad 0 \le t &amp;lt; T\]

&lt;p&gt;이 알고리즘은 각 단계별 계산이 $n$으로 확장되지 않도록 효율적으로 구현할 수 있습니다. (즉, 시간 복잡도가 $n$에 비례하지 않도록) $n$-step TD 방법과 마찬가지로 각 Episode의 처음 $n-1$ 시간 단계에서는 Update가 수행되지 않으며, Episode가 종료 후 $n-1$에 대한 추가적인 Update가 수행됩니다. 효율적인 구현을 위해 $k$-step $\lambda$-Return은 다음과 같이 표현으로 수정할 수 있습니다.&lt;/p&gt;

\[G_{t:t+k}^{\lambda} = \hat{v} (S_t, \mathbf{w}_{t-1}) + \sum_{i=t}^{t+k-1} (\gamma \lambda)^{i-t} \delta_i &apos; \tag{12.10}\]

&lt;p&gt;이 때, $\delta_i^{\prime} \doteq R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v} (S_t, \mathbf{w}_{t-1})$입니다.&lt;/p&gt;

&lt;h2 id=&quot;redoing-updates-online-lambda-return-algorithm&quot;&gt;Redoing Updates: Online $\lambda$-Return Algorithm&lt;/h2&gt;

&lt;p&gt;Truncated TD($\lambda$)에서 Truncation Parameter $n$을 선택할 때는 Trade-off가 있습니다. Truncated TD($\lambda$)가 Off-line $\lambda$-Return Algorithm에 근접하기 위해서 $n$이 커야 하지만, Update가 더 빨리 이루어지기 위해서는 $n$이 작아야 합니다. 이런 상황에서 둘 다 포기하지 않는 방법은 있지만, 그만큼 계산 복잡도가 증가하는 문제가 있습니다. 이번 Section에서는 이 방법을 소개하겠습니다.&lt;/p&gt;

&lt;p&gt;기본적인 아이디어는 새로운 데이터의 Increment를 얻을 때마다 현재 Episode의 시작 부분으로 되돌아가 모든 Update를 다시 실행하는 것입니다. 그러면 각 시간 단계에서 새로운 데이터를 고려할 수 있기 때문에 새 Update는 이전에 계산한 결과보다 더 나을 것이기 때문입니다. 즉, Update는 항상 최신 Horizon $h$를 사용하여 $n$-step Truncated $\lambda$-Return의 Target을 계산하는 것입니다. 각 Episode가 끝날 때마다 약간 더 긴 Horizon $h$를 사용하면 약간 더 나은 결과를 얻을 수 있습니다. 먼저, 식 (12.9)에서의 Truncated $\lambda$-Return은 다음과 같이 정의했었습니다.&lt;/p&gt;

\[G_{t:h}^{\lambda} \doteq (1 - \lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{h-t-1}G_{t:h}\]

&lt;p&gt;계산 복잡도가 문제가 되지 않는 상황에서 이 Target을 이상적으로 사용할 수 있는 방법을 단계별로 살펴보겠습니다. 각각의 Episode는 이전 Episode의 끝에서 $\mathbf{w}_0$을 사용하여 시간 0에서의 추정으로 시작합니다. 데이터 Horizon이 시간 단계 1로 확장될 때 학습이 시작됩니다. Horizon 1까지의 데이터가 주어지면, 시간 단계 0에서의 추정 Target은 $R_1$과 추정치 $\hat{v}(S_1, \mathbf{w}_0)$의 Bootstrap을 포함한 1-step Return $G_{0:1}$입니다. 이것은 정확하게 $G_{0:1}^{\lambda}$이며, 위 식의 첫 번째 항의 합은 0으로 감소합니다. 그 후 이 Target Update를 사용하여 $\mathbf{w}_1$을 만듭니다. 그 두 데이터 Horizon을 시간 단계 2로 진행한 후, $R_2$, $S_2$를 얻을 수 있으므로 $S_0$의 더 나은 Update인 $G_{0:2}^{\lambda}$와 $S_1$의 Update인 $G_{1:2}^{\lambda}$를 계산할 수 있습니다. 이렇게 더 나아진 Target을 사용하여 $S_1$과 $S_2$를 다시 Update하고, Weight Update를 $\mathbf{w}_0$부터 시작하여 $\mathbf{w}_2$를 계산합니다. 데이터 Horizon이 시간 단계 3으로 넘어가면 이 과정을 또다시 반복하는 것입니다. 이렇게 새로운 데이터 Horizon을 얻을 때마다 Weight Update를 $\mathbf{w}_0$부터 다시 계산하여 Update를 수행합니다.&lt;/p&gt;

&lt;p&gt;이러한 개념적인 알고리즘은 각각의 Horizon $h$에서 동일한 Episode에 대한 서로 다른 Weight Vector를 생성합니다. 이것을 명확하게 설명하기 위해서는 다른 Horizon에서 계산된 Weight Vector를 구별할 수 있어야 합니다. Horizon $h$까지의 과정에서 시간 $t$의 Value를 추정하는데 사용한 Weight를 $\mathbf{w}_t^h$라 합시다. 각 과정에서의 첫 번째 Weight Vector $\mathbf{w}_0^h$는 이전 Episode로부터 상속된 것이고 (모든 $h$에 대해 마찬가지), 각 과정의 마지막 Weight Vector $\mathbf{w}_h^h$는 알고리즘의 궁극적인 Weight Vector를 정의합니다. 마지막 Horizon $h = T$에서는 다음 Episode의 초기 Weight를 생성하기 위해 전달할 최종 Weight $\mathbf{w}_T^T$를 얻습니다. 이러한 과정을 $h = 3$까지 수식으로 표현하자면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
h = 1 : \mathbf{w}_1^1 \doteq \mathbf{w}_0^1 + \alpha \left[ G_{0:1}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^1) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^1) \\ \\
h = 2 : \mathbf{w}_1^2 \doteq \mathbf{w}_0^2 + \alpha \left[ G_{0:2}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^2) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^2) \\ \\
\quad \mathbf{w}_2^2 \doteq \mathbf{w}_1^2 + \alpha \left[ G_{1:2}^{\lambda} - \hat{v} (S_1, \mathbf{w}_1^2) \right] \nabla \hat{v} (S_1, \mathbf{w}_1^2) \\ \\
h = 3 : \mathbf{w}_1^3 \doteq \mathbf{w}_0^3 + \alpha \left[ G_{0:3}^{\lambda} - \hat{v} (S_0, \mathbf{w}_0^3) \right] \nabla \hat{v} (S_0, \mathbf{w}_0^3) \\ \\
\quad \mathbf{w}_2^3 \doteq \mathbf{w}_1^3 + \alpha \left[ G_{1:3}^{\lambda} - \hat{v} (S_1, \mathbf{w}_1^3) \right] \nabla \hat{v} (S_1, \mathbf{w}_1^3) \\ \\
\quad \mathbf{w}_3^3 \doteq \mathbf{w}_2^3 + \alpha \left[ G_{2:3}^{\lambda} - \hat{v} (S_2, \mathbf{w}_2^3) \right] \nabla \hat{v} (S_2, \mathbf{w}_2^3)
\end{align}\]

&lt;p&gt;이것을 일반화하면 다음과 같은 수식을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1}^h \doteq \mathbf{w}_t^h + \alpha \left[ G_{t:h}^{\lambda} - \hat{v} (S_t, \mathbf{w}_t^h) \right] \nabla \hat{v} (S_t, \mathbf{w}_t^h), \quad 0 \le t &amp;lt; h \le T\]

&lt;p&gt;만약 $\mathbf{w}_t \doteq \mathbf{w}_t^t$로 정의된다면 &lt;span style=&quot;color:red&quot;&gt;On-line $\lambda$-Return Algorithm&lt;/span&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;On-line $\lambda$-Return Algorithm은 완전하게 On-line으로 동작하며, 시간 $t$에서 사용할 수 있는 정보만 사용하여 새로운 Weight Vector $\mathbf{w}_t$를 계산합니다. 이 때의 단점은 매 시간 단계마다 경험한 Episode의 일부를 사용하여 계산하는 것이 계산 복잡도가 높다는 것입니다. Off-line $\lambda$-Return Algorithm은 Episode를 수행하는 동안 Update를 수행하지 않고, Episode를 종료하는 시점에서 모든 시간 단계에 대한 Update를 수행했기 때문에 계산 복잡도가 높지 않았습니다. On-line $\lambda$-Return Algorithm은 그 계산 복잡도를 대가로 Episode가 진행되는 도중 뿐만 아니라 Episode가 끝날 때도 더 나은 성능을 기대할 수 있습니다. Bootrstrapping에 사용되는 Weight Vector에 반영되는 정보가 더 많기 때문입니다. 아래 그림은 지금까지 보았던 Random Walk Example에서 On-line과 Off-line 알고리즘을 비교한 그래프입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;true-online-tdlambda&quot;&gt;True Online TD($\lambda$)&lt;/h2&gt;

&lt;p&gt;이전 Section에서 제시한 On-line $\lambda$-Return Algorithm은 현재 가장 성능이 좋은 시분할 알고리즘입니다. 즉, On-line TD($\lambda$)를 근사화하는 이상적인 알고리즘입니다. On-line $\lambda$-Return Algorithm은 Forward View Algorithm이지만, 효율적으로 구현하기 위해서 Backward View Algorithm으로 변형시킬 방법이 있을까요? Linear Function Approximation의 경우라면 그 대답은 Yes입니다. 이 구현은 TD($\lambda$) Algorithm보다 On-line $\lambda$-Return Algorithm에 이상적으로 가깝기 때문에 &lt;span style=&quot;color:red&quot;&gt;True On-line TD($\lambda$) Algorithm&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)를 유도하는 과정을 여기에서 보이기에는 너무 복잡하기 때문에 여기에서는 생략하겠습니다. (다음 Section 및 van Seijen et al., 2016) 대략적인 아이디어를 소개하자면, 먼저 On-line $\lambda$-Return Algorithm은 다음과 같이 삼각형으로 나열할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 삼각형에서 하나의 행은 각 시간 단계에서 생성됩니다. 삼각형을 구성하는 요소는 많지만, 이전 Section에서 보았듯이 우리에게 필요한 것은 대각선 요소인 $\mathbf{w}_t^t$뿐입니다. 첫 번째 요소인 $\mathbf{w}_0^0$는 Episode의 초기 Weight Vector이고, 마지막 요소인 $\mathbf{w}_T^T$는 최종 Weight Vector이며, 그 중간 요소인 $\mathbf{w}_t^t$는 Update에 필요한 $n$-step Return을 얻기 위한 Bootstrapping 역할을 합니다.&lt;/p&gt;

&lt;p&gt;이제 삼각형의 대각선 구성 요소(가장 오른쪽 요소)는 표기의 편의를 위해 $\mathbf{w}_t \doteq \mathbf{w}_t^t$로 재정의하겠습니다. 이제 해야할 것은 대각선 구성 요소인 $\mathbf{w}_t$를 간결하고 효율적으로 계산하는 방법을 찾는 것입니다. 그렇게 하면 $\hat{v}(\mathbf{s}, \mathbf{w}) = \mathbf{w}^{\sf T} \mathbf{x} (\mathbf{s})$와 같은 Linear에 대해 다음과 같은 True On-line TD($\lambda$) Algorithm을 만들 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t + \alpha \left( \mathbf{w}_t^{\sf T} \mathbf{w}_t - \mathbf{w}_{t-1}^{\sf T} \mathbf{x}_t \right) \left( \mathbf{z}_t - \mathbf{x}_t \right),\]

&lt;p&gt;위 식에서 $\mathbf{x}_t \doteq \mathbf{x} (S_t)$이며, $\delta_t$는 TD($\lambda$)인 식 (12.6)을 의미합니다. 또한 $\mathbf{z}_t$는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma \lambda \mathbf{z}_{t-1} + \left( 1 - \alpha \gamma \lambda \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t \tag{12.11}\]

&lt;p&gt;이 알고리즘은 On-line $\lambda$-Return Algorithm과 정확하게 동일한 Weight Vector $\mathbf{w}_t (0 \le t \le T)$를 생성하는 것으로 증명되었습니다. (van Seijen et al., 2016) 이전 Section에서의 마지막 그림인 Random Walk의 On-line $\lambda$-Return Algorithm도 이것을 사용한 결과입니다. 이제 지금까지 단점으로 남아있던 높은 계산 복잡도가 해결되었습니다. 공간 복잡도 측면에서 보면 On-line TD($\lambda$)의 메모리 요구량은 기존 TD($\lambda$)의 메모리 요구량과 동일하고, 시간 복잡도 측면에서 보면 각 단계별 계산량은 약 50% 증가했지만, 전체적으로 보았을 때 각 단계별 시간 복잡도는 TD($\lambda$)와 동일하게 $O(d)$로 유지됩니다.&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;True On-line TD($\lambda$)에 사용된 Eligibility Trace 식 (12.11)은 기존의 TD($\lambda$)에서 사용한 Eligibility Trace 식 (12.5)와 구분하기 위해 &lt;span style=&quot;color:red&quot;&gt;Dutch Trace&lt;/span&gt;라고 부릅니다. 참고로 식 (12.5)와 같은 Eligibility Trace은 Accumulating Trace라고 부르기도 합니다.&lt;/p&gt;

&lt;p&gt;이전에는 Tabular 방법이나 Tile Coding과 같은 Binary Feature Vector에 대해서는 &lt;span style=&quot;color:red&quot;&gt;Replacing Trace&lt;/span&gt;라고 하는 또 다른 방법을 사용했었습니다. Replacing Trace는 Feature Vector의 구성 요소가 1인지, 0인지에 따라 다르게 정의됩니다.&lt;/p&gt;

\[z_{i, t} \doteq \begin{cases} 1, &amp;amp; \text{if } x_{i, t} = 1 \\ \gamma \lambda z_{i, t-1}, &amp;amp; \text{otherwise} \end{cases} \tag{12.12}\]

&lt;p&gt;요즘에는 Replacing Trace를 Dutch Trace의 조잡한 근사치 정도로 간주합니다. Replacing Trace는 일반적으로 Dutch Trace보다 성능이 낮기 때문에 Dutch Trace로 이를 대체하는 경우가 많습니다. 물론, 이에 대한 이론적인 근거 또한 있습니다. Accumulating Trace는 Dutch Trace를 사용할 수 없는 non-Linear Function Approximation에서 사용하기 때문에 중요한 Trace로 간주합니다.&lt;/p&gt;

&lt;h2 id=&quot;dutch-traces-in-monte-carlo-learning&quot;&gt;Dutch Traces in Monte Carlo Learning&lt;/h2&gt;

&lt;p&gt;Eligibility Trace는 TD 학습과 밀접한 관련이 있는 것 같지만 실제로는 별로 관련이 없습니다. 이번 Section에서 보이겠지만, Monte Carlo 학습도 Eligibility Trace가 발생합니다. 9장에서 다루었던 Forward View에서 본 Linear Monte Carlo 알고리즘에 Dutch Trace를 사용하여 더 계산적으로 효율적인 Backward View 알고리즘을 유도할 수 있다는 것을 보일 예정입니다. 이점은 이 책의 저자가 인정하는 Forward View와 Backward View의 유일한 동치 부분입니다. 또한 이것은 True On-line TD($\lambda$)와 On-line $\lambda$-Return Algorithm의 동등성 증명 방향을 제공하지만, 훨씬 더 간단합니다.&lt;/p&gt;

&lt;p&gt;먼저 Gradient Monte Carlo 예측 알고리즘의 Linear 버전은 Episode의 각 시간 단계에서 하나씩 다음과 같은 Update가 발생합니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G - \mathbf{w}_t^{\sf T} \mathbf{x}_t \right] \mathbf{x}_t , \quad 0 \le t &amp;lt; T \tag{12.13}\]

&lt;p&gt;예시를 단순화하기 위해 Return $G$는 Episode가 끝날 때 받은 단일 Reward이고 Discount가 없다고 가정합니다. (Return $G$는 &lt;strong&gt;단일&lt;/strong&gt; 보상이기 때문에 $G_t$와 같이 시간에 대한 첨자가 붙지 않습니다.) 이 경우 Update는 Least Mean Square (LMS) 규칙이라고도 합니다. Monte Carlo 알고리즘에서의 모든 Update는 최종 Reward/Return에 따라 달라지므로 Episode가 끝날 때까지 아무것도 할 수 없기 때문에 Monte Carlo 알고리즘은 Off-line 알고리즘입니다. 그래서 여기서는 계산상의 이점이 있는 새로운 알고리즘의 구현을 목적으로 합니다. 새로운 알고리즘에서도 기존과 마찬가지로 Episode가 끝날 때만 Weight Vector를 Update하겠지만, Episode의 각 단계에서 약간의 계산을 수행함으로써 전체적으로 계산량을 고르게 분배할 계획입니다. 이것을 통해 단계당 $O(d)$의 시간 복잡도가 소요되지만, 각 단계에서 Feature Vector를 저장할 필요가 없습니다. 대신 Eligibility Trace를 도입하여 지금까지 경험한 모든 Feature Vector의 요점만을 저장합니다. 이 방법은 Episode가 끝날 때까지 식 (12.13)의 Update 과정과 정확히 동일한 Update를 효율적으로 재생성합니다. 식에 대한 전개 과정은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{T} &amp;amp;= \mathbf{w}_{T-1} + \alpha \left( G - \mathbf{w}_{T-1}^{\sf T} \mathbf{x}_{T-1} \right) \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{w}_{T-1} + \alpha \mathbf{x}_{T-1} \left( - \mathbf{x}_{T-1}^{\sf T} \mathbf{w}_{T-1} \right) + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \left( \mathbf{I} - \alpha \mathbf{x}_{T-1} \mathbf{x}_{T-1}^{\sf T} \right) \mathbf{w}_{T-1} + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{w}_{T-1} + \alpha G \mathbf{x}_{T-1}
\end{align}\]

&lt;p&gt;이 때 $\mathbf{F}_{t} \doteq \mathbf{I} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T}$는 &lt;span style=&quot;color:red&quot;&gt;Forgetting&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;Fading&lt;/span&gt;이라고 부르는 행렬입니다. 이제 위 식을 재귀적으로 전개해보면,&lt;/p&gt;

\[\begin{align}
&amp;amp;= \mathbf{F}_{T-1} \left( \mathbf{F}_{T-2} \mathbf{w}_{T-2} + \alpha G \mathbf{x}_{T-2} \right) + \alpha G \mathbf{x}_{T-1} \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{w}_{T-2} + \alpha G \left( \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \left( \mathbf{F}_{T-3} \mathbf{w}_{T-3} + \alpha G \mathbf{x}_{T-3} \right) + \alpha G \left( \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp;= \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{F}_{T-3} \mathbf{w}_{T-3} + \alpha \left( \mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{x}_{T-3} + \mathbf{F}_{T-1} \mathbf{x}_{T-2} + \mathbf{x}_{T-1} \right) \\ \\
&amp;amp; \qquad \qquad \vdots \\ \\
&amp;amp;= \underbrace{\mathbf{F}_{T-1} \mathbf{F}_{T-2} \cdots \mathbf{F}_0 \mathbf{w}_0}_{\mathbf{a}_{T-1}} + \alpha G \underbrace{\sum_{k=1}^{T-1} \mathbf{F}_{T-1} \mathbf{F}_{T-2} \cdots \mathbf{F}_{k+1} \mathbf{x}_k}_{\mathbf{z}_{T-1}} \\ \\
&amp;amp;= \mathbf{a}_{T-1} + \alpha G \mathbf{z}_{T-1} \tag{12.14}
\end{align}\]

&lt;p&gt;여기서 $\mathbf{a}_{T-1}$과 $\mathbf{z}_{T-1}$는 시간 $T-1$에서의 메모리 Vector로써, $G$에 대한 정보 없이 각 시간 단계에서 $O(d)$의 시간 복잡도로 Update할 수 있습니다. 이 중 Vector $\mathbf{z}_{t}$는 Dutch-style Eligibility Trace입니다. 이 Vector는 시간 단계 0에서 $\mathbf{z}_0 = \mathbf{x}_0$으로 초기화된 후 다음과 같이 Update됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_t &amp;amp; \doteq \sum_{k=1}^t \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_{k+1} \mathbf{x}_k, \quad 1 \le t &amp;lt; T \\ \\
&amp;amp;= \sum_{k=0}^{t-1} \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_{k+1} \mathbf{x}_k + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{F}_t \sum_{k=1}^{t-1} \mathbf{F}_{t-1} \mathbf{F}_{t-2} \cdots \mathbf{F}_{k+1} \mathbf{x}_k + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{F}_t \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \left( I - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \right) \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \mathbf{z}_{t-1} + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} - \alpha \left( \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t + \mathbf{x}_t \\ \\
&amp;amp;= \mathbf{z}_{t-1} + \left( 1 - \alpha \mathbf{z}_{t-1}^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t
\end{align}\]

&lt;p&gt;이것은 식 (12.11)에서 $\gamma \lambda = 1$인 경우에 대한 Dutch Trace입니다. Auxiliary Vector $\mathbf{a}_t$는 시간 단계 0에서 $\mathbf{a}_0 = \mathbf{w}_0$로 초기화 된 후, 다음과 같이 Update됩니다.&lt;/p&gt;

\[\mathbf{a}_t \doteq \mathbf{F}_t \mathbf{F}_{t-1} \cdots \mathbf{F}_0 \mathbf{w}_0 = \mathbf{F}_t \mathbf{a}_{t-1} = \mathbf{a}_{t-1} - \alpha \mathbf{x}_t \mathbf{x}_t^{\sf T} \mathbf{a}_{t-1}, \quad 1 \le t &amp;lt; T\]

&lt;p&gt;Auxiliary Vector $\mathbf{a}_t$와 Dutch Trace $\mathbf{z}_{t}$는 각 시간 단계 $t &amp;lt; T$에서 Update되고 $G$를 알 수 있는 시간 단계 $T$에서는 $\mathbf{w}_T$를 계산하기 위해 식 (12.14)에서 사용됩니다. 이런 방법으로 계산 복잡도가 높은 식 (12.13)과 같은 MC/LMS 알고리즘과 정확히 동일한 최종 결과를 얻었습니다. 새로 구한 방법은 각 시간 단계별 시간 복잡도 및 공간 복잡도가 $O(d)$인 증분 알고리즘을 사용합니다. 이것은 TD Learning이 아닌 환경에서 Eligibility Trace의 개념이 발생했기 때문에 흥미로운 결과입니다. 그러므로 Eligibility Trace는 TD Learning에만 국한되지 않는다는 것을 알 수 있습니다. Eligibility Trace의 필요성은 효율적인 방식으로 장기적인 예상치를 학습하고자 할 때 나타나는 것으로 보면 될 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;sarsalambda&quot;&gt;Sarsa($\lambda$)&lt;/h2&gt;

&lt;p&gt;Eligibility Trace를 Action-Value로 확장하기 위해서는 다행스럽게도 이미 제시된 아이디어에서 거의 변경할 필요가 없습니다. Estimated Value인 $\hat{q} (s, a, \mathbf{w})$를 학습하기 위해서는 아래와 같이 10장에서 배운 $n$-step Return의 Action-Value 형태를 사용해야 합니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{q} \left( S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1} \right), \quad t+n &amp;lt; T\]

&lt;p&gt;이 때 만약 $t + n \ge T$인 경우라면 $G_{t:t+n} \doteq G_t$입니다. 이 방법을 통해 $\lambda$-Return의 Action-Value 형태를 만들 수 있습니다. Off-line $\lambda$-Return Algorithm의 Action-Value 형식은 단순히 $\hat{v}$를 $\hat{q}$로 대체하면 됩니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G_t^{\lambda} - \hat{q} \left( S_t, A_t, \mathbf{w}_t \right) \right] \nabla \hat{q} \left( S_t, A_t, \mathbf{w}_t \right), \quad t = 0, \ldots, T - 1 \tag{12.15}\]

&lt;p&gt;이 때 $G_t^{\lambda} \doteq G_{t:\infty}^{\lambda}$입니다. 이 Forward View 에 대한 복합적인 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Backup Diagram과 TD($\lambda$)의 Backup Diagram을 비교해보면 굉장히 유사하다는 것을 알 수 있습니다. 또한 $\lambda$-Return에서 각 $n$-step Update의 Weight는 TD($\lambda$) 및 $\lambda$-Return 알고리즘에서와 같습니다.&lt;/p&gt;

&lt;p&gt;Action Value에 대한 TD 방법은 &lt;span style=&quot;color:red&quot;&gt;Sarsa($\lambda$)&lt;/span&gt;로 알려져 있는데, 이것은 이 Forward View를 추정합니다. 이 방법의 Update 규칙은 아래와 같이 TD($\lambda$)와 동일합니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t\]

&lt;p&gt;단, TD Error의 Action-Value 형태는 예외적입니다.&lt;/p&gt;

\[\delta_t \doteq R_{t+1} + \gamma \hat{q} \left( S_{t+1}, A_{t+1}, \mathbf{w}_t \right) - \hat{q} \left( S_t, A_t, \mathbf{w}_t \right) \tag{12.16}\]

&lt;p&gt;또한 Eligibility Trace의 Action-Value 형태는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_{-1} &amp;amp; \doteq \mathbf{0} \\ \\
\mathbf{z} &amp;amp; \doteq \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{q} \left( S_t, A_t, \mathbf{w} \right), \quad 0 \le t \le T
\end{align}\]

&lt;p&gt;Sarsa($\lambda$)의 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 12.1) Traces in Gridworld&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 사용하면 1-step 방법이나 $n$-step 방법보다 Control 알고리즘의 효율성을 크게 높일 수 있습니다. Gridworld 예제를 이용하여 이것을 설명하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 그림은 단일 Episode에서 Agent가 이동한 경로를 나타냅니다. 초기 Estimated Value는 0이고, G로 표시된 Target 지점을 제외하면 모든 Reward는 0입니다. 나머지 그림에 나타난 화살표는 각각의 알고리즘에 대해 어떤 Action-Value가 얼마나 증가하는지를 나타냅니다. 1-step Sarsa는 Target에 도달했을 때 마지막 Action에 대한 Value만 증가시키지만, $n$-step 방법은 마지막 $n$개의 Action에 대한 Value를 동일하게 증가시킵니다. ($\gamma = 1$이라고 가정) 가장 오른쪽에 있는 Sarsa($\lambda$) 방법은 Episode에서의 모든 Action에 대한 Value를 Update 하지만, Target 지점에서 (시간적으로) 멀어질수록 더 적게 반영됩니다. 이러한 Update 방법을 &lt;span style=&quot;color:red&quot;&gt;Fading&lt;/span&gt;이라고 하는데, 일반적으로 Fading 방법이 제일 좋은 경우가 많습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 12.2) Sarsa($\lambda$) on Mountain Car&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에는 10장에서 다루었던 Mountain Car 예제에 Sarsa($\lambda$)를 적용해 보겠습니다. 기본적인 예제의 세팅은 10장에서와 동일합니다. 위의 그림은 Mountain Car 문제에 대해  Sarsa($\lambda$)와 $n$-step Sarsa의 성능을 비교한 그래프입니다. $n$-step Sarsa에서는 변수로써 $n$의 값을 변경하며 비교했지만, Sarsa($\lambda$)에서는 $\lambda$의 값을 변경하며 비교합니다. 두 그래프를 비교해보면 Sarsa($\lambda$)의 Fading-trace bootstrapping 전략이 이 문제에 대해 더 효율적인 학습 방법이라는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;또한 이상적인 TD 방법의 Action-Value 버전을 On-line $\lambda$-Return 알고리즘 및 True On-line TD($\lambda$)으로 구현할 수도 있습니다. Section 12.4에서 다룬 On-line $\lambda$-Return 알고리즘의 Action-Value 버전은 $n$-step Return을 Action-Value 형식으로 바꾸는 것 외에는 변경할 부분이 없습니다. 또한 Section 12.5와 12.6에서의 분석은 Action-Value에 대해서도 동일하며, 유일한 차이점은 State에 대한 Feature Vector를 $\mathbf{x}_t = \mathbf{x}(S_t)$ 대신 $\mathbf{x}_t = \mathbf{x}(S_t, A_t)$로 사용한다는 것입니다. True On-line Sarsa($\lambda$)에 대한 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 그림은 Mountain Car 예제에서 Sarsa($\lambda$)의 여러 버전에 대해 성능을 비교하는 그래프입니다. True On-line Sarsa($\lambda$)는 일반 Sarsa($\lambda$)보다 더 나은 성능을 보여줌을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;variable-lambda-and-gamma&quot;&gt;Variable $\lambda$ and $\gamma$&lt;/h2&gt;

&lt;p&gt;이제 기본적인 TD Learning 알고리즘에 대한 개발은 끝을 향해 달려가고 있습니다. 최종 알고리즘을 일반적인 형태로 나타내기 위해서는 State와 Action에 잠재적으로 의존하는 함수에 대해 일정한 매개변수는 물론, Bootstrapping 및 Discounting에 대한 정도를 일반화하는 것이 좋습니다. 즉, 각 시간 단계에 대해 서로 다른 $\lambda$및 $\gamma$를 설정하여, 이것을 각각 $\lambda_t$와 $\gamma_t$로 표현하는 것입니다. 또한 표기법을 변경하여 기존의 $\lambda$는 $\lambda : \mathcal{S} \times \mathcal{A} \to [0, 1]$와 같이 State와 Action에 대한 함수로 정의하고, 시간에 따라 변하는 $\lambda_t$는 $\lambda_t \doteq \lambda (S_t, A_t)$와 같이 함수 $\lambda$를 사용하여 표현합니다. 비슷하게, $\gamma$ 또한 $\gamma : \mathcal{S} \to [0, 1]$로 정의하고, $\gamma_t$를 $\gamma_t \doteq \gamma (S_t)$로 표현합니다.&lt;/p&gt;

&lt;p&gt;함수 $\gamma$는 특히 더 중요한데, 우리가 추정하고자 하는 기본 확률 변수인 Return을 변경하기 때문입니다. 이 함수 $\gamma$를 앞으로 &lt;span style=&quot;color:red&quot;&gt;Termination Function&lt;/span&gt;이라고 부르겠습니다. 이제 Return을 다음과 같이 더 일반적으로 정의하겠습니다.&lt;/p&gt;

\[\begin{align}
G_t &amp;amp; \doteq R_{t+1} + \gamma_{t+1} G_{t+1} \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} R_{t+2} + \gamma_{t+1} \gamma_{t+2} R_{t+3} + \gamma_{t+1} \gamma_{t+2} \gamma_{t+3} R_{t+4} + \cdots \\ \\
&amp;amp;= \sum_{k=t}^{\infty} \left( \prod_{i=t+1}^{k} \gamma_i \right) R_{k+1} \tag{12.17}
\end{align}\]

&lt;p&gt;식 (12.17)에서 합이 유한함을 보장하기 위해서는 모든 $t$에 대해 1의 확률로 $\prod_{k=t}^{\infty} \gamma_k = 0$를 만족해야 합니다.&lt;/p&gt;

&lt;p&gt;위와 같은 정의의 편리한 점은 Episode의 설정이나 알고리즘이 특정한 Terminal State나 Start Distribution, 종료 시간과 같은 특별한 설정 없이 단일 경험의 관점에서 표시될 수 있다는 것입니다. 특별한 경우로, 이전의 Terminal State는 $\gamma (s) = 0$인 State가 되어 Start Distribution으로 전환됩니다. 그리고 다른 모든 State에서 상수로 $\gamma ( \cdot )$를 선택함으로써 고전적인 Episode Task로 설정할 수 있습니다. State에 의존적인 종료에는 Markov Process의 흐름을 변경하지 않고 수량을 예측하는 Pseudo Termination과 같은 다른 예측 사례가 포함됩니다. Discounted Return은 그러한 수량으로 생각할 수 있으며, 이 경우 State에 의존적인 종료는 Episodic Task과 Discounted-Continuing Task 모두를 통합합니다. 물론 unDiscounted-Continuing Task의 경우 여전히 특별한 해결 방법이 필요합니다. (이 문단의 번역이 굉장히 어렵네요. 최대한 노력했습니다만 이해가 어려우실 것 같아 원문을 함께 봐주시기 바랍니다)&lt;/p&gt;

&lt;p&gt;가변 Bootsrtapping에 대한 일반화는 Discounting과 같은 문제의 변경이 아니라 해결 방법의 변경입니다. 일반화하는 State와 Action을 위한 $\lambda$-Return에 영향을 미칩니다. 새로운 State 기반 $\lambda$-Return은 다음과 같이 재귀적으로 작성할 수 있습니다.&lt;/p&gt;

\[G_t^{\lambda s} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \hat{v} (S_{t+1}, \mathbf{w}_t) + \lambda_{t+1} G_{t+1}^{\lambda s} \right) \tag{12.18}\]

&lt;p&gt;식 (12.18)은 위 첨자 $s$를 추가하여 이것이 State-Value에서 Bootstrapping 하는 Return임을 나타내고 있습니다. 이와 반대로 아래 식 (12.19)는 위 첨자로 $a$를 추가하여 Action-Value에서 Bootstrapping 하는 Return임을 나타냅니다. 이 식의 첫 번째 항은 $\lambda$-Return에서 Bootstrapping에 영향받지 않고 unDiscounted인 첫 번째 Reward를 의미합니다. 두 번째 항은 만약 다음 State가 Terminal State라면 0이 됩니다.&lt;/p&gt;

&lt;p&gt;만약 다음 State가 Terminal State가 아니라면, 두 번째 항은 State의 Bootstrapping 정도에 따라 두 가지 경우로 구분됩니다. Bootstrapping하는 범위 내에서 이 항은 State에서 추정된 값이지만, Bootstrapping 하지 않는 범위에서 이 항은 다음 시간 단계에서의 $\lambda$-Return입니다.&lt;/p&gt;

&lt;p&gt;Action 기반의 $\lambda$-Return Sarsa 형태는 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda a} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t) + \lambda_{t+1} G_{t+1}^{\lambda a} \right) \tag{12.19}\]

&lt;p&gt;위 식을 Expected Sarsa 형태로 수정하면 다음과 같습니다.&lt;/p&gt;

\[G_t^{\lambda a} \doteq R_{t+1} + \gamma_{t+1} \left( (1 - \lambda_{t+1}) \bar{V}_t (S_{t+1}) + \lambda_{t+1} G_{t+1}^{\lambda a} \right) \tag{12.20}\]

&lt;p&gt;식 (12.20)에서 $\bar{V}_t (s)$는 다음과 같이 Function Approximation으로 정의됩니다.&lt;/p&gt;

\[\bar{V}_t (s) \doteq \sum_a \pi (a | s) \hat{q} (s, a, \mathbf{w}_t) \tag{12.21}\]

&lt;h2 id=&quot;off-policy-traces-with-control-variates&quot;&gt;Off-policy Traces with Control Variates&lt;/h2&gt;

&lt;p&gt;Eligibility Trace의 마지막 단계는 Importance Sampling을 통합하는 것입니다. non-Truncated $\lambda$-Return을 사용하는 방법의 경우 Importance Sampling의 Weight가 Target Return에 적용할 수 있는 옵션이 없습니다. (ex. Section 7.3의 $n$-step 방법) 그래서 대신 Section 7.4에서와 같이 Control Variate가 있는 Per-decision Importance Sampling의 Bootstrapping 일반화로 해결하고자 합니다.&lt;/p&gt;

&lt;p&gt;State의 경우, 식 (12.18)의 $\lambda$-Return 일반화에 대한 최종 정의는 식 (7.13)과 결합하여 다음과 같이 정의됩니다.&lt;/p&gt;

\[G_t^{\lambda s} \doteq \rho_t \Big( R_{t+1} + \gamma_{t+1} \big( (1 - \lambda_{t+1}) \hat{v} (S_{t+1}, \mathbf{w}) + \lambda_{t+1} G_{t+1}^{\lambda s} \big) \Big) + (1 - \rho_t) \hat{v} (S_t, \mathbf{w}_t) \tag{12.22}\]

&lt;p&gt;여기서 $\rho_t = \frac{\pi (A_t \mid S_t)}{b (A_t \mid S_t)}$는 단일 단계 Importance Sampling Ratio입니다. 이 교재에서 다루었던 다른 Return과 마찬가지로 이 최종 $\lambda$-Return은 단순히 State 기반 TD Error의 합계로 근사할 수 있습니다. 먼저 State 기반 TD Error는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\delta_t^s \doteq R_{t+1} + \gamma_{t+1} \hat{v} (S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{12.23}\]

&lt;p&gt;이것을 통해 $G_t^{\lambda s}$를 근사하면,&lt;/p&gt;

\[G_t^{\lambda s} \approx \hat{v}(S_t, \mathbf{w}_t) + \rho_t \sum_{k=t}^{\infty} \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \tag{12.24}\]

&lt;p&gt;이 때, Approximate Value Function $\hat{v}$가 변하지 않으면 식 (12.24)의 근사는 정확해집니다.&lt;/p&gt;

&lt;p&gt;식 (12.24)와 같은 $\lambda$-Return의 형태는 Forward-View Update에서 사용하기 편리해 보입니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t + \alpha \big( G_t^{\lambda s} - \hat{v} (S_t, \mathbf{w}_t) \big) \nabla \hat{v} (S_t, \mathbf{w}_t) \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \rho_t \left( \sum_{k=t}^{\infty} \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \right) \nabla \hat{v} (S_t, \mathbf{w}_t)
\end{align}\]

&lt;p&gt;위 식은 Eligibility에 기반한 TD Update처럼 보입니다. $\prod$ 연산 부분은 Eligibility Trace와 같으며, 여기에 TD Error가 곱해집니다. 그러다 이것은 Forward View의 한 단계일 뿐입니다. 우리가 찾고 있는 관계는 시간이 지남에 따라 합산되는 Forward View Update가 역시 시간이 지남에 따라 합산되는 Backward View Update와 거의 같다는 것입니다. (다만 이 관계는 Value Function의 변경을 무시하기 때문에 대략적인 것으로만 성립합니다) 시간 경과에 따른 Forward View Update의 합계는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\sum_{t=0}^{\infty} (\mathbf{w}_{t+1} - \mathbf{w}_t) &amp;amp; \approx \sum_{t=0}^{\infty} \sum_{k=t}^{\infty} \alpha \rho_t \delta_k^s \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;= \sum_{k=0}^{\infty} \sum_{t=0}^k \alpha \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \delta_k^s \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;(\text{using the summation rule : } \sum_{t=x}^y \sum_{k=t}^y = \sum_{k=x}^y \sum_{t=x}^k) \\ \\
&amp;amp;= \sum_{k=0}^{\infty} \alpha \delta_k^s \sum_{t=0}^k \rho_t \nabla (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i
\end{align}\]

&lt;p&gt;위 식의 두 번째 합계부터는 전체 표현식이 Eligibility Trace으로 작성되고 점진적으로 Update 될 수 있는 경우 Backward View TD Update의 합계 형태가 될 것입니다. 즉, 이 표현식이 시간 $k$에서의 trace이면, 시간 $k-1$의 Value에서 이것을 Update할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{z}_k &amp;amp;= \sum_{t=0}^k \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \\ \\
&amp;amp;= \sum_{t=0}^{k-1} \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i + \rho_k \nabla \hat{v}(S_k, \mathbf{w}_k) \\ \\
&amp;amp;= \gamma_k \lambda_k \rho_k \underbrace{\sum_{t=0}^{k-1} \rho_t \nabla \hat{v} (S_t, \mathbf{w}_t) \prod_{i=t+1}^{k-1} \gamma_i \lambda_i, \rho_i}_{\mathbf{z}_{k-1}} + \rho_k \nabla \hat{v} (S_k, \mathbf{w}_k) \\ \\
&amp;amp;= \rho_k \big( \gamma_k \lambda_k \mathbf{z}_{k-1} + \nabla \hat{v} (S_k, \mathbf{w}_k) \big)
\end{align}\]

&lt;p&gt;위 전개식에서는 아래 첨자가 $k$로 나와있지만, 시간에 대해 일반화하는 것을 명시하기 위해 아래 첨자를 $t$로 수정하여 최종 결과를 정리하겠습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \rho_t \big( \gamma_t \lambda_t \mathbf{z}_{t-1} + \nabla \hat{v} (S_t, \mathbf{w}_t) \big) \tag{12.25}\]

&lt;p&gt;이 Eligibility Trace는 식 (12.7)과 같은 TD($\lambda$)에 대한 일반적인 Semi-gradient 매개변수 Update 규칙과 함께 On-policy 또는 Off-policy 데이터에 적용할 수 있는 일반적인 TD($\lambda$) 알고리즘을 형성합니다. On-policy의 경우 $\rho_t$가 항상 1이므로 식 (12.25)가 식 (12.5)와 동일해지기 때문에 알고리즘은 정확히 TD($\lambda$)입니다. Off-policy의 경우 알고리즘이 잘 작동하는 경우가 많지만 Semi-gradient 방법으로는 안정성이 보장되지 않습니다. 이어지는 다음 여러 Section을 통해 안정성을 보장할 수 있는 방법을 고려할 것입니다.&lt;/p&gt;

&lt;p&gt;위와 유사한 과정을 거쳐 Action-Value에 대한 방법과 이에 해당하는 일반적인 Sarsa($\lambda$) 알고리즘에 대한 Off-policy Eligibility Trace를 얻을 수 있습니다. 전자는 식 (12.19)나 (12.20)과 같은 일반적인 Action 기반 $\lambda$-Return에 대한 재귀 방법으로 시작해야 하지만, 후자(Expected Sarsa 형식)는 더 간단합니다. 식 (12.20)에 식 (7.14)를 결합하여 다음과 같이 Off-policy로 확장하면 되기 때문입니다.&lt;/p&gt;

\[\begin{align}
G_t^{\lambda a} &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \Big( (1 - \lambda_{t+1}) \bar{V}_{t} (S_{t+1}) + \lambda_{t+1} [ \rho_{t+1} G_{t+1}^{\lambda a} + \hat{V}_t (S_{t+1}) - \rho_{t+1} \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t) ] \Big) \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} \Big( \bar{V}_t (S_{t+1}) + \lambda_{t+1} \rho_{t+1} \left[ G_{t+1}^{\lambda a} - \hat{q} (S_{t+1}, A_{t+1}, \mathbf{w}_t ) \right] \Big) \tag{12.26}
\end{align}\]

&lt;p&gt;식 (12.26)에서 $\bar{V}_t (S_{t+1})$은 식 (12.21)과 동일합니다. 그리고 또 다시  $\lambda$-Return은 TD Error의 합으로 표현할 수 있습니다.&lt;/p&gt;

\[G_t^{\lambda a} \approx \hat{q}(S_t, A_t, \mathbf{w}_t) + \sum_{k=t}^{\infty} \delta_k^a \prod_{i=t+1}^k \gamma_i \lambda_i \rho_i \tag{12.27}\]

&lt;p&gt;식 (12.27)의 $\delta_t^a$는 다음과 같이 Action 기반 TD Error의 기대 형식으로 정의됩니다.&lt;/p&gt;

\[\delta_t^a = R_{t+1} + \gamma_{t+1} \bar{V}_t (S_{t+1}) - \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{12.28}\]

&lt;p&gt;식 (12.24)와 마찬가지로, Approximate Value Function $\hat{q}$가 변하지 않으면 식 (12.27)의 근사값은 정확해집니다.&lt;/p&gt;

&lt;p&gt;State의 경우에 대한 과정과 유사한 과정을 사용하여 식 (12.27)에 기반한 Forward View Update를 유도하고, Summation Rule을 사용하여 Update의 합계를 변환한 후, 마지막으로 Action-Value에 대한 Eligibility Trace를 다음과 같이 유도할 수 있습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma_t \lambda_t \rho_t \mathbf{z}_{t-1} + \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{12.29}\]

&lt;p&gt;이 Eligibility Trace는 식 (12.28)과 같은 TD Error 및 식 (12.7)의 일반적인 Semi-gradient Update 규칙과 함께 On-policy, 또는 Off-policy에 적용할 수 있는 효율적인 Expected Sarsa($\lambda$) 알고리즘을 생성합니다. 이것은 아마도 현 시점에서 가장 좋은 알고리즘일 것입니다. (물론, 위에서 언급했듯이 아직까지는 안정성이 보장되지 않습니다.) On-policy의 경우 상수 $\lambda$와 $\gamma$, 그리고 식 (12.16)과 같은 State-Action TD Error를 사용한 알고리즘은 Section 12.7에서 제시된 Sarsa($\lambda$)와 동일합니다.&lt;/p&gt;

&lt;p&gt;$\lambda = 1$에서 이러한 알고리즘은 Monte Carlo 알고리즘과 밀접한 관련이 있습니다. Episodic Task와 Off-line Update에 대해 정확하게 동일할 것으로 생각할 수도 있지만, 실제 관계는 그것보단 약합니다. 가장 간단한 조건은 Episode별로 Update가 없으며 기대치만 있는 경우입니다. 이 방법은 Trajectory가 이어짐에 따라 (철회할 수 없는) Update를 만들지만, True Monte Carlo Method는 Trajectory가 있는 경우 Target Policy 하에 0의 확률을 가진 Action이 있을 경우 Trajectory를 Update하지 않습니다. 특히 이 모든 방법들은 $\lambda = 1$일지라도 Target이 현재 Value에 대한 추정치에 의존하기 때문에 여전히 Bootstrap합니다. 이것이 실제로 좋은지 나쁜지는 또 다른 문제입니다.&lt;/p&gt;

&lt;p&gt;관련 연구를 하나 소개하자면 (Sutton, Mahmood, Precup and van Hasselt, 2014)의 논문에서 정확한 동등성을 달성하는 방법이 제안되었습니다. 이 방법은 Update를 추적하지만, 나중에 취한 Action에 따라 철회할 수 있는 &lt;strong&gt;Provisional Weight&lt;/strong&gt;라는 추가적인 Vector를 이용합니다. 이 방법의 State 및 State-Action 방법 버전을 각각 &lt;span style=&quot;color:red&quot;&gt;PTD($\lambda$)&lt;/span&gt;와 &lt;span style=&quot;color:red&quot;&gt;PQ($\lambda$)&lt;/span&gt;라고 합니다. 여기서 ‘P’는 &lt;strong&gt;Provisional&lt;/strong&gt;의 약자입니다.&lt;/p&gt;

&lt;p&gt;하지만 이러한 새로운 Off-policy 방법의 실질적인 결과는 아직 확립되지 않았습니다. 확실한 것은, Importance Sampling을 사용하는 모든 Off-policy 방법과 마찬가지로 높은 Variance 문제가 발생할 것입니다.&lt;/p&gt;

&lt;p&gt;만약 $\lambda &amp;lt; 1$이면 모든 Off-policy 알고리즘은 Bootstrapping을 포함하고 Section 11.3에서 언급한 &lt;strong&gt;Deadly Traid&lt;/strong&gt;가 적용됩니다. 이것은 Tabular, State Aggregation 및 기타 제한된 형태의 Function Approximation에 대해서만 안정성이 보장될 수 있다는 것을 의미합니다. Linear과 같은 보다 일반적인 형태의 Function Approximation의 경우, 매개변수 Vector는 11장의 예제에서와 같이 무한대로 발산할 수 있습니다. 11장에서 논의한 바와 같이 Off-policy 학습의 과제는 크게 두 부분으로 나눌 수 있습니다. Off-policy Eligibility Trace는 문제의 첫 번째 부분은 효과적으로 처리하여 Target의 Expected Value를 추정하지만, Update의 Distribution과 관련된 두 번째 문제는 전혀 처리하지 못합니다. Eligibility Trace를 포함한 Off-policy 학습에서 두 번째 문제를 해결하기 위한 알고리즘 전략은 Section 12.11에서 보일 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;watkinss-qlambda-to-tree-backuplambda&quot;&gt;Watkins’s Q($\lambda$) to Tree-Backup($\lambda$)&lt;/h2&gt;

&lt;p&gt;Q-learning을 Eligibility Trace로 확장하기 위해 여러 방법이 제안되었습니다. 가장 처음 제안된 방법은 Watkins의 &lt;span style=&quot;color:red&quot;&gt;Q($\lambda$)&lt;/span&gt;로, Greedy Action이 수행되는 한 일반적인 방식으로 Eligibility Trace을 감소시킨 다음, 첫 번째 non-Greedy Action 후에 Trace를 0으로 줄입니다. Q($\lambda$)의 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;6장에서 Q-learning과 Expected Sarsa를 통합하여 임의의 Target Policy로 일반화했으며, 이 장의 이전 Section에서 Expected Sarsa를 Off-policy Eligibility Trace로 일반화하였습니다. 그러나 7장에서는 Importance Sampling을 사용하지 않는 속성을 유지한 $n$-step Tree Backup과 $n$-step Expected Sarsa를 구분했습니다. 이제 우리는 Tree Backup의 Eligibility Trace 버전인 &lt;span style=&quot;color:red&quot;&gt;Tree Backup($\lambda$)&lt;/span&gt;, 또는 &lt;span style=&quot;color:red&quot;&gt;TB($\lambda$)&lt;/span&gt;를 제시해야 합니다. 이는 Off-policy 데이터에 적용할 수 있음에도 불구하고 Importance Sampling이 없다는 장점이 있기 때문에 Q-learning의 진정한 확장이라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;TB($\lambda$)의 개념은 간단합니다. Section 7.5에서와 같이 Tree Backup의 Update는 Bootstrapping Parameter $\lambda$에 따라 일반적인 방식으로 Weight가 부여됩니다. TB($\lambda$)의 Backup Diagram은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반적인 Bootstrapping 및 Discounting Parameter에 대한 올바른 Index를 사용하여 구체적인 방정식을 얻으려면, 다음과 같이 식 (12.20) $\lambda$-Return의 재귀 형식으로 시작한 다음, 식 (7.16) Target의 Bootstrapping 경우로 확장하는 것이 가장 좋습니다.&lt;/p&gt;

\[\begin{align}
G_t^{\lambda a} &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \Big( (1 - \lambda_{t+1}) \bar{V}_t (S_{t+1}) + \lambda_{t+1} \big[ \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) \hat{q} (S_{t+1}, a, \mathbf{w}_t) + \pi (A_{t+1} | S_{t+1}) G_{t+1}^{\lambda a} \big] \Big) \\ \\
&amp;amp;= R_{t+1} + \gamma_{t+1} \Big( \bar{V}_t (S_{t+1}) + \lambda_{t+1} \pi (A_{t+1} | S_{t+1}) \big( G_{t+1}^{\lambda a} - \hat{v} (S_{t+1}, A_{t+1}, \mathbf{w}_t) \big) \Big)
\end{align}\]

&lt;p&gt;이전과 마찬가지로, $G_t^{\lambda a}$를 TD Error의 합으로 근사할 수도 있습니다. 이 때 TD Error는 식 (12.28)과 같은 형태를 사용합니다.&lt;/p&gt;

\[G_t^{\lambda a} \approx \hat{q} (S_t, A_t, \mathbf{w}_t) + \sum_{k=t}^{\infty} \delta_k^a \prod_{i=t+1}^k \gamma_i \lambda_i \pi (A_i | S_i)\]

&lt;p&gt;이전 Section과 동일한 단계에 따라, 선택한 Action의 Target Policy 확률과 관련된 특별한 Eligibility Trace Update를 유도할 수 있습니다.&lt;/p&gt;

\[\mathbf{z}_t \doteq \gamma_t \lambda_t \pi (A_t | S_t) \mathbf{z}_{t+1} + \nabla \hat{q}(S_t, A_t, \mathbf{w}_t)\]

&lt;p&gt;이것은 식 (12.7)과 같은 일반적인 매개변수 Update 규칙과 함께 TB($\lambda$) 알고리즘을 정의합니다. 모든 Semi-gradient 알고리즘과 마찬가지로 TB($\lambda$)는 Off-policy 데이터와 강력한 Function Approximation 방법과 함께 사용했을 때 안정성이 보장되지 않습니다. 안정성을 보장받기 위해서는 TB($\lambda$)를 다음 Section에 나오는 방법 중 하나와 결합해야 합니다.&lt;/p&gt;

&lt;h2 id=&quot;stable-off-policy-methods-with-traces&quot;&gt;Stable Off-policy Methods with Traces&lt;/h2&gt;

&lt;p&gt;Eligibility Trace를 사용한 Off-policy 학습에서 안정성을 보장하기 위한 여러 방법이 제안되었습니다. 여기에서는 일반적인 Bootstrapping 및 Discount Function을 포함한 네 가지 방법을 제시합니다. 이 방법들은 모두 Section 11.7과 11.8에 제시한 Gradient-TD 또는 Emphatic-TD의 아이디어에 기반하고 있습니다. 모든 알고리즘은 Linear Function Approximation를 사용한다고 가정하지만, non-Linear Function Approximation에 대한 확장도 여러 논문에서 찾을 수 있습니다.&lt;/p&gt;

&lt;p&gt;첫 번째 방법으로 &lt;span style=&quot;color:red&quot;&gt;GTD($\lambda$)&lt;/span&gt;는 TDC와 유사한 Eligibility Trace 알고리즘으로, Section 11.7에서 제시한 두 가지 State-Value Gradient TD 예측 알고리즘보다 우수합니다. 이 알고리즘의 목표는 Behavior Policy $b$를 따르는 데이터를 위해 $\hat{v} (s, \mathbf{w}) \doteq \mathbf{w}_t^{\sf T} \mathbf{x} (s) \approx v_{\pi}(s)$와 같은 식에서 매개변수 $\mathbf{w}_t$를 학습하는 것입니다. 이 방법의 Update 식은 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t^s \mathbf{z}_t - \alpha \gamma_{t+1} (1 - \lambda_{t+1}) (\mathbf{z}_t^{\sf T} \mathbf{v}_t) \mathbf{x}_{t+1}\]

&lt;p&gt;위 식에서 $\delta_t^s$는 식 (12.23), $\mathbf{z}$는 식 (12.25), $\rho_t$는 식 (11.1)과 같습니다. 그리고 $\mathbf{v}_t$는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\mathbf{v}_{t+1} \doteq \mathbf{v}_t + \beta \delta_t^s \mathbf{z}_t - \beta (\mathbf{v}_t^{\sf T} \mathbf{x}_t) \mathbf{x}_t \tag{12.30}\]

&lt;p&gt;Section 11.7에서와 같이 $\mathbf{v} \in \mathbb{R}^d$는 $\mathbf{w}$와 같은 차원의 Vector이고, $\mathbf{v} = \mathbf{0}$으로 초기화됩니다. 그리고 $\beta &amp;gt; 0$은 두 번째 Step-size Parameter입니다.&lt;/p&gt;

&lt;p&gt;두 번째 방법인 &lt;span style=&quot;color:red&quot;&gt;GQ($\lambda$)&lt;/span&gt;는 Eligibility Trace가 포함된 Action-Value에 대한 Gradient-TD 알고리즘입니다. 이 알고리즘의 목표는 Off-policy 데이터에서 $\hat{q} (s, a, \mathbf{w}_t) \doteq \mathbf{w}_t^{\sf T} \mathbf{x}(s, a) \approx q_{\pi} (s, a)$와 같은 식의 매개변수 $\mathbf{w}_t$를 학습하는 것입니다. 만약 Target Policy가 $\epsilon$-greedy인 경우, 또는 $\hat{q}$에 대한 Greedy Policy로 Bias되는 경우 GQ($\lambda$)를 Control 알고리즘으로 사용할 수 있습니다. 이 방법의 Update는 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w} + \alpha \delta_t^a \mathbf{z}_t - \alpha \gamma_{t+1} (1 - \lambda_{t+1}) (\mathbf{z}_t^{\sf T} \mathbf{v}_t) \bar{\mathbf{x}}_{t+1}\]

&lt;p&gt;위 식에서 $\bar{\mathbf{x}}_t$는 Target Policy를 따르는 $S_t$에 대한 평균 Feature Vector로 정의됩니다.&lt;/p&gt;

\[\bar{\mathbf{x}}_t \doteq \sum_a \pi (a | S_t) \mathbf{x} (S_t, a)\]

&lt;p&gt;또한 $\delta_t^a$는 다음과 같은 TD Error로 정의됩니다.&lt;/p&gt;

\[\delta_t^a \doteq R_{t+1} + \gamma_{t+1} \mathbf{w}_t^{\sf T} \bar{\mathbf{x}}_{t+1} - \mathbf{w}_t^{\sf T} \mathbf{x}_t\]

&lt;p&gt;$\mathbf{z}_t$는 식 (12.29)와 동일하게 정의되고, $\mathbf{v}_t$의 Update를 포함한 나머지는 GTD($\lambda$)와 동일합니다.&lt;/p&gt;

&lt;p&gt;세 번째로 &lt;span style=&quot;color:red&quot;&gt;HTD($\lambda$)&lt;/span&gt;는 GTD($\lambda$)와 TD($\lambda$)를 결합한 State-Value 알고리즘입니다. 이 알고리즘의 가장 큰 장점은 TD($\lambda$)를 Off-policy 학습으로 엄격하게 일반화한다는 것입니다. &lt;strong&gt;엄격하게&lt;/strong&gt; 라는 의미는 Behavior Policy가 Target Policy와 같게 되면 HTD($\lambda$)가 TD($\lambda$)와 동일하게 된다는 뜻입니다. (GTD($\lambda$)는 그렇게 되지 않습니다.) 보통 TD($\lambda$)가 GTD($\lambda$)보다 빠르게 수렴하기 때문에 이 장점은 매력적입니다. 또한 TD($\lambda$)는 단일 Step-size Parameter만 필요하다는 장점도 있습니다. HTD($\lambda$)는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_t + \alpha \delta_t^s \mathbf{z}_t + \alpha \big( (\mathbf{z}_t - \mathbf{z}_t^b)^{\sf T} \mathbf{v}_t \big) (\mathbf{x}_t - \gamma_{t+1} \mathbf{x}_{t+1}) \\ \\
\mathbf{v}_{t+1} &amp;amp; \doteq \mathbf{v}_t \beta \delta_t^s \mathbf{z}_t - \beta \Big( \mathbf{z}_t^{b^{\sf T}} \mathbf{v}_t \Big) (\mathbf{x}_t - \gamma_{t+1} \mathbf{x}_{t+1}) \quad \text{with} \quad \mathbf{v}_0 \doteq \mathbf{0} \\ \\
\mathbf{z}_t &amp;amp; \doteq \rho_t \big( \gamma_t \lambda_t \mathbf{z}_{t-1} + \mathbf{x}_t \big) \quad \text{with} \quad \mathbf{z}_{-1} \doteq \mathbf{0} \\ \\
\mathbf{z}_t^b &amp;amp; \doteq \gamma_t \lambda_t \mathbf{z}_{t-1}^b + \mathbf{x}_t \quad \text{with} \quad \mathbf{z}_{-1}^b \doteq \mathbf{0}
\end{align}\]

&lt;p&gt;위 식에서 $\beta &amp;gt; 0$은 두 번째 Step-size Parameter입니다. 또한 두 번째 Weight 집합인 $\mathbf{v}_t$ 외에도 HTD($\lambda$)는 두 번째 Eligibility Trace 집합인 $\mathbf{z}_t^b$가 있습니다. 이것들은 Behavior Policy에 대한 누적된 Eligibility Trace이며, 모든 $\rho_t$가 1이면 $\mathbf{w}_t$ Update의 마지막 항이 0이 되면서 $\mathbf{z}_t$와 같아집니다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;span style=&quot;color:red&quot;&gt;Emphatic TD($\lambda$)&lt;/span&gt;는 1-step Emphatic-TD 알고리즘을 Eligibility Trace로 확장한 것입니다. (Section 9.11과 11.8 참고) 결과적으로 이 알고리즘은 강력한 Off-policy 수렴을 보장하면서 어느 정도 Bootstrapping도 가능하게 하지만, 높은 Variance를 가지고 수렴 속도가 느리다는 단점이 있습니다. Emphatic TD($\lambda$)는 다음과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t \\ \\
\delta_t &amp;amp; \doteq R_{t+1} + \gamma_{t+1} \mathbf{w}_t^{\sf T} \mathbf{x}_{t+1} - \mathbf{w}_t^{\sf T} \mathbf{x}_t \\ \\
\mathbf{z}_t &amp;amp; \doteq \rho_t \left( \gamma_t \lambda_t \mathbf{z}_{t-1} + M_t \mathbf{x}_t \right) \quad \text{with} \quad \mathbf{z}_{-1} \doteq \mathbf{0} \\ \\
M_t &amp;amp; \doteq \lambda_t I_t + \left( 1 - \lambda_t \right) F_t \\ \\
F_t &amp;amp; \doteq \rho_{t-1} \gamma_t F_{t-1} + I_t \quad \text{with} \quad F_0 \doteq i (S_0)
\end{align}\]

&lt;p&gt;위 식에서 $M_t \ge 0$은 &lt;strong&gt;Emphasis&lt;/strong&gt;의 일반적인 형태이고, $F_t \ge 0$은 &lt;span style=&quot;color:red&quot;&gt;Followon Trace&lt;/span&gt;라고 하며, $I_t \ge 0$은 Section 11.8에서 설명한 &lt;strong&gt;Interest&lt;/strong&gt;입니다. 이 알고리즘의 중요한 점은 $\delta_t$와 같이 $M_t$ 또한 공간 복잡도를 높이지 않는다는 것입니다. 이 식의 정의를 Eligibility Trace 식에 대입하여 처리가 가능합니다. Emphatic TD($\lambda$)의 True On-line 버전에 대한 Pseudocode 및 프로그램은 (Sutton, 2015b) 논문에서 확인이 가능합니다.&lt;/p&gt;

&lt;p&gt;On-policy의 경우 (즉, 모든 $t$에 대해 $\rho_t = 1$) Emphatic TD($\lambda$)는 기존 TD($\lambda$)와 유사하지만 차이점도 많습니다. 예를 들어, Emphatic TD($\lambda$)는 모든 State에 종속적인 $\lambda$ 함수에 대해 수렴이 보장되지만, TD($\lambda$)는 그렇지 않습니다. TD($\lambda$)는 모든 상수 $\lambda$에 대해서만 수렴이 보장됩니다. 이에 대한 반례는 (Ghiassian, Rafiee, and Sutton, 2016) 논문을 참고해주시기 바랍니다.&lt;/p&gt;

&lt;h2 id=&quot;implementation-issues&quot;&gt;Implementation Issues&lt;/h2&gt;

&lt;p&gt;처음에는 Eligibility Trace를 사용하는 Tabular 방법이 1-step 방법보다 복잡해 보일 수도 있습니다. 단순한 구현은 모든 State(또는 State-Action 쌍)가 모든 시간 단계에서 Estimated Value와 Eligibility Trace를 모두 Update해야 합니다. 이것은 단일 명령, 다중 데이터, 병렬 컴퓨터 또는 Artificial Neural Network에서 구현하는 경우 문제가 되지 않지만, 기존의 직렬 컴퓨터에서 구현하는 경우에는 문제가 될 수 있습니다. 다행히도 일반적인 $\lambda$와 $\gamma$에 대해 거의 모든 State에서의 Eligibility Trace는 항상 0에 가깝습니다. 최근에 방문한 State에서만 0보다 훨신 큰 Trace가 있을 것이기 때문에 이러한 몇 개의 State만 Update하여 구현함으로써 간단하게 알고리즘을 근사적으로 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;실제로 이러한 방법을 사용하면 기존 컴퓨터로도 0보다 훨씬 큰 일부 Trace만 Update함으로써 구현할 수 있습니다. 이러한 꼼수를 사용하면 Tabular 방법에서 Trace를 사용하는 계산 비용이 일반적인 1-step 방법의 몇 배에 불과합니다. 물론 정확한 배수는 $\lambda$ 및 $\gamma$의 값과 다른 계산 비용에 따라 달라집니다. Tabular 방법의 경우는 어떤 의미에서 Eligibility Trace의 최악의 계산 복잡도를 가지고 있습니다. Function Approximation을 사용할 때는 Function Approximation 방법 자체의 계산 복잡도가 높기 때문에 Eligibility Trace를 사용하지 않는 것과 크게 차이가 나지 않기 때문입니다. 예를 들어, Artificial Neural Network 및 Backpropagation Algorithm을 사용하는 경우 Eligibility Trace를 추가해도 각 단계별로 필요한 메모리나 계산량이 두 배 정도만 늘어납니다. Section 12.3의 Truncated $\lambda$-Return 방법은 항상 추가적인 메모리 용량이 필요하지만, 기존 컴퓨터에서 계산적으로 효율적인 구현이 가능하기도 합니다.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;TD Error를 사용하는 Eligibility Trace는 Monte Carlo와 TD 방법의 중간 지점을 선택할 수 있는 효율적이고 점진적인 방법을 제공합니다. 7장의 $n$-step TD 방법도 이것을 가능하게 했지만 Eligibility Trace 방법은 더 일반적이고 더 빨리 학습할 수도 있으며 Trade-off를 통해 다른 계산 복잡성을 가질 수도 있습니다. 이번 장에서는 On-policy와 Off-policy 학습에서 Variable Bootstrapping 및 Discounting을 위해 Eligibility Trace에 대한 새로운 이론을 제시했습니다. 이 이론의 하나로써 기존 TD 방법의 계산 복잡도를 유지한 채 이상적인 방법의 Action을 정확하게 재현하는 True On-line 방법이 있습니다. 또 다른 것으로는 직관적인 Forward View 방법에서 보다 계산적으로 효율적인 Backward View로 전환할 수 있는 가능성입니다. 이제 고전적이고 계산량이 많은 Monte Carlo 알고리즘으로 시작하여 True On-line TD 방법에 사용한 것과 동일한 Eligibility Trace를 사용하여 계산량이 적은 Incremental non-TD를 구현함으로써 이 일반적인 아이디어를 설명했습니다.&lt;/p&gt;

&lt;p&gt;5장에서 언급했듯이 Monte Carlo Method은 Bootstrap하지 않기 때문에 non-Markov Process에서 이점이 있을 수 있습니다. Eligibility Trace는 TD 방법을 Monte Carlo Method와 유사하게 만들기 때문에 이런 경우에도 이점을 가질 수 있습니다. 예를 들어, TD 방법의 장점으로 인해 이것을 사용하고 싶지만, 일부 작업이 non-Markov인 경우 Eligibility Trace를 도입함으로써 이 문제를 해결할 수 있습니다. Eligibility Trace는 장기간의 지연된 Reward와 non-Markov Process 모두에 대한 해결 방법을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;$\lambda$의 값을 조절하여 Monte Carlo에서 1-step TD 방법에 이르기까지 Eligibility Trace를 어디에나 사용할 수 있습니다. 그렇다면 어느 단계에서 사용하는 것이 가장 좋을까요? 안타깝게도 이 질문에 대한 명확한 이론적인 답이 없습니다. 대신 경험적인 답으로써, Episode당 단계가 많거나 Discounting이 반감기 내에 단계가 많은 작업에서 Eligibility Trace를 사용하는 것이 더 좋다고 판단됩니다. 아래의 그래프는 $\lambda$에 따른 강화학습의 성능을 나타내고 있는데, 이것을 통해 대략적인 답을 낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/12. Eligibility Traces/RL 12-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;반면에 순수한 Monte Carlo Method에 가까워지면 성능이 급격히 저하됩니다. 그렇기 때문에 적당히 중간 정도의 Step이 최선의 선택이라고 볼 수 있습니다. 미래에는 $\lambda$를 사용하여 TD와 Monte Carlo Method 간 Trade-off를 더 미세하게 조절하는 것이 가능할 수도 있겠지만, 현재로서는 이것을 어떻게 안정적이고 유용하게 사용할 수 있을지 명확한 결론을 내릴 수가 없습니다.&lt;/p&gt;

&lt;p&gt;Eligibility Trace를 사용하게 되면 1-step 방법보다 더 많은 계산이 필요하지만, 그 대가로 Reward가 여러 단계로 지연되는 경우 훨씬 더 빠른 학습 속도를 제공합니다. 따라서 On-line과 같이 데이터가 부족하고 반복적으로 처리할 수 없는 경우에는 Eligibility Trace를 사용하는 것이 좋습니다. 반면에, 시뮬레이션을 통해 데이터를 쉽게 생성할 수 있는 Off-line의 경우에는 Eligibility Trace를 사용하는데 큰 이점이 없는 경우가 많습니다. 이 때의 목표는 제한된 데이터에서 더 많은 것을 얻는 것이 아니라 가능한 한 빠르게 많은 데이터를 처리하는 것인데, Eligibility Trace로 인한 데이터의 속도 향상은 그만한 계산 비용를 소모할 가치가 없기 때문에 1-step 방법이 선호됩니다.&lt;/p&gt;

&lt;p&gt;이번 장은 내용이 길고 어려워서 그런지 깔끔하게 포스트를 작성하지 못한 것 같네요. 포스트를 먼저 게시한 다음 나중에 다시 읽어보며 조금씩 매끄럽게 수정하겠습니다.&lt;/p&gt;

&lt;p&gt;다음 장은 강화학습의 마지막 장인 Policy Gradient Methods입니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장에서 새로 배우는 Eligibility Trace는 강화학습의 기본 메커니즘 중 하나입니다. 예를 들어, TD($\lambda$)에서 $\lambda$는 Eligibility Trace를 사용한다는 것을 의미합니다. Q-learning과 Sarsa를 포함한 대부분의 TD 방법은 Eligibility Trace와 결합하여 보다 효율적으로 학습할 수 있습니다.</summary></entry><entry><title type="html">Off-policy Methods with Approximation</title><link href="http://localhost:4000/studies/off-policy-methods-with-approximation/" rel="alternate" type="text/html" title="Off-policy Methods with Approximation" /><published>2022-06-03T00:00:00+09:00</published><updated>2022-06-03T00:00:00+09:00</updated><id>http://localhost:4000/studies/off-policy-methods-with-approximation</id><content type="html" xml:base="http://localhost:4000/studies/off-policy-methods-with-approximation/">&lt;p&gt;이 책은 5장 이후로 Generalized Policy Iteration (GPI)에서 내재된 Exploitation과 Exploration 사이의 Trade-off를 처리하는 방법으로 On-policy와 Off-policy를 사용했습니다. 9장과 10장에서는 On-policy의 경우를 Function Approximation로 처리했으며, 이번 장에서는 Off-policy에서의 Function Approximation을 다룰 예정입니다. Off-policy 방법을 Function Approximation로 확장하는 것은 On-policy의 경우에서와 다른 점도 많고 어려운 점도 많습니다.&lt;/p&gt;

&lt;p&gt;6장과 7장에서 소개한 Tabular 형식의 Off-policy 방법은 Semi-gradient 알고리즘으로 확장하기 쉽지만, On-policy의 경우처럼 강력하게 수렴하지 않습니다. 이번 장에서는 Linear Function Approximation을 통해 Learnability의 개념을 소개한 다음, Off-policy의 수렴 문제와 더 강력한 수렴 보장을 가진 새로운 알고리즘에 대해 논의하겠습니다. 안타깝게도 새로운 알고리즘은 On-policy의 경우에서보다 이론적으로도, 경험적으로도 강력하지 않습니다. 이 과정에서 Off-policy 학습 뿐만 아니라 On-policy 학습에 대해서도 더 깊게 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Off-policy는 Target Policy $\pi$와 Behavior Policy $b$로 분리하여 Value Function을 학습합니다. Value를 추정할 때 두 Policy는 모두 정적으로 주어졌으며, State-Value $\hat{v} \approx v_{\pi}$나 Action-Value $\hat{q} \approx q_{\pi}$ 중 하나를 학습했습니다. Control에서는 Action-Value를 학습하고 $\pi$는 Greedy Policy, $b$는 $\epsilon$-greedy와 같은 탐색적인 Policy로 변경됩니다.&lt;/p&gt;

&lt;p&gt;Off-policy 학습은 크게 두 부분으로 나눌 수 있습니다. 하나는 Tabular 경우에서 발생하고, 다른 하나는 Function Approximation에서만 나타납니다. 첫 번째 부분은 Update Target ($\ne$ Target Policy)과 관련이 있고 두 번째 부분은 Update Distribution과 관련이 있습니다. 첫 번째 부분은 5장과 7장에서 다루었던 Importance Sampling과 관련이 있습니다. Importance Sampling은 Variance를 증가시킬 수 있지만 필수불가결합니다. 첫 번째 부분에서는 Function Approximation에서 이것의 확장을 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;On-policy Distribution은 Semi-gradient 방법의 안정성을 위해 중요했습니다. 하지만 Off-policy의 경우에는 Update Distribution이 On-policy Distribution을 따르지 않기 때문에 이와 다른 접근 방식이 필요합니다. 첫 번째로 생각해볼 수 있는 방법은 Importance Sampling을 사용함으로써 Update Distribution을 On-policy Distribution으로 변형하여 Semi-gradient 방법이 수렴되도록 보장하는 것입니다. 두 번째로는 특별한 Distribution에 의존하지 않는 새로운 Gradient 방법을 개발하는 것입니다. 두 번째 부분에서는 이것에 대해 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;이 부분에서 다루는 내용은 아직 연구가 진행중인 부분이기 때문에 실제로 무엇이 가장 효과적인지는 확실하게 언급할 수 없습니다.&lt;/p&gt;

&lt;h2 id=&quot;semi-gradient-methods&quot;&gt;Semi-gradient Methods&lt;/h2&gt;

&lt;p&gt;가장 먼저 이전 장에서 배운 Semi-gradient 방법으로 Off-policy를 Function Approximation으로 확장하는 것으로 시작하겠습니다. 이 방법은 Off-policy에서 첫 번째 부분(Update Target 변경)을 해결하지만 두 번째 부분(Update Distribution 변경)은 해결하지 않습니다. 따라서 이 방법은 일부 경우에서 발산할 위험이 있지만, 일반적으로는 성공적으로 사용할 수 있습니다. 이 방법은 Tabular에서 안정적이고 점근적으로 Bias되지 않지만, Function Approximation에서만 발생하는 문제입니다. 이러한 단점에도 불구하고 이 방법은 간단하기 때문에 첫 주제로 다루는 것이 좋습니다.&lt;/p&gt;

&lt;p&gt;Off-policy 알고리즘을 Semi-gradient 방법으로 변환하려면 Approximate Value Function($\hat{v}$또는 $\hat{q}$)와 Gradient를 이용하여 배열($V$ 또는 $Q$)에 대한 Update를 Weight Vector $\mathbf{w}$에 대한 Update로 바꾸면 됩니다. 이러한 알고리즘은 다음과 같은 단계별 Importance Sampling Ratio를 사용합니다.&lt;/p&gt;

\[\rho_t \doteq \rho_{t:t} = \frac{\pi (A_t | S_t)}{b(A_t | S_t)}\]

&lt;p&gt;예를 들어, 1-step State-Value 알고리즘은 $\rho_t$가 추가된 것만 제외하면 On-policy 알고리즘과 같은 &lt;span style=&quot;color:red&quot;&gt;Semi-gradient Off-policy TD(0)&lt;/span&gt;입니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \rho_t \delta_t \nabla \hat{v}(S_t, \mathbf{w}_t) \tag{11.2}\]

&lt;p&gt;여기서 $\delta_t$는 문제가 어떻게 정의되었는지에 따라 다릅니다. 만약 Epsidoic이고 Discounted를 사용한다면 다음 식 (11.3)처럼, Continuing이고 Average Reward를 사용한다면 식 (11.4)과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\delta_t &amp;amp; \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{11.3} \\ \\
\delta_t &amp;amp; \doteq R_{t+1} - \bar{R}_t + \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{11.4}
\end{align}\]

&lt;p&gt;만약 Action-Value인 경우라면, 1-step 알고리즘은 다음과 같은 &lt;span style=&quot;color:red&quot;&gt;Semi-gradient Expected Sarsa&lt;/span&gt;입니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp; \doteq \mathbf{w}_t + \alpha \delta_t \nabla \hat{q} (S_t, A_t, \mathbf{w}_t), \tag{11.5} \\ \\
\delta_t &amp;amp; \doteq R_{t+1} + \gamma \sum_a \pi(a|S_{t+1}) \hat{q} (S_{t+1}, a, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \tag{episodic} \\ \\
\delta_t &amp;amp; \doteq R_{t+1} - \bar{R}_t + \sum_a \pi (a|S_{t+1}) \hat{q}(S_{t+1}, a, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \tag{continuing}
\end{align}\]

&lt;p&gt;이 알고리즘은 Importance Sampling을 사용하지 않습니다. Tabular의 경우 유일한 Sample Action이 $A_t$이기 때문에 다른 Action을 고려할 필요가 없으므로 적절합니다. 만약 Function Approximation을 사용하면 서로 다른 State-Action 쌍이 서로 다른 Weight를 적용하면서 동일한 근사값에 반영될 수 있으므로 애매합니다. 이 문제를 해결하기 위해서는 강화학습에서 Function Approximation 이론에 대해 조금 더 깊게 파고들 필요가 있습니다.&lt;/p&gt;

&lt;p&gt;이 알고리즘을 $n$-step 버전으로 일반화할 때 State-Value 및 Action-Value는 모두 Importance Sampling을 포함합니다. Semi-gradient Sarsa의 $n$-step 버전은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+n} &amp;amp; \doteq \mathbf{w}_{t+n} + \alpha \rho_{t+1} \cdots \rho_{t+n} \left[ G_{t:t+n} - \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_{t+n-1}) \tag{11.6} \\ \\
G_{t:t+n} &amp;amp; \doteq R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}) \tag{episodic} \\ \\
G_{t:t+n} &amp;amp; \doteq R_{t+1} - \bar{R}_t + \cdots + R_{t+n} - \bar{R}_{t+n-1} + \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}) \tag{continuing}
\end{align}\]

&lt;p&gt;이 때, Episode의 마지막을 처리하는 방법은 이전에 배운 $n$-step 버전과 약간 다릅니다. 식 (11.6)에서 $k \ge T$에 해당하는 $\rho_k$는 모두 1로 처리하고, $t+n \ge T$에 해당하는 $G_{t:t+n}$은 모두 $G_t$로 처리합니다.&lt;/p&gt;

&lt;p&gt;또한 7장에서 Importance Sampling을 사용하지 않은 Off-policy 알고리즘인 $n$-step Tree-backup 알고리즘도 배운 적이 있습니다. 이것을 Semi-gradient 버전으로 바꾸면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+n} &amp;amp; \doteq \mathbf{w}_{t+n-1} + \alpha \left[ G_{t:t+n} - \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_{t+n-1}) \tag{11.7} \\ \\
G_{t:t+n} &amp;amp; \doteq \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) + \sum_{k=t}^{t+n-1} \delta_k \prod_{i=t+1}^k \gamma \pi (A_i | S_i) \tag{11.8}
\end{align}\]

&lt;p&gt;식 (11.8)의 $\delta_t$는 위의 Expected Sarsa 부분에 나온 식과 동일합니다. 이 외에도 7장에서는 모든 Action-Value 알고리즘을 통합하는 $n$-step $Q(\sigma)$를 정의했습니다만, 이 식의 Semi-gradient 변형은 생략하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;examples-of-off-policy-divergence&quot;&gt;Examples of Off-policy Divergence&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 Off-policy 학습의 두 번째 부분에 대해 다루겠습니다. 가장 먼저, 지난 Section 첫 부분에서 언급했던 Off-policy 학습에서 생길 수 있는 문제점을 알아보겠습니다. 특히, Semi-gradient나 다른 알고리즘이 불안정하고 발산하는 경우입니다.&lt;/p&gt;

&lt;p&gt;Off-policy가 어떤 상황에서 문제가 발생하는지 직관적으로 알기위해 간단한 예시를 하나 소개하겠습니다. MDP 중 일부에서 다음과 같은 2개의 State가 있다고 가정합니다. 여기서 매개변수 Vector $\mathbf{w}$는 단일 구성 요소 $w$로만 구성됩니다. 만약 두 State에 대한 Feature Vector가 각각 단순한 숫자로 이루어진 경우, Linear Function Approximation을 할 수 있습니다. 여기서는 각 State의 Value를 $w$와 $2w$로 설정합니다. 첫 번째 State에서는 Reward 0을 받고 두 번째 State로 이동하는 단 한 개의 Action만 존재합니다. 이것을 도식화하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 간단한 예제에서 어떻게 발산이 발생하는지 계산해보겠습니다. 먼저 초기에 $w = 10$이라고 가정합니다. 그러면 Estimated Value가 10인 State에서 Estimated Value가 20인 State로 이동합니다. Value가 상승했으므로 좋은 Action으로 판단되기 때문에, $w$는 첫 번째 State의 Estimated Value를 높이기 위해 증가합니다. $\gamma$가 1에 가까우면 TD Error는 10에 가까워질 것이고, 만약 $\alpha = 0.1$이면 TD Error를 줄이기 위해 $w$는 11에 가깝게 증가시킬 것입니다. 문제는 두 번째 State가 $2w$로 근사되었기 때문에 동시에 22 정도가 증가한다는 것입니다. 그렇게 되면 TD Error가 오히려 11로 증가합니다. 따라서 첫 번째 State의 Value가 다시 증가하며, 이전보다 더 많은 수치인 12.1 정도가 증가합니다. 이후로는 역시 두 번째 State의 Value도 증가하며, 결국 $w$는 무한대로 발산합니다.&lt;/p&gt;

&lt;p&gt;이 원인을 알아보기 위해 두 State의 Update 순서를 자세히 알아보겠습니다. 먼저 두 State 사이의 Transition에 대한 TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t = R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w}) = 0 + \gamma 2 w_t - w_t = (2 \gamma - 1) w_t\]

&lt;p&gt;그리고 식 (11.2)에 의한 Off-policy Semi-gradient TD(0)의 Update를 사용하면,&lt;/p&gt;

\[w_{t+1} = w_t + \alpha \rho_t \delta_t \nabla \hat{v} (S_t, w_t) = w_t + \alpha \cdot 1 \cdot (2 \gamma - 1) w_t \cdot 1 = \left(1 + \alpha (2 \gamma - 1) \right) w_t\]

&lt;p&gt;이 예제에서 Importance Sampling Ratio $\rho_t$는 첫 번째 State에서 선택할 수 있는 Action이 1개뿐이기 때문에 1로 고정됩니다. (Target Policy와 Behavior Policy가 같을 수밖에 없으므로) 또한 이 Update 식에서 주목할 점은 $w_{t+1}$가 $w_t$에 $1 + \alpha (2 \gamma - 1)$을 곱한 값이라는 것입니다. 이 값이 1보다 크면 초기 값 $w$이 무엇인지에 따라 결국 양의 무한대나 음의 무한대로 발산하게 되므로, 시스템이 불안정할 수밖에 없습니다. 이 경우에는 만약 $\gamma$가 0.5보다 큰 경우에 발산합니다. 이것으로부터 알 수 있는 점은 Update의 안정성은 Step-size Parameter $\alpha$와는 무관하다는 것입니다. $\alpha$의 크기에 따라 발산하는 속도에 영향을 끼칠지는 몰라도, 결국 발산하는가 수렴하는가 자체에는 영향이 없습니다.&lt;/p&gt;

&lt;p&gt;이 예제에서 발산이 발생하는 근본적인 이유는 $w$가 다른 State로 Transition되지 않는 이상 같은 Transition이 반복해서 일어나기 때문입니다. 만약 선택할 수 있는 다른 Action이 있다면, Off-policy는 Target Policy가 하지 않을 Action을 Behavior Policy가 선택할 수 있기 때문에 이 문제를 회피할 수 있습니다. 이 경우 $\rho_t$는 0이 되고 Update가 발생하지 않습니다. 하지만 On-policy에서 $\rho_t$는 항상 1입니다. State $w$에서 State $2w$로 Transition이 일어날 때마다 $w$가 증가하면, State $2w$를 벗어다는 Transition 또한 있어야 합니다. 만약 그 Transition이 $2w$보다 높은 State가 아니라면 $w$를 감소시키고, 그 State 뒤에 더 높은 Value의 State가 와야 하며, 그렇지 않으면 다시 $w$가 감소합니다. 각각의 State는 더 높은 기대값을 생성해야만 이전 State가 발산하지 않습니다.&lt;/p&gt;

&lt;p&gt;다만 이 예제는 전체 MDP의 일부만 놓고 가정한 예시입니다. 이제 따져봐야 할 것은 예제와 같이 불안정하고 완전한 MDP 시스템이 실제로 존재할 수 있는지에 대한 여부입니다. 결론부터 말하자면 그런 시스템은 존재하며, &lt;span style=&quot;color:red&quot;&gt;Baird’s Counterexample&lt;/span&gt;이라는 이름으로 알려져 있습니다. 아래 그림과 같이 7개의 State와 2개의 Action이 있는 Episodic MDP가 있습니다. 점선으로 표시된 동작은 동일한 확률로 6개의 State 중 하나로 Action할 수 있다는 뜻이고, 실선으로 표시된 동작은 모두 맨 아래의 7번째 State로 이동합니다. Behavior Policy $b$는 확률 $\frac{6}{7}$로 점선 Action, 확률 $\frac{1}{7}$로 실선의 Action을 선택합니다. 이 확률 분포는 각 Episode의 시작 분포이기도 합니다. Target Policy $\pi$는 항상 실선 Action만 선택하여 7번째 State로만 이동하려고 합니다. Reward는 모든 Transition에서 0입니다. Discount Factor는 $\gamma = 0.99$로 정의됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각각의 State에 나타난 식은 Linear Parameter를 사용하여 State-Value를 추정한 것입니다. $w$ 아래에 있는 첨자는 Weight Vector $\mathbf{w} \in \mathbb{R}^8$의 몇 번째 요소인지 표시한 것입니다. 그러므로 맨 왼쪽의 State를 Feature Vector로 표현하면 $\mathbf{x} = (2,0,0,0,0,0,0,1)^{\sf T}$입니다. Reward는 모든 Transition에서 0으로 정의되어 있으므로 실제 Value Function은 모든 State $s$에 대해 $v_{\pi} = 0$입니다.  따라서 Weight Vector가 $\mathbf{w} = \mathbf{0}$에 수렴한다면 정확하게 근사가 가능합니다. (다만 이전 예제와 달리 구성 요소가 많기 때문에 이 외에도 더 많은 해법이 존재합니다) 또한 Feature Vector의 집합 $\{ \mathbf{x}(s) : s \in \mathcal{S} \}$는 Linear Independent 집합입니다. 이렇게 보았을 때 이 문제는 Linear Function Approximation에 적합한 것처럼 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 이 문제에 식 (11.2)와 같은 Semi-gradient TD(0)를 적용하면 위의 그림의 왼쪽과 같이 Weight가 무한대로 발산합니다. 심지어 그림의 오른쪽처럼 Expected Update를 이용한 Dynamic Programming을 사용해도 마찬가지로 발산합니다. 이 때 Weight Vector $\mathbf{w}$가 DP Target을 사용하여 Semi-gradient 방법으로 동시에 Update 되는 방식은 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{k+1} \doteq \mathbf{w}_k + \frac{\alpha}{|\mathcal{S}|} \sum_s \bigg( \mathbb{E}_{\pi} [ R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}_k) | S_t = s ] - \hat{v}(s, \mathbf{w}_k) \bigg) \nabla (s, \mathbf{w}_k) \tag{11.9}\]

&lt;p&gt;이 방법은 기존 DP Update와 마찬가지로 &lt;strong&gt;Randomness&lt;/strong&gt;와 &lt;strong&gt;Asynchrony&lt;/strong&gt;가 없습니다. 그럼에도 불구하고 Semi-gradient Function Approximation을 적용했을 때 시스템이 불안정합니다. 만약 Baird’s counterexample에서 DP Update의 Distribution를 Uniform Distribution에서 On-policy Distribution으로 변경하면 수렴이 식 (9.14)와 같이 Bound가 있는 해법으로 보장됩니다.&lt;/p&gt;

&lt;p&gt;이 반례가 신기한 점은 사용하는 TD나 DP가 가장 간단한 Bootstrapping 방법이고, 사용하는 Semi-gradient 방법이 가장 간단한 Function Approximation임에도 불구하고 발산한다는 점입니다. 이 예시를 통해 알 수 있는 점은 On-policy Distribution에 따라 Update가 수행되지 않으면 가장 간단한 Bootstrapping과 Function Approximation의 조합이라고 할지라도 불안정함을 나타냅니다.&lt;/p&gt;

&lt;p&gt;만약 Baird’s counterexample과 달리 Expected Return을 사용하여 Value Function을 Least-Square로 바꾼다고 가정하겠습니다. 이 방법은 Feature Vector의 집합 $\{ \mathbf{x}(s) : s \in \mathcal{S} \}$이 Linear Independent일 때 각 Iteration에서 정확한 근사가 가능하고 표준 Tabular DP로 방법이 축소되기 때문에 불안정성 문제를 해결할 수 있을 것이라고 기대할 수 있습니다. 하지만 불행하게도, 다음 예제에 나오듯이 이 방법으로도 안정성이 보장되지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 11.1) Tsitsiklis and Van Roy’s Counterexample&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예제는 각각의 단계에서 Least-Squares 해법을 찾은 경우에도 Linear Function Approximation이 DP에서 제대로 동작하지 않는 것을 보여줍니다. 위의 그림에서 나온 것처럼 $w$와 $2w$로 근사된 State가 있고, $2w$ State에서 $1-\epsilon$ 확률로 자기 자신으로 돌아오는 Action과 $\epsilon$ 확률로 마지막 State로 이동하는 Action이 있습니다. 모든 Transition에서 Reward는 0으로 정의되어 있기 때문에 Real Value는 $w = 0$일 경우 정확하게 추정이 가능합니다. 각 단계에서 1-step Return과 Estimated Value에 대한 $\overline{\text{VE}}$를 최소화하는 것으로 $w_{k+1}$를 정의하면 다음과 같이 전개할 수 있습니다.&lt;/p&gt;

\[\begin{align}
w_{k+1} &amp;amp;= \underset{w \in \mathbb{R}}{\operatorname{argmin}} \sum_{s \in \mathcal{S}} \bigg( \hat{v}(s, w) - \mathbb{E}_{\pi} [ R_{t+1} + \gamma \hat{v} (S_{t+1}, w_k) | S_t = s ] \bigg)^2 \\ \\
&amp;amp;= \underset{w \in \mathbb{R}}{\operatorname{argmin}} (w - \gamma 2 w_k)^2 + \Big( 2w - (1 - \epsilon) \gamma 2 w_k \Big)^2 \\ \\
&amp;amp;= \frac{6 - 4 \epsilon}{5} \gamma w_k \tag{11.10}
\end{align}\]

&lt;p&gt;즉, Sequence $\{ w_k \}$은 $\gamma &amp;gt; \frac{5}{6 - 4 \epsilon}$이고 $w_0 \ne 0$일 때 발산합니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;불안정성을 해결하기 위한 또 다른 방법은 특별한 방법을 사용해 함수를 근사하는 것입니다. 특히 관찰되는 Target에서 Extrapolate 하지 않는 Function Approximation의 경우 안정성이 보장됩니다. 이 방법은 &lt;span style=&quot;color:red&quot;&gt;Averagers&lt;/span&gt;라고 부르며, Nearest Neighbor나 Locally Weighted Regression이 포함되지만, 아쉽게도 이전에 다룬 Tile Coding이나 Artificial Neural Network 같은 방법은 포함되지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;the-deadly-triad&quot;&gt;The Deadly Triad&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 어떤 조건 때문에 지난 Section의 예제처럼 발산하는지에 대해 논의해보겠습니다. 이번 Section에서는 불안정성과 발산의 위험을 증가시키는 3가지 위험 요소인 &lt;span style=&quot;color:red&quot;&gt;The Deadly Triad&lt;/span&gt;를 소개하도록 하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Function Approximation&lt;/strong&gt; : 메모리 및 계산 자원보다 훨씬 큰 State Space에서 일반화하는 방법&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt; : Actual Reward 및 Complete Return 대신 기존 추정치를 포함하는 Update Target&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Off-policy Training&lt;/strong&gt; : Target Policy와 다른 Distribution에 의해 생성된 데이터로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;특이한 점으로, Control이나 Generalized Policy Iteration 자체는 이러한 위험을 초래하지 않습니다. 이 경우 분석하기 더 복잡하지만, 이 3가지 위험 요소 중 해당하는 것이 많을 수록 불안정성과 발산의 가능성이 늘어납니다. 또한 이 불안정성은 학습이나 Environment의 불확실성과도 상관이 없습니다. Environment가 완전히 알려진 Planning과 같은 Dynamic Programming에서도 발생하기 때문입니다.&lt;/p&gt;

&lt;p&gt;이 3가지 위험 요소 중 2가지 요소만 해당하는 경우 불안정성을 피할 수 있습니다. 그렇기 때문에 사용하는 방법에서 이 3가지 중 어떤 것을 포기할 수 있는지 하나하나 따져봐야합니다.&lt;/p&gt;

&lt;p&gt;가장 먼저 포기할 수 없는 요소는 &lt;strong&gt;Function Approximation&lt;/strong&gt;입니다. Function Approximation 없이는 매우 큰 State Space를 학습하기 어렵기 때문입니다. 대안으로 생각해볼 수 있는 State Aggregation이나 Nonparametric 방법은 데이터의 수가 많아질수록 시간 및 공간 복잡도가 너무 커집니다. LSTD와 같은 방법 또한 큰 문제에서 시간 복잡도로 인해 사용하기 어렵습니다.&lt;/p&gt;

&lt;p&gt;두 번째로 &lt;strong&gt;Bootstrapping&lt;/strong&gt;을 포기한다면 그 대신 계산 및 데이터 효율성을 포기해야 합니다. 둘 중에서는 계산 효율성의 손실이 더 치명적입니다. Monte Carlo와 같이 Bootstrapping을 사용하지 않는 방법은 예측을 수행하고 최종 Return을 얻는 사이에 발생하는 모든 것을 저장할 메모리가 필요하며, 모든 계산은 최종 Return을 얻은 후에야 완료됩니다. 이런 계산 문제의 비용은 주로 특수한 하드웨어에서 발생할 수 있습니다. 물론 데이터 효율성의 관점에서도 Bootstrapping을 사용하지 않았을 때의 손실이 적지 않습니다. 또한 일반적으로 Bootstrapping을 사용했을 때 학습 속도가 더 빠른 결과를 보여줍니다. 그렇기 때문에 필수는 아니지만, 가급적이면 Bootstrapping을 사용하는 것이 좋습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;strong&gt;Off-policy Training&lt;/strong&gt;이 있습니다. On-policy이 효율적인 경우가 많기도 했고, Model이 없는 강화학습의 경우 Q-learning 대신 Sarsa를 사용하면 됩니다. Off-policy의 장점은 Target Policy에 상관 없는 Action을 할 수 있다는 점입니다. 편리하긴 하지만, 필수는 아닙니다. 하지만 Off-policy는 이 책에서 언급하지 않은 강력한 지능형 Agent를 만드는데 필수적입니다.&lt;/p&gt;

&lt;h2 id=&quot;linear-value-function-geometry&quot;&gt;Linear Value-function Geometry&lt;/h2&gt;

&lt;p&gt;Off-policy 학습의 안정성 문제를 더 잘 이해하기 위해서는 Value Function Approximation에 대해 더 자세히 알아보는 것이 좋습니다. 먼저 모든 State-Value Function은 State에서 실수 집합으로 매핑되는 함수입니다. ($v : \mathcal{S} \to \mathbb{R}$) 대부분의 Value Function은 어떤 Policy에도 해당하지 않는데, 중요한 점은 대부분의 Function Approximation 방법은 State보다 매개변수가 훨씬 적기 때문에 Policy로 표현될 수 없다는 것입니다.&lt;/p&gt;

&lt;p&gt;State Space $\mathcal{S} = \{ s_1, s_2, \ldots, s_{\lvert \mathcal{S} \rvert} \}$에 대해 임의의 Value Function $v$를 가정합니다. 각 State에 대한 Value를 Vector로 표현하면 $\big[ v(s_1), v(s_2), \ldots, v(s_{\mathcal{S}}) \big]^{\sf T}$와 같이 State 수만큼 구성 요소가 있습니다. Function Approximation을 사용하려는 경우 Vector를 명시적으로 나타내기에는 구성 요소가 너무 많습니다만, 개념적으로 이러한 아이디어는 유용합니다.&lt;/p&gt;

&lt;p&gt;간단하게 3개의 State $\mathcal{S} = \{ s_1, s_2, s_3 \}$와 2개의 매개변수 $\mathbf{w} = (w_1, w_2)^{\sf T}$를 예시로 들어보겠습니다. 이 예시에서 모든 Value Function/Vector는 3차원 공간의 점으로 볼 수 있습니다. 매개변수는 2차원 부분공간에 대한 대체 좌표계를 제공합니다. 또한 모든 Weight Vector $\mathbf{w} = (w_1, w_2)^{\sf T}$는 2차원 부분공간의 한 점이므로 3가지 State 전부에게 값을 할당하는 완전한 Value Function $v_{\mathbf{w}}$이기도 합니다. 일반적인 Function Approximation을 사용하면 전체 공간과 표현 가능한 함수 사이의 관계가 복잡할 수 있으나, 이 경우에는 Linear Value Function Approximation을 사용하면 부분 공간은 아래 그림과 같은 간단한 평면으로 나옵니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예시에서 고정된 단일 Policy $\pi$에 대하여, Real Value Function $v_{\pi}$가 너무 복잡해 근사값으로 정확하게 표현되지 않는다고 가정해보겠습니다. 이것은 $v_{\pi}$가 부분공간에 있지 않다는 의미입니다. 따라서 $v_{\pi}$는 위의 그림과 같이 표현 가능한 평면 (부분공간) 밖에 있는 것으로 나타낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;$v_{\pi}$를 정확하게 표현할 수 없기 때문에 표현 가능한 Value Function 중에서 가장 가까운 것을 찾아야합니다. 이 문제를 해결하기 위해서는 먼저 가장 가깝다는 의미를 파악해야 합니다. 즉, 두 Value Function 사이의 거리 측정이 필요합니다. 두 개의 Value Function $v_1$과 $v_2$가 주어졌을 때, 두 Value Function 사이의 차이를 $v = v_1 - v_2$로 표현하겠습니다. $v$가 작다는 것은 두 Value Function이 서로 가깝다는 의미입니다. 하지만 이 Vector의 크기를 측정하는 것이 문제입니다. Section 9.2에서 논의한 바와 같이 일부 State가 더 자주 발생하거나, 집중해야할 필요가 있기 때문에 Euclidean Norm은 적절하지 않습니다. 따라서 Section 9.2에서 배운 Distribution $\mu : \mathcal{S} \to [0, 1]$을 사용합니다. 이 Distribution은 서로 다른 State를 정확하게 평가하는데 어느 정도 관심이 있는지 그 정도를 표현하는 척도입니다. 그 후, 다음과 같은 Norm을 사용하여 Value Function 사이의 거리를 정의합니다.&lt;/p&gt;

\[||v||_{\mu}^2 \doteq \sum_{s \in \mathcal{S}} \mu(s) v(s)^2 \tag{11.11}\]

&lt;p&gt;Section 9.2의 $\overline{\text{VE}}$는 이 Norm을 사용하여 $\overline{\text{VE}} (\mathbf{w}) = \lVert v_{\mathbf{w}} - v_{\pi} \rVert_{\mu}^2 $로 간단하게 작성할 수 있습니다. Value Function $v$를 표현 가능한 Value Function 부분 공간에서 가장 가까운 Value Function을 찾는 방법은 &lt;strong&gt;Projection&lt;/strong&gt;입니다. 앞으로 Projection은 연산자 $\Pi$로 표현하겠습니다. Projection 연산자 $\Pi$를 사용하여 Value Function $v$를 표현 가능한 Value Function 부분 공간으로 Projection하는 것은 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[\Pi \doteq v_{\mathbf{w}} \quad \text{where} \quad \mathbf{w} = \underset{\mathbf{w} \in \mathbb{R}^d}{\operatorname{argmin}} || v - v_{\mathbf{w}} ||_{\mu}^2 \tag{11.12}\]

&lt;p&gt;따라서 Real Value Function $v_{\pi}$에 가장 가까운 표현 가능한 Value Function은 위의 그림과 같이 Projection된 Value Function $\Pi v_{\pi}$입니다. 이 해법은 Monte Carlo Method에 의해 점근적으로 구할 수 있지만, 구하는 속도가 종종 매우 느릴 수 있습니다. 다음 단계로 넘어가기 전에, Projection 연산에 대해 더 자세히 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Projection Matrix&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Linear Function Approximation 방법에서 Projection 연산은 마찬가지로 Linear이며, 이것은 다음과 같이 $\lvert \mathcal{S} \rvert \times \lvert \mathcal{S} \rvert$ 행렬로 표현할 수 있습니다.&lt;/p&gt;

\[\Pi \doteq \mathbf{X} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \mathbf{X}^{\sf T} \mathbf{D} \tag{11.13}\]

&lt;p&gt;$\mathbf{D}$는 대각 요소에 $\mu(s)$가 있는 $\lvert \mathcal{S} \rvert \times \lvert \mathcal{S} \rvert$ 크기의 대각 행렬이고, $\mathbf{X}$는 각 행이 Feature Vector $\mathbf{x}(s)^{\sf T}$로 이루어진 $\lvert \mathcal{S} \rvert \times d$ 크기의 행렬입니다. 만약 식 (11.13)의 역행렬이 존재하지 않는다면, &lt;strong&gt;Pseudo Inverse&lt;/strong&gt;로 대체됩니다. Pseudo Inverse에 대해서는 다음 포스트를 참고해주시기 바랍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/linear-models-1/&quot;&gt;[기계학습] 3. Linear Models I&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이를 토대로 Vector의 Square Norm을 표현하면 다음과 같습니다.&lt;/p&gt;

\[||v||_{\mu}^2 = v^{\sf T} \mathbf{D} v \tag{11.14}\]

&lt;p&gt;따라서 Approximate Linear Value Function은 아래와 같이 작성할 수 있습니다.&lt;/p&gt;

\[v_{\mathbf{w}} = \mathbf{X} \mathbf{w} \tag{11.15}\]

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;TD는 이와 다른 해법을 찾습니다. 그 이유를 알아보기 위해서, 먼저 Value Function $v_{\pi}$에 대한 Bellman Equation을 복습해보겠습니다.&lt;/p&gt;

\[v_{\pi} (s) = \sum_a \pi (a|s) \sum_{s&apos;, r} p(s&apos;, r | s, a) [r + \gamma v_{\pi} (s&apos;) ] \quad \text{for all } s \in \mathcal{S} \tag{11.16}\]

&lt;p&gt;Real Value Function $v_{\pi}$는 식 (11.16)을 정확하게 풀 수 있는 유일한 함수입니다. Real Value Function $v_{\pi}$를 Approximate Value Function $v_{\mathbf{w}}$로 대체한 후, 수정된 방정식에서 발생하는 우변과 좌변 사이의 차이를 Bellman Error라고 부릅니다. Bellman Error는 $v_{\mathbf{w}}$가 $v_{\pi}$에서 얼마나 멀리 떨어져 있는지를 측정하는 데 사용되며, State $s$에 대한 Bellman Error는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\bar{\delta}_{\mathbf{w}} (s) &amp;amp; \doteq \bigg( \sum_a \pi (a | s) \sum_{s&apos;, r} p(s&apos;, r|s, a) [r + \gamma v_{\mathbf{w}} (s&apos;)] \bigg) - v_{\mathbf{w}} (s) \tag{11.17} \\ \\
&amp;amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\mathbf{w}} (S_{t+1}) - v_{\mathbf{w}}(S_t) | S_t = s, A_t \sim \pi] \tag{11.18}
\end{align}\]

&lt;p&gt;식 (11.18)을 통해 Bellman Error는 TD Error (=식 11.3)의 기대값임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;모든 Bellman Error의 Vector $\bar{\delta}_{\mathbf{w}} \in \mathbb{R}^{\lvert \mathcal{S} \rvert}$는 &lt;span style=&quot;color:red&quot;&gt;Bellman Error Vector&lt;/span&gt;라고 합니다. 위의 그림에서 Bellman Error Vector는 BE로 표시되어 있습니다. Norm에서 이 Vector의 전체 크기는 Value Function의 전체적인 Error를 측정하며, &lt;span style=&quot;color:red&quot;&gt;Mean Square Bellman Error&lt;/span&gt;라고 부릅니다. 이 식은 다음과 같이 간단하게 표현할 수 있습니다.&lt;/p&gt;

\[\overline{\text{BE}} (\mathbf{w}) = || \bar{\delta}_{\mathbf{w}} ||_{\mu}^2 \tag{11.19}\]

&lt;p&gt;일반적으로 $\overline{\text{BE}}$를 0으로 줄이는 것은 불가능하지만, Linear Function Approximation의 경우 $\overline{\text{BE}}$가 최소화되는 고유한 $\mathbf{w}$가 있습니다. 이것은 위의 그림에서 $\min \overline{\text{BE}}$로 표현되어 있으며, 일반적으로 $\overline{\text{VE}}$를 최소화하는 지점(=$\Pi v_{\pi}$)과는 다릅니다. $\overline{\text{BE}}$를 최소화하는 방법은 다음 두 Section에서 더 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;Bellman Error Vector는 Bellman Operator $B_{\pi} : \mathbb{R}^{\lvert \mathcal{S} \rvert} \to \mathbb{R}^{\lvert \mathcal{S} \rvert}$를 Approximate Value Function에 적용한 결과로, 위의 그림에도 나와 있습니다. Bellman Operator는 State $s \in \mathcal{S}$와 Value Function $v : \mathcal{S} \to \mathbb{R}$에 대해 다음과 같이 정의됩니다.&lt;/p&gt;

\[(B_{\pi} v)(s) \doteq \sum_a \pi (a|s) \sum_{s&apos;, r} p(s&apos;, r | s, a) [r + \gamma v(s&apos;)] \tag{11.20}\]

&lt;p&gt;식 (11.20)을 이용하면 $v_{\mathbf{w}}$에 대한 Bellman Error Vector를 $\bar{\delta}_{\mathbf{w}} = B_{\pi} v_{\mathbf{w}} - v_{\mathbf{w}}$로 작성할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Bellman Operator를 표현 가능한 부분 공간의 Value Function에 적용하게 되면 일반적으로 위의 그림과 같이 부분 공간 외부에 새로운 Value Function을 생성합니다. Function Approximation을 사용하지 않는 Dynamic Programming에서 Bellman Operator는 그림 상단에 있는 회색 화살표와 같이 표현 가능한 공간 외부의 점에 반복적으로 적용됩니다. 결국 이 과정은 Real Value Function $v_{\pi}$에 수렴하게 됩니다.&lt;/p&gt;

\[v_{\pi} = B_{\pi} v_{\pi} \tag{11.21}\]

&lt;p&gt;이것은 식 (11.16)에서 $\pi$에 대한 Bellman Equation을 다르게 표현한 것입니다.&lt;/p&gt;

&lt;p&gt;하지만 Function Approximation을 사용하게 되면 부분 공간 외부에 있는 Value Function을 표현할 수 없습니다. 즉, Dynamic Programming처럼 그림 상단의 회색 화살표를 따라갈 수 없습니다. 첫 번째 Update 후에 Value Function이 표현할 수 있는 부분 공간에 Projection되어야 하기 때문입니다. 다음 Update는 부분 공간 내에서 시작되며, Value Function은 다시 Bellman Operator에 의해 부분 공간 외부로 나간 다음, 다시 Projection에 의해 부분 공간으로 매핑됩니다. 이 과정은 근사를 사용한 Dynamic Programming과 유사합니다.&lt;/p&gt;

&lt;p&gt;이 때 Bellman Error Vector를 표현 가능한 부분 공간으로 Projection한 $\Pi \bar{\delta}_{\mathbf{w}}$는 그림에서 PBE로 나타나 있습니다. Norm에서 이 Vector의 크기는 Approximate Value Function의 또 다른 Error 측정값입니다. 임의의 Approximate Value Function $v_{\mathbf{w}}$에 대해, Mean Square Projected Bellman Error는 $\overline{\text{PBE}}$라고 하며, 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\overline{\text{PBE}} (\mathbf{w}) = || \Pi \bar{\delta}_{\mathbf{w}}  ||_{\mu}^2 \tag{11.22}\]

&lt;p&gt;Linear Function Approximation을 사용하면 $\overline{\text{PBE}}$가 0인 Approximate Value Function이 (부분 공간 안에) 반드시 존재합니다. 이것은 Section 9.4에서 소개한 TD Fixed Point $\mathbf{w}_{\text{TD}}$와 같습니다. 이 점은 Semi-gradient TD 방법과 Off-policy 방법에서 안정적이지 않다고 배웠습니다. 게다가 그림에서 볼 수 있듯이 이 Value Function은 일반적으로 $\overline{\text{VE}}$나 $\overline{\text{BE}}$를 최소화하는 함수와 다릅니다. 이것이 수렴하는 것을 보장하는 방법은 Section 11.7과 11.8에서 더 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-in-the-bellman-error&quot;&gt;Gradient Descent in the Bellman Error&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 지난 Section에서 배운 Bellman Error를 토대로 Off-policy 학습의 안정성 문제에 대해 다시 다루겠습니다. 이번 장에서 궁극적으로 하고싶은 것은 Off-policy 기반의 Stochastic Gradient Descent (SGD) 입니다. SGD의 Update는 목적 함수의 음의 기울기와 같습니다. 이 방법은 항상 목표에 대해 내리막길 방향으로 이동하기 때문에 안정적인 수렴을 기대할 수 있습니다.&lt;/p&gt;

&lt;p&gt;SGD 방법은 On-policy와 Off-policy 학습 뿐만 아니라 일반 non-Linear (미분가능한) Function Approximation 방법에 대해서도 수렴하지만, Bootstrapping이 있는 Semi-gradient 방법보다 느린 경향이 있습니다. 대신 이번 장 앞부분에서 다루었듯이 Semi-gradient 방법은 Off-policy 학습과 non-Linear Function Approximation의 경우 발산할 수 있다는 문제가 있습니다. 하지만 SGD 방법은 그런 발산이 일어나지 않습니다.&lt;/p&gt;

&lt;p&gt;SGD는 발산하지 않는다는 매우 강력한 장점을 가졌기 때문에 많은 연구자들이 강화학습에서 SGD를 활용하고자 연구하였습니다. 강화학습에 SGD를 활용하는 첫 번째 단계는 최적화할 Error나 목적 함수를 선택하는 것입니다. 따라서 이전 Section에서 소개한 Bellman Error를 기반으로 이번 Section과 다음 Section에 걸쳐 가장 많이 쓰이는 목적 함수를 소개하고 그 한계를 알아볼 것입니다. 결론부터 말하자면 이 방법들은 좋은 접근 방식이기는 하나, 좋은 학습 알고리즘은 생성하지 못합니다. 어째서 이러한 결론이 나오지는지는 차근차근히 일아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;가장 먼저 Bellman Error 대신 TD Error를 먼저 다시 살펴보겠습니다. Discount가 포함된 1-step TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t = R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w})\]

&lt;p&gt;이 때 가능한 목적 함수는 다음과 같은 Mean Square TD Error로 볼 수 있습니다.&lt;/p&gt;

\[\begin{align}
\overline{\text{TDE}} (\mathbf{w}) &amp;amp;= \sum_{s \in \mathcal{S}} \mu(s) \mathbb{E} \left[ \delta_t^2 | S_t = s, A_t \sim \pi \right] \\ \\
&amp;amp;= \sum_{s \in \mathcal{S}} \mu(s) \mathbb{E} \left[ \rho_t \delta_t^2 | S_t = s, A_t \sim b \right] \\ \\
&amp;amp;= \mathbb{E}_b \left[ \rho_t \delta_t^2 \right] \quad ( \text{if } \mu \text{ is the distribution encountered under } b )
\end{align}\]

&lt;p&gt;위의 마지막 방정식이 바로 SGD에 필요한 형태입니다. Behavior Policy $b$를 토대로 Sampling할 수 있는 기대값이 목적이 됩니다. 따라서 표준 SGD 접근 방식에 따라 이 기대값의 Sample을 토대로 단계별 Update를 유도할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha \nabla (\rho_t \delta_t^2) \\ \\
&amp;amp;= \mathbf{w}_t - \alpha \rho_t \delta_t \nabla \delta_t \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \rho_t \delta_t \big( \nabla \hat{v} (S_t, \mathbf{w}_t) - \gamma \nabla \hat{v} (S_{t+1}, \mathbf{w}) \big) \tag{11.23}
\end{align}\]

&lt;p&gt;재밌는 점은 식 (11.23)에서 Discount Factor $\gamma$가 붙은 항만 제외하면 식 (11.2)의 Semi-gradient TD 알고리즘과 동일하다는 것입니다. 이 추가적인 항은 진정한 SGD 알고리즘으로 만드는 역할을 합니다. Update에 식 (11.23)을 사용하는 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;Naive Residual-gradient Algorithm&lt;/span&gt;이라고 부릅니다. (Baird, 1995) Naive Residual-gradient Algorithm은 강력하게 수렴하지만, 반드시 원하는 위치로 수렴하지는 않습니다. 다음 예제를 통해 이게 무슨 의미인지 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 11.2) A-split example, showing the naiveté of the naive residual-gradient algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래 그림과 같이 3개의 State로 이루어진 Episodic MRP를 가정해보겠습니다. Episode는 항상 State A에서 시작한 다음, 확률적으로 Split됩니다. 50% 확률로 B로 간 다음 1의 Reward를 받고 종료되는 분기와, 50% 확률로 C로 간 다음 0의 Reward를 받고 종료되는 분기가 있습니다. A에서 B를 가는 것과 C를 가는 것 자체는 모두 0의 Reward를 받게 설계되어 있습니다. 이것은 Episodic Task이기 때문에 $\gamma$를 1로 간주할 수 있습니다. 또한 $\rho_t$가 항상 1이 될 수 있게 On-policy라고 가정하고, Tabular Function Approximation을 사용하도록 하겠습니다. 문제 설정만 보면 굉장히 간단한 것처럼 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제에서 State A의 Value를 구해보면 50% 확률로 1의 Reward를 받고 50% 확률로 0의 Reward를 받기 때문에 A의 Value는 $1/2$이라고 쉽게 계산할 수 있습니다. 마찬가지로 State B의 Value는 1, State C의 Value는 0이 됩니다.&lt;/p&gt;

&lt;p&gt;그런데 만약 이 문제에 Naive Residual-gradient Algorithm을 사용하게 되면 다른 결과가 나옵니다. State B의 Value는 $3/4$로, State C의 Value는 $1/4$로 수렴합니다. 다행히 State A의 Value는 똑같이 $1/2$로 수렴합니다. 이 추정한 Value들은 실제로 $\overline{\text{TDE}}$를 최소화하는 값입니다.&lt;/p&gt;

&lt;p&gt;이 Value를 사용하여 $\overline{\text{TDE}}$를 계산해봅시다. 각 Episode의 첫 번째 Transition은 A의 Value $1/2$에서 B의 Value $3/4$로 변해 그 차이가 $1/4$이 되거나 C의 Value $1/4$로 변해 그 차이가 $-1/4$이 됩니다. 이 Transition 자체의 보상은 0이고 $\gamma = 1$ 이므로 첫 번째 Transition에서 TD Error의 제곱은 항상 $1/16$ 입니다. 두 번째 Transition 또한 B의 경우 $3/4 \to 1$, C의 경우 $1/4 \to 0$이므로 TD Error의 제곱은 마찬가지로 $1/16$ 입니다. 따라서 두 단계 모두 $\overline{\text{TDE}}$는 $1/16$ 임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;이제 Real Value를 사용하여 $\overline{\text{TDE}}$를 계산해보겠습니다. 이 경우에는 첫 번째 Transition에서 B로 갈 경우 $1/2 \to 1$, C로 갈 경우 $1/2 \to 0$ 입니다. 두 경우 모두 TD Error의 제곱은 $1/4$ 입니다. 두 번째 Transition은 State의 Value와 Reward가 동일하기 때문에 둘 다 TD Error가 0입니다. 따라서 TD Error의 제곱은 첫 번째 Transition에서 $1/4$, 두 번째 Transition에서 0이 됩니다. 따라서 평균적으로 TD Error의 제곱은 $1/8$이므로 방금 전에 계산한 $1/16$보다 크기 때문에 $\overline{\text{TDE}}$ 관점에서 더 나쁜 해법이라고 볼 수 있습니다. 이 간단한 문제에서도 Real Value는 가장 작은 $\overline{\text{TDE}}$를 갖지 않는다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;A-split 예제에서는 Tabular 방법을 사용했기 때문에 State의 Real Value를 정확하게 표현할 수 있었지만, Naive Residual-gradient Algorithm은 State의 Value를 Real Value와 다르게 추정했고, 이 Value를 사용했을 때 Real Value보다 $\overline{\text{TDE}}$가 더 낮았습니다. 이것이 바로 지금까지 Mean Square TD Error를 최소화하는 것을 목적으로 하지 않았던 이유입니다.&lt;/p&gt;

&lt;p&gt;이전 장에서 배운 Mean Square Bellman Error $\overline{\text{BE}}$는 Real Value를 학습하면 모든 지점에서 Error가 0이었습니다. 따라서 Bellman Error를 최소화하는 알고리즘은 A-split 예제에서 문제가 없어야 합니다. 일반적으로 Bellman Error가 0이 될 것이라고 기대할 수는 없지만, Real Value Function을 찾는데 관련이 있기 때문입니다. State에 대한 Bellman Error는 해당 State에서의 평균 TD Error이기 때문에 Update 식을 유도해보겠습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha \nabla (\mathbb{E}_{\pi} [\delta_t]^2) \\ \\
&amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha \nabla (\mathbb{E}_{\pi} [\rho_t \delta_t]^2) \\ \\
&amp;amp;= \mathbf{w}_t - \alpha \mathbb{E}_b \left[ \rho_t \delta_t \right] \nabla \mathbb{E}_b \left[ \rho_t \delta_t \right] \\ \\
&amp;amp;= \mathbf{w}_t - \alpha \mathbb{E}_b \left[ \rho_t ( R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w})) \right] \mathbb{E}_b \left[ \rho_t \nabla \delta_t \right] \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \bigg[ \mathbb{E}_b \left[ \rho_t (R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w})) \right] - \hat{v}(S_t, \mathbf{w}) \bigg] \bigg[ \nabla \hat{v} (S_t, \mathbf{w}) - \gamma \mathbb{E}_b \left[ \rho_t \nabla \hat{v} (S_{t+1}, \mathbf{w}) \right] \bigg]
\end{align}\]

&lt;p&gt;이 Update와 이것을 Sampling하는 방법들을 &lt;span style=&quot;color:red&quot;&gt;Residual-gradient Algorithm&lt;/span&gt;이라고 합니다. 모든 기대값에서 단순한 Sample을 사용한 경우 위의 식은 식 (11.23)의 Naive Residual-gradient Algorithm과 거의 정확하게 감소합니다. 하지만 이 식이 식 (11.23)과 다른 점은 다음 State $S_{t+1}$과 함께 곱해지는 두 개의 평균값입니다. Bias되지 않는 Sample을 얻기 위해서는 다음 State에서 두 개의 독립적인 Sample이 필요하지만, Environment와의 상호작용에서 얻는 Sample은 한 개 뿐입니다. 여기서 구현의 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;Residual-gradient Algorithm을 구현하기 위한 두 가지 방법이 있습니다. 하나는 Environment가 Deterministic으로 주어진 경우입니다. Deterministic Environment라면 다음 State로 Transition하는 두 개의 Sample은 같을 수밖에 없기 때문입니다. 또 다른 방법은 $S_t$에서 다음 State인 $S_{t+1}$의 두 개의 독립된 Sample을 얻는 것입니다. 하나는 위의 식에서 첫 번째 평균에 대한 것이고, 다른 하나는 두 번째 평균에 대한 것입니다. Environment와 실제로 상호작용할 때 이렇게 2개의 Sample을 얻는 것은 불가능하지만, Simulated Environment라면 가능합니다. 이러한 경우에 Residual-gradient Algorithm은 Step-size Parameter가 일반적인 조건일 때 $\overline{\text{BE}}$의 최소값으로 수렴하는 것이 보장됩니다. 진짜 SGD 방법으로써 이 수렴은 강력하며, Linear 및 non-Linear Function Approximation 방법 모두에 적용이 가능합니다. Linear인 경우라면 수렴은 항상 $\overline{\text{BE}}$를 최소화하는 고유한 $\mathbf{w}$를 생성합니다.&lt;/p&gt;

&lt;p&gt;하지만 그럼에도 불구하고 Residual-gradient Algorithm의 문제점은 적어도 3가지가 남아 있습니다.&lt;/p&gt;

&lt;p&gt;첫 번째 문제는 Semi-gradient 방법보다 훨씬 느리다는 것입니다. 이를 해결하기 위한 아이디어로 초기에 Semi-gradient 방법을 사용하고, 나중에 Residual-gradient Algorithm으로 전환하여 수렴을 보장시키는 방법이 제안되었습니다. (Baird and Moore, 1999)&lt;/p&gt;

&lt;p&gt;두 번째 문제는 Naive Residual-gradient Algorithm처럼 몇몇 경우에 잘못된 값으로 수렴한다는 것입니다. 이 문제의 예시는 다음과 같이 A-split 문제의 변형에서 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 11.3) A-presplit example, a counterexample for the $\overline{\text{BE}}$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래 그림과 같은 3개의 State A, B, C로 이루어진 Episodic MRP가 있습니다. Episode는 동일한 확률로 A1 또는 A2에서 시작합니다. 그러나 State A1, A2는 Function Approximation을 수행할 때 A라는 동일한 State로 간주됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Function Approximation 방법의 매개변수는 3가지 구성요소를 갖고 있습니다. 하나는 State B의 Value, 하나는 State C의 Value, 나머지 하나는 State A (=A1과 A2)의 Value를 표현합니다. 초기 State를 제외하고 시스템은 Deterministic입니다. Episode가 A1에서 시작하면 0의 Reward를 받고 B로 Transition되고, 1의 Reward를 받은 다음 Episode가 종료됩니다. 마찬가지로 A2에서 시작하는 경우에는 0의 Reward를 받고 C로 Transition되고, 0의 Reward를 받고 Episode가 종료됩니다.&lt;/p&gt;

&lt;p&gt;외부적으로 보았을 때 이 예제는 A-split 예제와 동일한 것처럼 보입니다. A-split 예제와 마찬가지로 B와 C의 Real Value는 각각 1과 0이고, A는 $1/2$입니다. A-split 예제에서, Semi-gradient TD는 이 Real Value로 수렴하지만, Naive Residual-gradient Algorithm은 B와 C의 Value는 각각 $3/4$와 $1/4$로 수렴합니다. 모든 State Transition은 Deterministic이기 때문에 Residual-gradient Algorithm 또한 이 값으로 수렴합니다.&lt;/p&gt;

&lt;p&gt;따라서 Naive Residual-gradient Algorithm도 $\overline{\text{BE}}$를 최소화하는 해법이 됩니다. Deterministic 문제에서 Bellman Error와 TD Error는 동일하므로, $\overline{\text{BE}}$가 $\overline{\text{TDE}}$와 동일한 것입니다. 이 예제에서 $\overline{\text{BE}}$를 최적화하면 A-split 예제와 마찬가지로 잘못된 Value로 수렴함을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;세 번째 문제는 바로 다음 Section에서 설명합니다. 간단하게 설명하자면, $\overline{\text{BE}}$를 최소화하는 알고리즘에 문제가 있는 것이 아니라, $\overline{\text{BE}}$를 목표로 하는 것 자체에 문제가 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;the-bellman-error-is-not-learnable&quot;&gt;The Bellman Error is Not Learnable&lt;/h2&gt;

&lt;p&gt;이번 Section의 제목에 Learnable이 있습니다만, 이것은 일반적으로 기계학습에서 사용하는 Learnable과는 다른 개념입니다. 일반적인 Learnable 용어에 대해서는 다음 포스트를 참고해주시기 바랍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/is-learning-feasible/&quot;&gt;[기계학습] 2. Is Learning Feasible?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 Section에서 Learnable은 &lt;strong&gt;무한한 양의 Sample을 갖고 있다고 해도 학습이 불가능하다는 의미&lt;/strong&gt;로 사용하겠습니다. 지난 Section에서 다루었던 $\overline{\text{BE}}$는 이러한 의미에서 학습할 수 없다는 것을 밝힐 것입니다.&lt;/p&gt;

&lt;p&gt;Learnable의 개념을 명확하게 하기 위해 몇 가지 간단한 예부터 시작하겠습니다. 다음과 같은 두 개의 Markov Reward Process (MRP)를 가정해봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 두 MRP에서 Edge는 모두 동일한 확률로 발생하는 것으로 가정하고, Edge에 있는 숫자는 그 Transition이 일어났을 때 받게 되는 Reward를 의미합니다. 모든 State는 단일 성분 Feature Vector $x = 1$과 $w$로 포함됩니다. 따라서 MRP에서 유일하게 변화하는 부분은 Reward의 순서입니다. 왼쪽 MRP는 계속 같은 State에 있으면서 각각 0.5의 확률로 0이나 2의 Reward를 받습니다. 오른쪽 MRP는 같은 확률로 다른 State로 움직이거나 같은 State에 있는데, Reward는 한 State에서는 무조건 0이고 다른 State에서는 무조건 2입니다. 두 MRP가 다르게 설계되어 있지만, Reward를 관찰하게 되면 두 MRP 모두 0, 2, 2, 0, 2, 0, 0, 0, … 과 같은 Sequence로 나오게 됩니다. 따라서 무한한 양의 데이터가 주어진다고 하더라도 이 Sequence이 주어졌을 때 두 MRP중 어느 곳에서 생성된 MRP인지 알 수 없습니다. 심지어, 데이터로부터 MRP의 State가 1개인지 2개인지, Stochastic인지 Deterministic인지조차 알 수 없습니다. 이런 것을 &lt;span style=&quot;color:red&quot;&gt;Not Learnable&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;이 예시는 식 (9.1)과 같은 $\overline{\text{VE}}$ 또한 학습할 수 없습니다. $\gamma = 0$이면 예시의 MRP에서 State의 Real Value는 왼쪽부터 1, 0, 2입니다. 만약 $w = 1$이라고 가정하면, $\overline{\text{VE}}$는 왼쪽 MRP에서 0이고, 오른쪽 MRP에서 0입니다. 두 문제에서 $\overline{\text{VE}}$는 다르지만 생성된 데이터의 Distribution이 동일하기 때문에 $\overline{\text{VE}}$를 학습할 수 없습니다.&lt;/p&gt;

&lt;p&gt;목표를 학습할 수 없기 때문에 $\overline{\text{VE}}$를 왜 사용하는건가라는 의문이 듭니다. 그런데 만약 $w = 1$이라는 해법을 구한다면 그 해법은 예시의 두 MRP에 대해 최적입니다. 즉, $\overline{\text{VE}}$는 학습할 수는 없지만 이를 최적화하는 매개변수는 학습이 가능합니다. 따라서 $\overline{\text{VE}}$는 여전히 사용 가능한 목표입니다.&lt;/p&gt;

&lt;p&gt;이것을 설명하기 위해 또 다른 목적 함수를 하나 정의하도록 하겠습니다. 항상 관찰할 수 있는 한 가지 Error는 각 시점에서의 Estimated Value와 그 시점의 Return 사이의 차이입니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Mean Square Return Error $\overline{\text{RE}}$&lt;/span&gt;로 정의하며, On-policy의 경우에는 다음과 같이 나타낼 수 있습니다.&lt;/p&gt;

\[\begin{align}
\overline{\text{RE}} (\mathbf{w}) &amp;amp;= \mathbb{E} \Big[ \big( G_t - \hat{v} (S_t, \mathbf{w}) \big)^2 \Big] \\ \\
&amp;amp;= \overline{\text{VE}} (\mathbf{w}) + \mathbb{E} \Big[ \big( G_t - v_{\pi} (S_t) \big)^2 \Big] \tag{11.24}
\end{align}\]

&lt;p&gt;식 (11.24)를 보면 $\overline{\text{RE}}$와 $\overline{\text{VE}}$는 매개변수 Vector $\mathbf{w}$에 의존하지 않는 항을 제외하고는 동일합니다. 따라서 두 목표는 동일한 최적의 매개변수 $\mathbf{w}$가 필요합니다. 이 둘의 관계는 아래 그림을 보시면 더 이해가 쉬울 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 다시 $\overline{\text{BE}}$로 돌아가보면, $\overline{\text{RE}}$는 MDP에 대한 지식을 토대로 계산하는 것이 가능하지만, 데이터에서 학습할 수 없다는 점에서 $\overline{\text{VE}}$와 유사합니다. 하지만 $\overline{\text{BE}}$가 $\overline{\text{VE}}$와 다른 점은 최적화하는 해법을 학습할 수 없다는 점입니다. 바로 아래의 반례를 통해 그것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 11.4) Counterexample to the learnability of the Bellman error&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이번에는 앞에서 다루었던 예시보다 약간 더 복잡한 MRP가 필요합니다. 반례에 사용할 두 개의 MRP는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 왼쪽 MRP에는 서로 구분되는 두 개의 State가 있습니다. 하지만 오른쪽 MRP는 세 가지 State가 있는데, 그 중 B와 B’은 동일한 State로 판단되기 때문에 동일한 Value로 추정해야 합니다. 구체적으로 매개변수 Vector $\mathbf{w}$는 2개의 성분이 있으며, State A의 Value는 첫 번째 성분에 의해 표현되고, B와 B’의 Value는 두 번째 성분에 의해 표현됩니다. 두 번째 MRP는 세 State 모두 동일한 시간이 소요되도록 설계되었으므로 모든 State에 대해 $\mu(s) = 1/3$임을 쉽게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;역시 이전 예제와 마찬가지로 관찰 가능한 데이터의 분포는 두 MRP가 동일합니다. 두 MRP 모두 State A 다음에는 0의 Reward를 얻으며 State B로 이동하는 것을 알 수 있습니다. State B는 State A로 Transition될 때 1의 Reward를 받고, 그 이외에는 모두 -1의 Reward를 받습니다. 두 MRP는 통계적으로 다른 정보도 동일합니다. 예를 들어, State B가 $k$번 나타날 확률은 둘 다 $2^{-k}$입니다.&lt;/p&gt;

&lt;p&gt;만약 $\mathbf{w} = \mathbf{0}$이라고 가정해 보겠습니다. 첫 번째 MRP에서 이것은 정확한 해법이고 $\overline{\text{BE}}$는 0입니다. 두 번째 MRP에서 이 해법은 $\overline{\text{BE}} = \mu(B) 1 + \mu(B’)1 = 2/3$이 되도록 B와 B’ 모두 Error의 제곱을 1로 생성합니다. 동일한 데이터 분포를 생성하는 두 MRP에서 $\overline{\text{BE}}$가 다르므로, $\overline{\text{BE}}$는 학습할 수 없습니다.&lt;/p&gt;

&lt;p&gt;게다가 $\mathbf{w}$를 최소화하는 값은 두 MRP가 다르게 나타납니다. 첫 번째 MRP의 경우 $\mathbf{w} = \mathbf{0}$은 $\gamma$에 대해 $\overline{\text{BE}}$를 최소화합니다. 그런데 두 번째 MRP에서는 $\gamma \to 1$이 될 때 $\mathbf{w} = (-1/2, 0)^{\sf T}$이 됩니다. 따라서 $\overline{\text{BE}}$를 최소화하는 해법은 데이터만으로 추정할 수 없습니다. 데이터에 드러난 것 이상으로 MRP에 대한 추가적인 지식이 필요합니다. 따라서 $\overline{\text{BE}}$를 학습의 목적으로 추구하는 것은 불가능합니다.&lt;/p&gt;

&lt;p&gt;여기서 놀라운 점은 두 번째 MRP에서 State A의 $\overline{\text{BE}}$ 최소화 값이 0과 상당히 차이난다는 것입니다. State A는 Transition할 때 무조건 0의 Reward를 얻고, 거의 0의 Value를 갖는 State로 Transition합니다. 따라서 $v_{\mathbf{w}}(A)$는 0이어야 한다는 뜻입니다. 그런데 그렇게 나오지 않은 이유는 $v_{\mathbf{w}}(A)$를 음수로 만들었을 때 State B에서 A에 도달할 때 Error가 감소하기 때문입니다. 이 Deterministic Transition의 Reward는 1이며, 이것은 State B가 State A보다 더 큰 값을 가져야함을 의미합니다. State B의 Value가 0에 가깝기 때문에 State A의 Value는 -1에 가까워집니다. 따라서 $\overline{\text{BE}}$의 최소화 값은 State A를 $-1/2$로 추정함으로써 State A를 떠날 때와 돌아올 때의 Error의 차이를 줄이는 것입니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;서로 다른 두 개의 MRP는 동일한 데이터를 생성하지만 최소화하는 매개변수 Vector가 서로 다릅니다. 이 때 최적의 매개변수 Vector가 데이터에 대한 함수가 아니기 때문에 데이터에서 학습할 수 없다는 것이 증명됩니다. 지금까지 고려한 다른 Bootstrapping 목표인 $\overline{\text{PBE}}$와 $\overline{\text{TDE}}$는 학습 가능한 데이터에서 결정될 수 있으며, 일반적으로 $\overline{\text{BE}}$를 최소화함으로써 최적의 해법을 구할 수 있습니다. 다시 위의 그림으로 돌아가보면 이것이 요약되어 있습니다.&lt;/p&gt;

&lt;p&gt;어쨌든 $\overline{\text{BE}}$는 학습이 불가능합니다. 즉, Feature Vector나 관찰 가능한 데이터를 토대로 추정할 수 없습니다. 따라서 $\overline{\text{BE}}$는 Model에 기반한 환경으로 제한될 수밖에 없다는 결론을 낼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;gradient-td-methods&quot;&gt;Gradient-TD Methods&lt;/h2&gt;

&lt;p&gt;이제 $\overline{\text{PBE}}$를 최소화하기 위한 SGD 방법을 논의하겠습니다. 진짜 SGD 방법으로써 Gradient TD 방법은 Off-policy 학습 및 non-Linear Function Approximation에서도 강력한 수렴을 보장합니다. 만약 Linear Function Approximation이라면 $\overline{\text{PBE}}$가 0인 TD Fixed-point $\mathbf{w}_{\text{TD}}$가 항상 존재합니다. 이 해법은 Section 9.8에서 다룬 LSTD로 구할 수 있지만, 매개변수의 수 $d$에 대해 $O(d^2)$의 시간 복잡도를 갖습니다. 여기서는 이 방법 대신 계산이 복잡하더라도 $O(d)$의 시간 복잡도를 갖는 방법인 Gradient-TD에 대해 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;먼저 목적인 $\overline{\text{PBE}}$의 식 (11.22)를 행렬식으로 표현해보겠습니다.&lt;/p&gt;

\[\begin{align}
\overline{\text{PBE}} (\mathbf{w}) &amp;amp;= || \Pi \bar{\delta}_{\mathbf{w}} ||_{\mu}^2 \\ \\
&amp;amp;= (\Pi \bar{\delta}_{\mathbf{w}})^{\sf T} \mathbf{D} \Pi \bar{\delta}_{\mathbf{w}} \tag{from (11.14)} \\ \\
&amp;amp;= \bar{\delta}_{\mathbf{w}}^{\sf T} \Pi^{\sf T} \mathbf{D} \Pi \bar{\delta}_{\mathbf{w}}
\end{align}\]

&lt;p&gt;위 식을 식 (11.13)과 항등식 $\Pi^{\sf T} \mathbf{D} \Pi = \mathbf{D} \mathbf{X} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \mathbf{X}^{\sf T} \mathbf{D}$를 이용하면 다음과 같이 수정할 수 있습니다.&lt;/p&gt;

\[\begin{align}
&amp;amp;= \bar{\delta}_{\mathbf{w}}^{\sf T} \mathbf{D} \mathbf{X} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \tag{11.25} \\ \\
&amp;amp;= \big( \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \big)^{\sf T} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \big( \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \big) \tag{11.26}
\end{align}\]

&lt;p&gt;식 (11.26)을 $\mathbf{w}$에 대해 미분하면 Gradient는 다음과 같습니다.&lt;/p&gt;

\[\nabla \overline{\text{PBE}} (\mathbf{w}) = 2 \nabla \left[ \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \right]^{\sf T} \big( \mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} \big)^{-1} \big( \mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} \big)\]

&lt;p&gt;이것을 SGD 방법으로 바꾸기 위해서 이 값을 기대값으로 갖는 모든 시간 단계에서 무언가를 Sampling해야 합니다. Behavior Policy에 따라 방문한 State의 Distribution을 $\mu$라고 했을 때, 위의 식에서 3개의 항(여기서는 괄호로 구분된 부분) 모두를 이 Distribution에서 기대값으로 쓸 수 있습니다. 예를 들어서, 마지막 항 $\mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}}$은 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

\[\mathbf{X}^{\sf T} \mathbf{D} \bar{\delta}_{\mathbf{w}} = \sum_s \mu(s) \mathbf{x} \bar{\delta}_{\mathbf{w}} (s) = \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right]\]

&lt;p&gt;이것은 Semi-gradient TD(0)의 Update 식 (11.2)의 기대값에 불과합니다. $\nabla \overline{\text{PBE}} (\mathbf{w})$의 첫 번째 항은 이 식에 Transpose를 붙이고 미분한 값이므로, 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\nabla \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right]^{\sf T} &amp;amp;= \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \\ \\
&amp;amp;= \mathbb{E} \left[ \rho_t \nabla (R_{t+1} + \gamma \mathbf{w}^{\sf T} \mathbf{w}_{t+1} - \mathbf{w}^{\sf T} \mathbf{x}_t )^{\sf T} \mathbf{x}_t^{\sf T} \right] \\ \\
&amp;amp;= \mathbb{E} \left[ \rho_t ( \gamma \mathbf{x}_{t+1} - \mathbf{x}_t ) \mathbf{x}_t^{\sf T} \right]
\end{align}\]

&lt;p&gt;여기서 $\delta_t$는 Episodic Task라고 가정합니다. 마지막으로, 가운데 항은 다음과 같이 Feature Vector의 Outer Product 기대값으로 간단하게 표현이 가능합니다.&lt;/p&gt;

\[\mathbf{X}^{\sf T} \mathbf{D} \mathbf{X} = \sum_s \mu (s) \mathbf{x} (s) \mathbf{x} (s)^{\sf T} = \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]\]

&lt;p&gt;이렇게 기대값으로 표현한 식을 이용하여 $\overline{\text{PBE}}$의 Gradient 식 $\nabla \overline{\text{PBE}} (\mathbf{w})$을 다시 작성하면 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\nabla \overline{\text{PBE}} (\mathbf{w}) = 2 \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \tag{11.27}\]

&lt;p&gt;식 (11.27)을 보면 첫 항과 마지막 항이 Feature Vector $\mathbf{x}_{t+1}$에 의존하기 때문에, 단순히 Sampling한 데이터의 기대값으로 Residual-gradient Algorithm을 적용하면 편향된 추정값이 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;다른 방법을 생각해보면, 3개의 기대값을 각각 개별적으로 추정한 다음 결합하여 Bias되지 않은 추정치를 생성할 수도 있습니다. 괜찮은 방법이기는 하지만, $d \times d$ 행렬인 처음 두 개 항의 기대값을 계산하고, 두 번째 항의 역행렬을 계산하기 위해서는 많은 계산 자원이 필요합니다. Section 9.8에서와 같이 계산량을 줄이는 방법이 있긴 하지만, 그럼에도 불구하고 여전히 시간 복잡도는 $O(d^2)$입니다.&lt;/p&gt;

&lt;p&gt;또 다른 방법으로는 일부 추정값을 별도로 저장한 다음 결합하는 것을 생각해 볼 수 있습니다. 이번 Section에서 다루는 Gradient-TD가 이 방법을 사용하는데, 식 (11.27)에서 두 번째 항과 세 번째 항을 곱한 결과를 추정합니다. 두 번째 항이 $d \times d$ 행렬이고, 세 번째 항이 $d \times 1$ Vector이기 때문에 곱셈의 결과는 $d \times 1$ 크기가 됩니다. 이 Vector를 다음과 같이 $\mathbf{v}$로 표현합니다.&lt;/p&gt;

\[\mathbf{v} \approx \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \tag{11.28}\]

&lt;p&gt;식 (11.28)과 같은 형태는 Linear Supervised Learning에서 자주 보던 형태입니다. Feature에서 $\rho_t \delta_t$를 근사하기 위한 Linear Least Square 문제의 해법입니다. 이것은 다음과 같이 Importance Sampling Ratio가 포함된 식 $(\mathbf{v}^{\sf T} \mathbf{x}_t - \rho_t \delta_t)^2$의 Expected Squared Error를 최소화하는 Vector $\mathbf{v}$를 점진적으로 계산하는 표준 SGD 방법으로 구할 수 있으며, 이 방법을 &lt;span style=&quot;color:red&quot;&gt;Least Mean Square (LMS)&lt;/span&gt;라고 합니다.&lt;/p&gt;

\[\mathbf{v}_{t+1} \doteq \mathbf{v}_t + \beta \rho_t \big( \delta_t - \mathbf{v}_t^{\sf T} \mathbf{x}_t \big) \mathbf{x}_t\]

&lt;p&gt;위의 식에서 $\beta &amp;gt; 0$은 또 다른 Step-size Parameter입니다. 이런 Update 방법을 사용하면 식 (11.28)을 $O(d)$의 시간 복잡도로 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게 식 (11.28)에서 $\mathbf{v}_t$를 계산하고 나면 식 (11.27)을 기반으로 하는 SGD 방법을 사용하여 매개변수 $\mathbf{w}_t$를 Update할 수 있습니다. 이 과정을 정리하면 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha \nabla \overline{\text{PBE}} (\mathbf{w}_t) \tag{the general SGD rule} \\ \\
&amp;amp;= \mathbf{w}_t - \frac{1}{2} \alpha 2 \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \tag{from (11.27)} \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \tag{11.29} \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbf{v}_t \tag{based on (11.28)} \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \rho_t (\mathbf{x}_t - \gamma \mathbf{x}_{t+1}) \mathbf{x}_t^{\sf T} \mathbf{v}_t \tag{sampling}
\end{align}\]

&lt;p&gt;위와 같은 Update 식을 사용한 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;GTD2&lt;/span&gt;라고 합니다. 만약 마지막의 Inner Product $\mathbf{x}_t^{\sf T} \mathbf{v}_t$가 먼저 수행되면 전체 알고리즘은 $O(d)$의 시간 복잡도로 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;머리를 더 쓴다면, 식 (11.29) 부분에서 $\mathbf{v}_t$로 대체하기 전에 몇 가지 단계를 더 거쳐 약간 더 나은 알고리즘으로 만들 수도 있습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t + \alpha \mathbb{E} \left[ \rho_t \nabla \delta_t^{\sf T} \mathbf{x}_t^{\sf T} \right] \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \big( \mathbb{E} \left[ \rho_t \mathbf{x}_t \mathbf{x}_t^{\sf T} \right] - \gamma \mathbb{E} \left[ \rho_t \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \right] \big) \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \big( \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right] - \gamma \mathbb{E} \left[ \rho_t \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \right] \big) \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \big( \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] - \gamma \mathbb{E} \left[ \rho_t \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \right] \big) \mathbb{E} \left[ \mathbf{x}_t \mathbf{x}_t^{\sf T} \right]^{-1} \mathbb{E} \left[ \rho_t \delta_t \mathbf{x}_t \right] \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \big( \mathbb{E} \left[ \rho_t delta_t \mathbf{x}_t \right] - \gamma \mathbb{E} \left[ \rho_t \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \right] \mathbf{v}_t \big) \tag{based on (11.28)} \\ \\
&amp;amp; \approx \mathbf{w}_t + \alpha \rho_t \big( \delta_t \mathbf{x}_t - \gamma \mathbf{x}_{t+1} \mathbf{x}_t^{\sf T} \mathbf{v}_t \big) \tag{sampling}
\end{align}\]

&lt;p&gt;마찬가지로, 마지막 곱셈 부분인 $(\mathbf{x}_t^{\sf T} \mathbf{v}_t)$가 먼저 수행되면 시간 복잡도는 $O(d)$가 됩니다. 이 알고리즘은 &lt;span style=&quot;color:red&quot;&gt;TD(0) with Gradient Correction (TDC)&lt;/span&gt; 또는 &lt;span style=&quot;color:red&quot;&gt;GTD(0)&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 Baird’s Counterexample에 대한 Sample과 TDC의 추정 결과를 보여줍니다. 의도했던 대로 $\overline{\text{PBE}}$는 0으로 감소하지만 매개변수 Vector $\mathbf{w}$의 구성 요소들은 0에 접근하지 않습니다. 그래프를 보면 최적의 해법인 $\hat{v} = 0$과는 거리가 멀며, 다른 최적의 해법이라도 $\mathbf{w}$가 $(1, 1, 1, 1, 1, 1, 4, -2)^{\sf T}$에 비례해야 하는데, 1000번의 반복 후에도 그것에 가깝게 수렴하지 않습니다. 사실 실제로 최적의 해법으로 수렴을 하고 있긴 하지만, $\overline{\text{PBE}}$가 이미 0에 너무 가깝기 때문에 수렴 속도가 너무 느릴 뿐입니다.&lt;/p&gt;

&lt;p&gt;GTD2와 TDC는 $\mathbf{w}$를 학습하는 프로세스(1차 프로세스)와 $\mathbf{v}$를 학습하는 프로세스(2차 프로세스)로 나눌 수 있습니다. 1차 프로세스는 2차 프로세스에 의존하는 반면, 2차 프로세스는 독립적으로 진행됩니다. 이렇게 비대칭적으로 의존하는 성질을 &lt;span style=&quot;color:red&quot;&gt;Cascade&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Cascade에서는 보통 2차 프로세스가 더 빠르게 진행되므로 항상 1차 프로세스를 지원할 준비가 되어 있고, 정확한 값에 점근적이라고 가정합니다. Cascade에 대한 수렴 증명은 보통 이렇게 명시적으로 가정합니다. 이런 증명 방법을 &lt;span style=&quot;color:red&quot;&gt;Two-time-scale 증명&lt;/span&gt;이라고 합니다. &lt;strong&gt;Fast time scale&lt;/strong&gt;은 2차 프로세스를 의미하며, &lt;strong&gt;Slower time scale&lt;/strong&gt;은 1차 프로세스를 의미합니다. 만약 1차 프로세스의 Step-size Parameter를 $\alpha$, 2차 프로세스의 Step-size Parameter를 $\beta$라고 한다면 수렴에 대한 증명은 보통 $\beta \to 0$과 $\frac{\alpha}{\beta} \to 0$이라는 조건이 필요합니다.&lt;/p&gt;

&lt;p&gt;Gradient-TD 방법은 현재 가장 널리 사용되는 안정적인 Off-policy 방법입니다. 이에 대한 여러 연구가 많이 이루어졌는데, 대표적으로 Semi-gradient TD와 Gradient-TD 방법을 융합한 Hybrid-TD 방법이 있습니다. (Hackman, 2012; White and White, 2016) Hybrid-TD 방법은 Target Policy와 Behavior Policy가 동일한 State에서는 Semi-gradient TD 처럼 동작하고, 그렇지 않은 State에서는 Gradient-TD 처럼 동작하는 방법입니다. 마지막으로, 이 외에도 Gradient TD 방법의 아이디어는 Proximal 방법이나 Control 방법에 융합한 연구도 있습니다. (Mahadevan et al., 2014; Du et al., 2017)&lt;/p&gt;

&lt;h2 id=&quot;emphatic-td-methods&quot;&gt;Emphatic-TD Methods&lt;/h2&gt;

&lt;p&gt;이제 두 번째 아이디어로 넘어가서, 특별한 Distribution에 의존하지 않는 새로운 Gradient 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;Section 9.4에서 Linear Semi-gradient TD 방법은 On-policy Distribution에서 훈련될 때 효율적이고 안정적임을 배웠습니다. 이것은 식 (9.11)의 행렬 $\mathbf{A}$가 Positive Definiteness인 것, On-policy의 State Distribution $\mu_{\pi}$, Target Policy 하에 State Transition 확률 $p(s \mid s, a)$과 관련이 있었습니다. Off-policy에서는 Importance Sampling을 사용하여 State Transition에 대한 Weight를 부여하여 Target Policy를 학습하였으나, State Distribution은 여전히 Behavior Policy로부터 나왔었습니다.&lt;/p&gt;

&lt;p&gt;이것을 해결하는 방법은 일부 State를 강조하고, 나머지 State를 덜 강조하여 Update Distribution을 On-policy Distribution으로 만드는 것입니다. 그렇게 하면 안정성과 수렴성이 뒤따를 것이기 때문입니다. 이것이 이번 Section에서 배울 Emphatic-TD 방법의 핵심 아이디어입니다.&lt;/p&gt;

&lt;p&gt;사실 On-policy Distribution는 여러 개가 있고, 그 중 어떤 것이든 안정성을 보장하는데 충분하기 때문에 &lt;strong&gt;On-policy Distribution&lt;/strong&gt;이라는 개념은 맞지 않습니다.&lt;/p&gt;

&lt;p&gt;Discount가 없는 Episodic Task를 가정해봅시다. 여기서 Episode가 종료되는 방식은 Transition Probability에 의해 결정되지만, Episode가 시작되는 방법은 여러 방법이 있을 수 있습니다. 그러나 Episode가 시작되고, 만약 모든 State Transition이 Target Policy에 의해 일어난다면 State Distribution은 On-policy Distribution에 의한 결과라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;그리고 Episode가 어디에서 시작하는지에 따라 방문하는 State의 수가 달라질 것이라고 예상할 수 있습니다. 만약 Episode가 Terminal State에 가까운 곳에서 시작한다면 종료 전까지 몇 개의 State만 방문하고 끝나겠지만, Terminal State에서 먼 곳에서 시작한다면 많은 State를 방문할 수 있기 때문입니다. 두 경우 모두 On-policy Distribution이며, Semi-gradient 방법으로 안정적인 학습을 보장할 수 있습니다. 하지만 프로세스가 시작되면, On-policy Distribution은 Episode가 종료될 때까지 방문하는 모든 State를 Update 하는 것과 같은 결과를 갖습니다.&lt;/p&gt;

&lt;p&gt;만약 Discount가 있는 경우 이러한 목적을 위해 부분적으로, 또는 확률적으로 Episode가 종료하게 만들 수 있습니다. 예를 들어, Discount Factor를 $\gamma = 0.9$로 설정한다면 모든 시간 단계에서 $0.1$의 확률로 Episode가 종료되고 Transition된 State에서 새로운 Episode로 시작한다는 것으로 볼 수 있습니다. 즉, Discount를 사용한 문제는 모든 시간 단계에서 $1 - \gamma$ 확률로 Episode가 종료되고 다시 시작되는 문제나 마찬가지입니다. Discount를 이러한 관점으로 보는 것은 &lt;span style=&quot;color:red&quot;&gt;Pseudo Termination&lt;/span&gt;에 대한 일반적인 개념의 한 예시입니다. Pseudo Termination에서 Episode의 종료는 State Transition의 순서에 영향을 미치지 않지만, 학습 과정과 학습의 양에는 영향을 미친다는 의미입니다. 이러한 종류의 Pseudo Termination은 Off-policy 학습에 중요합니다. 왜냐하면 원하는 방식으로 Episode를 다시 시작할 수 있기 때문입니다. Pseudo Termination으로 인해 On-policy Distribution을 토대로 방문한 State를 유지할 필요성이 줄어듭니다. 즉, 만약 새 State를 다시 시작한 Episode로 간주하지 않으면 Discounting을 통해 빠르게 On-policy Distribution이 제한됩니다.&lt;/p&gt;

&lt;p&gt;Episodic State-Value를 학습하기 위한 1-step Emphatic-TD 알고리즘은 다음과 같이 정의됩니다.&lt;/p&gt;

\[\begin{align}
\delta_t &amp;amp;= R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w}) - \hat{v} (S_t, \mathbf{w}) \\ \\
\mathbf{w}_{t+1} &amp;amp;= \mathbf{w}_t + \alpha M_t \rho_t \delta_t \nabla \hat{v} (S_t, \mathbf{w}_t) \\ \\
M_t &amp;amp;= \gamma \rho_{t-1} M_{t-1} + I_t
\end{align}\]

&lt;p&gt;위 식에서 Interest $I_t$는 임의의 초기값을 갖고, Emphasis $M_t$는 $M_{-1} = 0$으로 초기화됩니다. 아래 그림은 Baird’s counterexample에서 Parameter Vector의 구성 요소 값이 어떻게 변하는지 나타내고 있습니다. (이 그래프는 모든 시간 단계 $t$에 대해서 $I_t = 1$로 설정되어 있다고 가정했습니다) 그래프 중간에서 약간의 출렁임이 보이긴 하지만, 결국 모든 값은 특정 값으로 수렴하며 $\overline{\text{VE}}$는 0에 가까워집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/11. Off-policy Methods with Approximation/RL 11-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 Trajectory는 Transition나 Reward에 대한 Sampling으로 인한 Variance 없이, Parameter Vector Trajectory의 기대값을 반복적으로 계산해서 얻습니다. 안타깝게도 Emphatic-TD 알고리즘을 직접 적용한다면 Baird’s counterexample에서 Variance가 너무 크기 때문에 계산 실험에서 일관된 결과를 얻는 것이 불가능합니다. 알고리즘은 이 문제에 대해 이론적으로 최적의 해법으로 수렴하지만, 실제로는 그렇지 않습니다. 다음 Section에서는 이런 알고리즘의 Variance를 어떻게 줄일 수 있을지에 대해 논의합니다.&lt;/p&gt;

&lt;h2 id=&quot;reducing-variance&quot;&gt;Reducing Variance&lt;/h2&gt;

&lt;p&gt;Off-policy 학습은 On-policy 학습보다 본질적으로 Variance가 더 클 수밖에 없습니다. 왜냐하면 Policy와 덜 관련있는 데이터를 토대로 학습할수록, Policy의 Value를 그만큼 덜 학습하기 때문입니다. 극단적인 경우를 가정한다면 데이터를 토대로 아무것도 학습하지 못할 수도 있습니다. 결국, Target Policy와 Behavior Policy가 어느 정도는 관련이 있어야만 유사한 State를 방문하고, 유사한 Action을 취하면서 Off-policy 학습을 진행할 수 있습니다.&lt;/p&gt;

&lt;p&gt;다른 한편으로는, 방문한 State와 선택한 Action이 상당히 겹치지만 동일하지는 않은 유사한 Policy가 많이 있습니다. Off-policy 학습의 존재 이유는 이러한 많은 Policy에 일반화를 가능하게 하는 것입니다. 문제는 주어진 경험을 어떻게 최대한 활용하는지에 대한 것입니다. Step-size Parameter가 올바르게 설정된 경우 Expected Value를 안정적으로 구할 몇 가지 방법이 있으므로, 이제는 Estimated Value의 Variance를 줄여보겠습니다. 이에 대한 많은 아이디어가 있지만, 여기서는 그 중 일부만 소개합니다.&lt;/p&gt;

&lt;p&gt;먼저 Importance Sampling을 기반으로 하는 Off-policy 방법에서 Variance를 제어하는 것이 중요한 이유를 논의해보겠습니다. 지금까지 알아본 Importance Sampling에는 종종 Policy Ratio의 곱셈이 포함됩니다. 이 떄의 Ratio는 항상 식 (5.13)과 같은 예상치이지만, 매우 높거나 0에 가까울 정도로 낮을 수도 있습니다. 연속적인 Ratio들은 서로 상관 관계가 없기 때문에 곱셈의 기대값이 항상 1이지만, Variance는 매우 높을 수 있습니다. 이 비율은 SGD 방법에서의 Step-size를 곱하기 때문에 Variance가 크다는 것은 Step-size가 크게 달라지는 단계를 수행한다는 의미가 됩니다. 이것은 때때로 매우 큰 Step-size를 가질 수 있기 때문에 SGD에 문제가 생길 수 있습니다. SGD 방법은 여러 단계의 평균에 의존하기 때문에 단일 Sample에서 크게 이동하면 신뢰할 수 없기 때문입니다.&lt;/p&gt;

&lt;p&gt;이를 해결하는 방법으로 가장 먼저 떠올릴 수 있는 것은 이 문제를 방지할 수 있을 정도로 Step-size Parameter를 작게 설정하는 것입니다. 문제는 이렇게 할 경우 학습 속도가 매우 느려질 수 있다는 것입니다. 따라서 문제가 생길 만한 몇몇 Sample에서만 Step-size Parameter를 조절하는 방법이 연구되었습니다. 대표적으로 매개변수 Vector에 따라 Step-size Parameter를 적응적으로 설정하는 방법이 있습니다. (Jacobs, 1988; Sutton, 1992b, c)&lt;/p&gt;

&lt;p&gt;또는 5장에서 Weighted Importance Sampling이 일반적인 Importance Sampling보다 Variance Update가 낮을 때 훨씬 더 잘 작동한다는 사실을 배웠습니다. 하지만 Weighted Importance Sampling을 Function Approximation에 적용하는 것은 어려운 일이며, $O(d)$의 시간 복잡도에서만 대략적으로 수행할 수 있습니다. (Mahmood and Sutton, 2015)&lt;/p&gt;

&lt;p&gt;Section 7.5에서 배운 Tree-backup 알고리즘은 Importance Sampling을 사용하지 않고 일부 Off-policy 학습을 수행할 수 있었습니다. 이 아이디어는 Munos, Stepleton, Harutyunyan 및 Bellemare(2016)와 Mahmood, Yu 및 Sutton(2017)에 의해 보다 안정적이고 효율적인 Off-policy Policy에 확장되었습니다.&lt;/p&gt;

&lt;p&gt;마지막으로, Target Policy가 부분적으로 Behavior Policy에 의해 결정되도록 만들어서 Importance Sampling Ratio가 비교적 크지 않도록 만드는 방법이 있습니다. (Precup et al., 2006)&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Off-policy 방법은 On-policy와는 다르게 Policy이 Target Policy와 Behavior Policy로 분리되어 Value Function을 학습합니다. Tabular Q-learning을 배웠을 때 Off-policy 학습은 쉬운 것처럼 보였고, Expected Sarsa 및 Tree-backup 알고리즘으로 자연스럽게 일반화까지 해냈습니다. 그러나 이번 장에서 보았듯이 이 아이디어를 Function Approximation, 특히 Linear Function Approximation로 확장하는 것은 생각보다 어려운 일이며, 강화학습에 대한 심도 깊은 이해를 필요로 합니다.&lt;/p&gt;

&lt;p&gt;Off-policy 알고리즘을 찾는 이유 중 하나는 Exploration과 Exploitation 사이의 Trade-off를 처리하는 데 유연성을 제공하기 위해서입니다. 또 다른 이유는 학습으로부터 Action을 분리하여 Target Policy에 얶매이지 않기 위해서입니다. TD 학습은 동시에 여러 작업을 해결하기 위해 하나의 경험을 사용하여 여러가지를 병렬로 학습할 수 있는 가능성을 보여줍니다. 모든 경우에 그런 것은 아니지만, 특별한 경우에는 원하는 만큼 효율적으로 이 작업을 수행할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 장에서는 Off-policy 학습에서 Function Approximation을 위한 해결 방법을 두 부분으로 나누었습니다. 첫 번째 부분에서는 Behavior Policy에 대한 학습 목표를 수정하여 Tabular에서 고안된 기술로 간단하게 처리했습니다. 다만 이 방법은 Update의 Variance를 증가시켜 학습 속도를 느리게 만드는 단점이 있었습니다. Off-policy 학습에서 높은 Variance는 아직까지도 완전히 해결되지 않은 부분이며, 앞으로도 많은 연구자들의 목표로 남을 것입니다.&lt;/p&gt;

&lt;p&gt;두 번째 부분에서는 Off-policy 학습에서 Bootstrapping을 포함한 Semi-gradient TD 방법의 불안정성을 다루었습니다. Function Approximation, Off-policy 학습, Bootstrapping이라는 3가지 위험 요소가 있고, 이 3가지 요소 중 최소한 한 개 이상을 포기해야 불안정성을 제거할 수 있습니다. 이번 장에서 다룬 알고리즘 중 가장 널리 쓰이는 방법은 Bellman Error를 토대로 진짜 SGD 방법을 수행하는 것입니다.&lt;/p&gt;

&lt;p&gt;그러나 이어지는 분석을 통해 이 방법은 많은 경우에 매력적인 목표가 아니며, 결국 학습 알고리즘으로 완성하는 것이 불가능하다는 결론이 나왔습니다. 또 다른 접근 방식인 &lt;strong&gt;Gradient-TD&lt;/strong&gt; 방법은 Projected Bellman Error를 토대로 SGD를 수행합니다. $\overline{\text{PBE}}$의 Gradient는 $O(d)$의 시간 복잡도로 학습할 수 있지만, 두 번째 Step-size와 두 번째 매개변수 Vector가 필요했습니다. 최신 방법으로 &lt;strong&gt;Emphatic-TD&lt;/strong&gt; 방법은 Update에 Weight를 부여하여 일부 State를 강조하는 방식으로 On-policy 학습처럼 만들었습니다. 이로 인해 계산적으로 간단한 Semi-gradient 방법을 사용할 수 있게끔 유도하였습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 정리해보면, Off-policy 학습은 아직도 새로운 영역이며, 불안정합니다. 어떤 방법이 가장 좋을지, 또는 적절할지 명확하지 않습니다. 이번 장에서 소개한 방법이 과연 현실적이며, 필요한 방법인지, 소개한 방법 외에 더 좋은 방법이 있는 것은 아닌지와 같은 연구가 현재도 진행중이기 때문입니다.&lt;/p&gt;

&lt;p&gt;11장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이 책은 5장 이후로 Generalized Policy Iteration (GPI)에서 내재된 Exploitation과 Exploration 사이의 Trade-off를 처리하는 방법으로 On-policy와 Off-policy를 사용했습니다. 9장과 10장에서는 On-policy의 경우를 Function Approximation로 처리했으며, 이번 장에서는 Off-policy에서의 Function Approximation을 다룰 예정입니다. Off-policy 방법을 Function Approximation로 확장하는 것은 On-policy의 경우에서와 다른 점도 많고 어려운 점도 많습니다.</summary></entry><entry><title type="html">On-policy Control with Approximation</title><link href="http://localhost:4000/studies/on-policy-control-with-approximation/" rel="alternate" type="text/html" title="On-policy Control with Approximation" /><published>2022-05-20T00:00:00+09:00</published><updated>2022-05-20T00:00:00+09:00</updated><id>http://localhost:4000/studies/on-policy-control-with-approximation</id><content type="html" xml:base="http://localhost:4000/studies/on-policy-control-with-approximation/">&lt;p&gt;지난 장에서 근사를 이용한 Value Function Approximation에 대해 알아보았습니다. 이번 장에서는 매개변수를 사용하여 Action-Value Function $\hat{q}(s, a, \mathbf{w}) \approx q_* (s, a)$를 근사하는 Control 문제를 다루겠습니다. (Weight Vector $\mathbf{w} \in \mathbb{E}^d$는 유한 차원 Vector입니다) 이번 장에서는 먼저 On-policy에만 집중하고, Off-policy의 문제는 다음 장에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;가장 먼저 지난 장에서 다룬 Semi-gradient TD(0)를 확장한 Semi-gradient Sarsa 알고리즘을 다룹니다. Episodic Task에서는 이 확장이 간단하지만, Continuing Task에서는 Optimal Policy를 찾기 위해 Discounting을 다시 사용해야 합니다. 신기한 점은, 진짜 Function Approximation을 얻게 되면 Discounting을 버리고 새로운 Differential Value Function을 사용하여 Control 문제를 새로운 Average-reward 식으로 바꿔야 합니다.&lt;/p&gt;

&lt;p&gt;Episodic Task에서는 먼저 지난 장에서 다룬 함수 근사를 State-Value에서 Action-Value로 확장합니다. 그런 다음 $\epsilon$-greedy를 사용한 Action 선택을 통해 On-policy GPI의 일반적인 패턴을 따라 Control하도록 확장합니다. 그 후 Mountain Car 예제에서 $n$-step Linear Sarsa가 어떤 성능을 보이는지 결과를 확인하고, Continuing Task로 넘어가는 순서로 진행하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;episodic-semi-gradient-control&quot;&gt;Episodic Semi-gradient Control&lt;/h2&gt;

&lt;p&gt;먼저 지난 장에서 배운 Semi-gradient Prediction을 Action-Value로 확장해보겠습니다. 이 경우 Weight Vector $\mathbf{w}$를 매개변수로 사용하여 Action-Value Function으로 표현한다면 $\hat{q} \approx q_{\pi}$가 됩니다. $S_t \mapsto U_t$ 형태의 무작위 Training Data를 고려하기 전에, $S_t, A_t \mapsto U_t$ 형태의 데이터를 먼저 살펴보겠습니다. Update의 Target인 $U_t$는 Monte Carlo의 Return $G_t$나 $n$-step Sarsa의 Return 식 (7.4)와 같이 일반적인 Backed-up 값을 포함하여 $q_{\pi} (S_t, A_t)$의 근사값이 될 수 있습니다. Action-Value 예측을 위한 일반적인 Gradient Descent Update는 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ U_t - \hat{q} (S_t, A_t, \mathbf{w}_t) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{10.1}\]

&lt;p&gt;이것을 1-step Sarsa 방법에 맞게 수정하면 다음과 같습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ R_{t+1} + \gamma \hat{q}(S_{t+1, A_{t+1}, \mathbf{w}_t}) - \hat{q} (S_t, A_t, \mathbf{w}_t) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{10.2}\]

&lt;p&gt;이 Update 식을 사용한 Control 방법을 &lt;span style=&quot;color:red&quot;&gt;Episodic Semi-gradient 1-step Sarsa&lt;/span&gt;라고 부릅니다. 고정된 Policy의 경우 이 방법은 TD(0)와 동일한 방식으로 수렴하며, 식 (9.14)와 동일한 Error Bound를 가집니다.&lt;/p&gt;

&lt;p&gt;Control 방법을 제대로 구축하기 위해서는 이러한 Action-Value Approximation을 Policy Improvement 및 Action 선택을 위한 방법들과 융합해야 합니다. 하지만 Continuous Action이나 수가 매우 많은 Discrete Action이 주어졌을 때 사용할 수 있는 적절한 기술은 현재도 연구가 이루어질 정도로 아직 명확한 방법이 없습니다. 다행히 Action 집합이 이산적이고 그 수가 적절하다면 이전 장에서 소개한 방법을 도입할 수 있습니다. 즉, 다음 State $S_{t+1}$에서 사용 가능한 Action에 대해 $\hat{q}(S_{t+1}, a, \mathbf{w}_t)$를 계산하면 Greedy Action인 $A_{t+1}^* = \underset{a}{\operatorname{argmax}} \hat{q} (S_{t+1}, a, \mathbf{w}_t)$를 구할 수 있습니다. 그 후 $\epsilon$-greedy와 같은 Softmax 방법을 통해 Policy Improvement를 수행할 수 있습니다. 이 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 10.1) Mountain Car Task&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;아래 그림의 왼쪽 위를 보시면 자동차가 가파른 산길을 통과하려고 합니다. 그런데 이 자동차는 동력이 부족하기 때문에 단순히 엔진의 힘으로는 Goal 지점에 도달할 수 없고, 유일한 해결책은 반대방향으로 먼저 자동차를 후진한 다음 가속도와 관성을 이용해 Goal 지점에 도달하는 것입니다. 이 예제는 목표에 도달하기 위해 오히려 목표에서 멀어져야 하는 Control 문제입니다. 많은 기존의 Control 방법들은 명시적으로 이러한 문제의 해결 방법을 지원하지 않는 한 해결하기 쉽지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mountain Car 문제에서 Reward는 자동차가 Goal 위치를 지나갈 때까지(=Episode가 종료될 때까지) 모든 시간 단계에서 -1로 정의되어 있습니다. 자동차가 선택할 수 있는 Action은 3개로, 전진(+1), 후진(-1) 및 정지(0)입니다. 자동차는 단순한 물리 법칙에 따라 움직이며, 자동차의 위치 $x_t$ 및 속도 $\dot{x}_t$는 다음과 같이 Update됩니다.&lt;/p&gt;

\[\begin{align}
x_{t+1} &amp;amp; \doteq \text{bound} \left[ x_t + \dot{x}_{t+1} \right] \\ \\
\dot{x}_{t+1} &amp;amp; \doteq \text{bound} \left[ \dot{x}_t + 0.001 A_t - 0.0025 \cos (3x_t) \right]
\end{align}\]

&lt;p&gt;위 식에서 $\text{bound}$ 연산은 위치와 속도가 $-1.2 \le x_{t+1} \le 0.5$ 및 $-0.07 \le \dot{x}_{t+1} \le 0.07$ 구간을 벗어나지 않게 만드는 연산입니다. 또한, $x_{t+1}$이 왼쪽 경계에 도달하면 $\dot{x}_{t+1}$는 0으로 재설정됩니다. 만약 오른쪽 경계에 도달하면 Goal 지점에 도달한 것이니 Episode가 종료됩니다. 각각의 Episode는 임의의 위치 $x_t \in \left[ -0.6, -0.4 \right)$에서 속도 0으로 시작합니다. 위치와 속도는 모두 Continuous State이므로 이를 Binary Feature로 변환하기 위해 Tile Coding을 사용하였습니다. Tile Coding에 의해 생성된 Feature Vector $\mathbf{x} (s, a)$는 Action-Value Function를 근사화하기 위해 다음과 같이 Weight Vector와 선형으로 결합되었습니다.&lt;/p&gt;

\[\hat{q} (s, a, \mathbf{w}) \doteq \mathbf{w}^{\sf T} \mathbf{x} (s, a) = \sum_{i=1}^d w_i \cdot x_i (s, a) \tag{10.3}\]

&lt;p&gt;위의 그림은 이 함수 근사로 Mountain Car 문제를 해결하는 방법을 학습하는 동안 발생하는 일을 나타내고 있습니다. 그래프는 단일 실행에서 학습된 Value Function의 부호를 뒤집은 결과입니다. 초기의 Action Value는 모두 0이었습니다. 모든 Action에 대한 Reward가 음수이기 때문에 Action Value를 낙관적으로 본 것이며, 이것을 통해 $\epsilon = 0$일지라도 탐색이 일어나도록 유도한 것입니다. 그림 중간에 428단계에서는 한 Episode도 완료되지 않았지만 자동차는 State 공간에서 궤도를 따라 앞뒤로 움직였습니다. 자주 방문하는 모든 State는 그렇지 않은 State보다 더 나쁘게 평가되고, 실제 Reward 또한 예상했던 것보다 나빴기 때문입니다. 이는 해법을 찾을 때까지 자동차가 있던 곳에서 계속 멀어져 새로운 State를 탐색하도록 유도합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그래프는 다양한 Step-size에 대해 Mountain Car 문제에서 Semi-gradient Sarsa의 학습 곡선을 보여줍니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;semi-gradient-n-step-sarsa&quot;&gt;Semi-gradient $n$-step Sarsa&lt;/h2&gt;

&lt;p&gt;Semi-gradient Sarsa의 $n$-step 버전의 Return만 구하면 식 (10.1)의 Update 식을 그대로 사용하여 계산할 수 있습니다. $n$-step 버전의 Return은 식 (7.4)를 함수 근사 형식으로 일반화하면 다음과 같이 변형할 수 있습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots +  \gamma^{n-1} R_{t+n} + \gamma^n \hat{q} (S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \quad t+n &amp;lt; T \tag{10.4}\]

&lt;p&gt;식 (7.4)와 마찬가지로 $t + n \ge T$라면 $G_{t:t+n} \doteq G_t$입니다. $n$-step Update는 식 (10.1)과 크게 다르지 않습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ G_{t:t+n} - \hat{q} (S_t, A_t, \mathbf{w}_t) \right] \nabla \hat{q} (S_t, A_t, \mathbf{w}_t), \quad 0 \le t &amp;lt; T \tag{10.5}\]

&lt;p&gt;Semi-gradient $n$-step Sarsa의 전체 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;7장에서 언급한 것과 마찬가지로 $n$이 1보다 크면서 중간 단계만큼 Bootstrapping 하는 경우 성능이 가장 좋습니다. 아래 그림은 $n=8$이 $n=1$보다 더 빨리 학습하는 것을 보여줍니다. 또한 그 아래 그림은 이 문제에서 매개변수 $\alpha$와 $n$이 학습에 어떠한 영향을 끼치는지 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;average-reward-a-new-problem-setting-for-continuing-tasks&quot;&gt;Average Reward: A New Problem Setting for Continuing Tasks&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 Continuing Task로 넘어가겠습니다. Continuing Task는 Episodic Task와 다르게 종료되지 않고 영원히 계속되는 문제이므로 Return을 다른 방법으로 계산합니다. 대표적으로 Average Reward 방법이 있습니다. Average Reward 방법에서 Policy $\pi$의 우수함은 Average Rate of Reward, 또는 단순히 Average Reward로 정의되며, 다음과 같이 $r(\pi)$로 표현되는 Policy를 따릅니다.&lt;/p&gt;

\[\begin{align}
r(\pi) &amp;amp; \doteq \lim_{h \to \infty} \frac{1}{h} \sum_{t=1}^h \mathbb{E} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \tag{10.6} \\ \\
&amp;amp;= \lim_{t \to \infty} \mathbb{E} \left[ R_t | S_0, A_{0:t-1} \sim \pi \right] \tag{10.7} \\ \\
&amp;amp;= \sum_s \mu_{\pi} (s) \sum_a \pi (a|s) \sum_{s&apos;, r} p(s&apos;, r | s, a) r
\end{align}\]

&lt;p&gt;위 식에서의 기대값은 초기 State $S_0$와 Policy $\pi$를 따르는 Action $A_0, A_1, \ldots, A_{t-1}$에 따라 정해집니다. 두 번째 및 세 번째 식은 Steady-state Distribution $\mu_{\pi} (s) \doteq \underset{t \to \infty}{\operatorname{lim}} \text{Pr} \{ S_t = s \mid A_{0:t-1} \sim \pi \}$가 존재하고 $S_0$에 독립적일 때 성립합니다. 즉, MDP가 &lt;strong&gt;Ergodic&lt;/strong&gt; 할 때 성립합니다. Ergodic MDP에서 시작 State와 Agent가 초기에 내린 선택은 일시적인 효과만 있습니다. 장기적으로 어떤 State에 있을지는 Policy와 MDP의 Transition Probability에만 영향을 받습니다. Ergodicity는 충분조건이지만, 식 (10.6)에서 극한의 존재를 보장할 필요는 없습니다.&lt;/p&gt;

&lt;p&gt;Discounting이 없는 Continuing Task의 경우 Optimality의 종류에 따라 약간의 차이가 있습니다. 하지만 대부분 실용적인 목적을 위해서는 시간 단계당 Average Reward, 즉 $r(\pi)$에 맞춰 Policy를 따르는 것이 적절합니다. 이것은 식 (10.7)에서 보여주는 $\pi$의 Average Reward, 또는 Reward Rate입니다. 여기서는 $r(\pi)$의 최대값을 달성하는 모든 Policy를 Optimal Policy로 간주하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Steady-state Distribution $\mu_{\pi}$는 Policy $\pi$에 따라 Action을 선택하면 동일한 Distribution을 유지하는 특수한 Distribution입니다. 수학적으로는 다음과 같이 표현합니다.&lt;/p&gt;

\[\sum_s \mu_{\pi} (s) \sum_a \pi(a|s) p(s&apos;|s, a) = \mu_{\pi} (s&apos;) \tag{10.8}\]

&lt;p&gt;Average Reward 방법에서 Return은 Reward와 Average Reward 간의 차이로 정의됩니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \cdots \tag{10.9}\]

&lt;p&gt;식 (10.9)와 같은 Return을 &lt;span style=&quot;color:red&quot;&gt;Differential Return&lt;/span&gt;이라고 하며, 이것을 사용한 Value Function을 &lt;span style=&quot;color:red&quot;&gt;Differential Value Function&lt;/span&gt;이라고 합니다. 기존 Value Function이 Discounted Return으로 정의된 것처럼 Differential Value Function 또한 새로운 Return의 관점에서 정의됩니다. 따라서 기존과 마찬가지로 Differential Value Function에서도 $v_{\pi} (s) \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right]$, $q_{\pi} (s, a) \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s, A_t = a \right]$와 같은 동일한 표기법을 사용할 것입니다. Differential Value Function에서도 Bellman Equation이 있는데, 이것은 이전에 다룬 것과 약간 다릅니다. 간단하게 설명하면, $\gamma$를 모두 제거하고 모든 Reward를 Reward와 실제 Average Reward 간의 차이로 대체합니다.&lt;/p&gt;

\[\begin{align}
v_{\pi} (s) &amp;amp;= \sum_a \pi(a|s) \sum_{r, s&apos;} p(s&apos;,r|s, a) \left[ r - r(\pi) + v_{\pi} (s&apos;) \right] \\ \\
q_{\pi} (s, a) &amp;amp;= \sum_{r, s&apos;} p (s&apos;, r | s, a) \left[ r - r(\pi) + \sum_{a&apos;} \pi (a&apos;|s&apos;) q_{\pi} (s&apos;, a&apos;) \right] \\ \\
v_* (s) &amp;amp;= \max_a \sum_{r, s&apos;} p(s&apos;, r|s, a) \left[ r - \max_{\pi} r(\pi) + v_* (s&apos;) \right] \\ \\
q_* (s, a) &amp;amp;= \sum_{r, s&apos;} p(s&apos;, r | s, a) \left[ r - \max_{\pi} r(\pi) + \max_{a&apos;} q_* (s&apos;, a&apos;) \right]
\end{align}\]

&lt;p&gt;마찬가지로 Differential TD Error 또한 다음과 같이 변경됩니다.&lt;/p&gt;

\[\begin{align}
\delta_t &amp;amp; \doteq R_{t+1} - \bar{R}_t + \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{10.10} \\ \\
\delta_t &amp;amp; \doteq R_{t+1} - \bar{R}_t + \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \tag{10.11}
\end{align}\]

&lt;p&gt;여기서 $\bar{R}_t$는 시간 단계 $t$에서 Average Reward $r(\pi)$를 추정한 값입니다. 이렇게 변형된 식을 알고리즘에 대입하기만 하면 이론적으로 큰 문제 없이 Average Reward에 대한 방법으로 바꿀 수 있습니다. 예를 들어, Semi-gradient Sarsa의 Average Reward 버전은 식 (10.2)를 다음과 같이 변경하면 됩니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w} + \alpha \delta_t \nabla \hat{q} (S_t, A_t, \mathbf{w}_t) \tag{10.12}\]

&lt;p&gt;식 (10.12)에서 $\delta_t$는 식 (10.11)과 같습니다. Differential Semi-gradient Sarsa 알고리즘의 완전한 Pseudocode는 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 알고리즘의 한 가지 문제점은 Differential Value로 수렴하지 않고 Differential Value에 임의의 Offset을 더한 값으로 수렴된다는 것입니다. 위에서 언급한 Bellman Equation과 TD Error는 모든 값이 같은 양만큼 변화해도 영향을 받지 않습니다. 따라서 Offset은 실제로 중요하지 않을 수 있지만, 이 Offset을 제거하기 위한 연구는 지금도 이루어지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 10.2) An Access-Control Queuing Task&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이 예제는 10개의 서버 집합에 대해 접근 제어와 관련된 결정 문제입니다. 이용자는 4가지 종류의 우선순위로 분류된 작업을 단일 Queue에 보냅니다. 서버에 대한 접근 권한이 부여되면 이용자는 우선 순위에 따라 서버에 1, 2, 4 또는 8의 비용을 지불하는데, 우선 순위가 높은 이용자는 더 많은 비용을 지불합니다. 각 시간 단계에서 Queue 맨 앞에 있는 이용자는 Accept (서버 중 하나에 할당)되거나 Reject (대기열에서 0의 Reward를 받고 제거) 됩니다. 어떤 결과가 나오든 다음 이용자의 순서로 넘어갑니다. Queue는 절때 비지 않고, Queue에 있는 이용자의 우선 순위는 무작위로 분포되어 있다고 가정합니다. 물론 여유있는 서버가 없다면 이용자의 요청은 모두 거부됩니다. 사용 중인 서버는 각각의 시간 단계에서 확률 $p = 0.06$에 따라 다시 여유 State로 돌아옵니다. 이용자의 도착 및 출발 통계는 알 수 없다고 가정하겠습니다. 이 문제의 목적은 이용자의 우선 순위와 여유 State의 서버 수에 따라 요청을 Accept할지, Reject 할지를 결정하는 것입니다. 즉, 장기적인 Reward를 극대화하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;교재에서는 이 문제를 Tabular 방식으로 해결하였습니다. State 간의 일반화는 없지만, 일반적인 함수 근사를 사용할 수는 있습니다. 따라서 각 State의 쌍(여유 State의 서버 수와 맨 앞에 있는 이용자의 우선 순위)과 Action(Accept 또는 Reject)에 대해 Differential Action-Value Function을 추정합니다. 위의 그림은 $\alpha = 0.01$, $\beta = 0.01$, $\epsilon = 0.1$로 설정한 Differential Semi-gradient Sarsa 해법을 보여줍니다. 초기의 Action-Value와 $\bar{R}$은 0으로 설정하였습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;deprecating-the-discounted-setting&quot;&gt;Deprecating the Discounted Setting&lt;/h2&gt;

&lt;p&gt;Continuing, Discounted Task에서는 각 State의 Return을 개별적으로 구분할 수 있는 Tabular에서 매우 유용했습니다. 하지만 근사를 사용하는 경우에는 Tabular에서 생각하지 못했던 여러 문제점이 발생하게 됩니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 시작이나 끝이 없고 State를 제대로 식별하지 못하는 무한 Return을 가정해봅시다. 각 State는 Feature Vector로만 표시할 수 있기 때문에 State를 구별하는 데 거의 도움이 되지 않습니다. 어떤 경우에는 모든 Feature Vector가 동일할 수도 있습니다. 따라서 Reward에 대한 순서만 주어지고 성능은 이러한 순서로만 평가될 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하는 방법 중 하나는 장기간에 걸쳐 Reward를 평균화하는 것입니다. 이것이 이전 Section에서 다룬 Average Reward의 아이디어입니다. 만약 Discount를 사용하고 싶다면 각 시간 단계에 대해 Discount된 Return을 측정할 수 있는데, 어떤 Return은 크고 어떤 Return은 작을 수 있으므로 충분히 긴 시간 단계에 걸쳐 평균을 내야 합니다. 그런데 간단히 생각해보면 결국 Discounted Return의 평균이 Reward의 평균에 비례한다는 것을 알 수 있습니다. 수학적으로 따져봐도 Policy $\pi$에 대해 Discounted Return의 평균은 $r(\pi) / ( 1 - \gamma)$입니다. 즉, 사실상 $r(\pi)$나 마찬가지입니다. 따라서 Discounted Return의 평균에서 Policy의 Ordering은 Average Reward에서와 동일하므로, Discount Factor $\gamma$는 문제에 영향을 끼치지 않습니다. Discount Factor는 $0 \le \gamma &amp;lt; 1$이기 때문입니다.&lt;/p&gt;

&lt;p&gt;이것을 엄밀하게 증명하기 전에, 먼저 직관적으로 간단하게 설명하겠습니다. (State를 제대로 식별하지 못하기 때문에) 각각의 시간 단계가 다른 모든 시간 단계와 동일하다고 가정해보겠습니다. Discount를 사용한 시간 단계 $t$의 Reward는 시간 단계 $t-1$의 Return에서 Discount를 사용하지 않은채 나오고, 시간 단계 $t-2$의 Return에서 1번 Discount 되며, 같은 이치로 시간 단계 $t-1000$의 Return에서 999번 Discount 됩니다. 따라서 시간 단계 $t$에서 Reward의 Weight는 $1 + \gamma + \gamma^2 + \gamma^3 + \cdots = 1/(1-\gamma)$가 됩니다. 모든 State가 동일하기 때문에 모든 State에 대해 이 Weight가 곱해지므로 Reward의 평균은 $r(\pi) / ( 1 - \gamma)$가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Futility of Discounting in Continuing Problems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이 부분에서는 Policy $\pi$에 따른 Discounted Value의 합이 어떻게 Average Reward 방법과 동일한지 수학적으로 식을 전개하도록 하겠습니다. Discounted Value Function을 $v_{\pi}^{\gamma}$라고 하면,&lt;/p&gt;

\[\begin{align}
J(\pi) &amp;amp;= \sum_s \mu_{\pi} (s) v_{\pi}^{\gamma} (s) \\ \\
&amp;amp;= \sum_s \mu_{\pi}(s) \sum_a \pi (a|s) \sum_{s&apos;} \sum_{r} p (s&apos;, r | s, a) \left[ r + \gamma v_{\pi}^{\gamma} (s&apos;) \right] \tag{Bellman Eq.} \\ \\
&amp;amp;= r(\pi) + \sum_s \mu_{\pi} (s) \sum_a \pi (a|s) \sum_{s&apos;} \sum_r p (s&apos;, r | s, a) \gamma v_{\pi}^{\gamma} (s&apos;) \tag{from (10.7)} \\ \\
&amp;amp;= r(\pi) + \gamma \sum_{&apos;s} v_{\pi}^{\gamma} (s&apos;) \sum_s \mu_{\pi} (s) \sum_a \pi (a|s) p (s&apos; | s, a) \tag{from (3.4)} \\ \\
&amp;amp;= r(\pi) + \gamma \sum_{s&apos;} v_{\pi}^{\gamma} (s&apos;) \mu_{\pi} (s&apos;) \tag{from (10.8)} \\ \\
&amp;amp;= r(\pi) + \gamma J(\pi) \\ \\
&amp;amp;= r(\pi) + \gamma r(\pi) + \gamma^2 J(\pi) \\ \\
&amp;amp;= r(\pi) + \gamma r(\pi) + \gamma^2 r(\pi) + \gamma^3 r(\pi)  + \cdots \\ \\
&amp;amp;= \frac{1}{1 - \gamma} r(\pi)
\end{align}\]

&lt;p&gt;즉, Discounted Value Function은 Discount되지 않은 Average Reward와 동일한 Policy를 생성하므로, Discount Factor $\gamma$는 영향을 끼치지 않는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;이 증명은 결국 On-policy Distribution에서 Discounted Value를 최적화하는 것과 Undiscounted Average Reward를 최적화 하는 것이 동일하다는 것을 의미합니다. $\gamma$가 실제로 어떤 값이든 상관이 없습니다. 즉, 이것은 함수 근사의 Control 문제에서 Discount가 아무런 역할을 하지 않는다는 것을 보여줍니다. 그럼에도 불구하고 Discount를 사용할 수 없는 것은 아닙니다. Discount Factor $\gamma$를 문제의 매개변수에서 해법의 매개변수로 바꾸면 됩니다. 하지만 그렇게 할 시 함수 근사를 사용하는 Discounting 알고리즘은 On-policy Distribution에서 Discounted Value를 최적화하지 않으므로 Average Reward를 최적화한다고 보장할 수 없습니다.&lt;/p&gt;

&lt;p&gt;Discount를 사용한 Control이 어려운 이유는 함수 근사를 사용하게 되면 Policy Improvement Theorem를 사용할 수 없기 때문입니다. 즉, State의 Discounted Value를 개선하기 위해 Policy를 변경하면 전체 Policy를 개선할 수 있다는 정리가 통용되지 않습니다. 이 정리는 지금까지의 강화학습 Control의 핵심 이론이었으나, 함수 근사를 사용함으로써 이 정리를 사용하는 것을 포기했기 때문입니다.&lt;/p&gt;

&lt;p&gt;사실 Discount를 사용한 Control 뿐만 아니라 전체 Episodic Task와 Average Reward에서도 Policy Improvement Theorem은 통하지 않습니다. 함수 근사를 사용하는 모든 방법은 Policy Improvement Theorem을 사용할 수 없기 때문입니다. 추후 13장에서 매개변수화된 Policy를 토대로 Policy Improvement Theorem과 유사한 정리인 Policy Gradient Theorem을 대안으로 사용할 예정입니다. 하지만 Action-Value를 학습하는 방법들은 Local Improvement에 대한 보장조차 없다고 밝혀졌습니다. (Perkins and Precup, 2003) 예를 들어, $\epsilon$-greedy를 사용하는 Policy는 때때로 비효율적인 Policy를 초래할 수 있기 때문입니다. (Gordon, 1996) 이것은 현재도 이론적인 논쟁과 연구가 이루어지고 있는 부분입니다.&lt;/p&gt;

&lt;h2 id=&quot;differential-semi-gradient-n-step-sarsa&quot;&gt;Differential Semi-gradient $n$-step Sarsa&lt;/h2&gt;

&lt;p&gt;Average Reward를 $n$-step Bootstrapping으로 일반화하려면 TD Error의 $n$-step 버전이 필요합니다. 함수 근사를 사용하여 식 (7.4)의 $n$-step Return을 Differential 형태로 일반화하면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} - \bar{R}_{t+n-1} + \cdots + R_{t+n} - \bar{R}_{t+n-1} + \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}) \tag{10.14}\]

&lt;p&gt;$\bar{R}$은 $r(\pi)$의 추정값이고, $n \ge 1$, $t + n &amp;lt; T$입니다. 만약 $t + n \ge T$라면 $G_{t:t+n} \doteq G_t$ 입니다. $n$-step TD Error는 다음과 같습니다.&lt;/p&gt;

\[\delta_t \doteq G_{t:t+n} - \hat{q}(S_t, A_t, \mathbf{w}) \tag{10.15}\]

&lt;p&gt;Update는 Semi-gradient Sarsa의 식 (10.12)를 그대로 사용합니다. &lt;span style=&quot;color:red&quot;&gt;Differential Semi-Gradient $n$-step Sarsa&lt;/span&gt;의 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/10. On-policy Control with Approximation/RL 10-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서는 지난 장에서 소개한 Parameterized Function Approximation 및 Semi-gradient Descent의 개념을 On-policy Control로 확장했습니다. Episodic Task에서는 확장하는 것이 크게 어렵지 않았으나, Continuing Task에서는 &lt;strong&gt;Average Reward&lt;/strong&gt;라는 새로운 개념을 도입했습니다. 왜냐하면 Discount를 사용한 식은 Function Approximation을 사용할 경우 Control에 있어 여러 이론적 문제가 발생했기 때문입니다. 따라서 Average Reward $r(\pi)$를 사용하여 &lt;strong&gt;Differential Value Function&lt;/strong&gt;과 &lt;strong&gt;Differential TD-Error&lt;/strong&gt;를 새롭게 정의하였습니다.&lt;/p&gt;

&lt;p&gt;다음 장에서는 Function Approximation을 사용한 Off-policy의 Control 방법에 대해 다루겠습니다.&lt;/p&gt;

&lt;p&gt;10장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">지난 장에서 근사를 이용한 Value Function Approximation에 대해 알아보았습니다. 이번 장에서는 매개변수를 사용하여 Action-Value Function $\hat{q}(s, a, \mathbf{w}) \approx q_* (s, a)$를 근사하는 Control 문제를 다루겠습니다. (Weight Vector $\mathbf{w} \in \mathbb{E}^d$는 유한 차원 Vector입니다) 이번 장에서는 먼저 On-policy에만 집중하고, Off-policy의 문제는 다음 장에서 다룰 예정입니다.</summary></entry><entry><title type="html">On-policy Prediction with Approximation</title><link href="http://localhost:4000/studies/on-policy-prediction-with-approximation/" rel="alternate" type="text/html" title="On-policy Prediction with Approximation" /><published>2022-05-17T00:00:00+09:00</published><updated>2022-05-17T00:00:00+09:00</updated><id>http://localhost:4000/studies/on-policy-prediction-with-approximation</id><content type="html" xml:base="http://localhost:4000/studies/on-policy-prediction-with-approximation/">&lt;h1 class=&quot;no_toc&quot; id=&quot;part-ii--approximate-solution-methods&quot;&gt;Part II : Approximate Solution Methods&lt;/h1&gt;

&lt;p&gt;2부에서는 1부에서 사용한 Tabular Method를 확장하여 매우 큰 State Space를 가진 문제에 적용합니다. 예를 들어, 카메라 이미지의 가능한 수는 우주의 원자 수보다 훨씬 많습니다. 이렇게 방대한 State Space를 가진 문제의 경우에는 시간과 데이터가 부족하기 때문에 지금까지 배운 Optimal Policy나 Optimal Value Function를 찾을 수 없습니다. 그렇기 때문에 이제부터는 제한된 계산 자원을 활용하여 정확하지 않더라도 그에 가까운 Approximate Solution을 대신 목표로 합니다.&lt;/p&gt;

&lt;p&gt;State Space가 클 때 Tabular Method를 사용하기 힘든 이유는 Table에 필요한 메모리 뿐만 아니라 Table을 정확하게 채우는 데 필요한 시간과 데이터도 부족하기 때문입니다. State Space가 매우 큰 경우, Episode마다 마주치게 되는 대부분의 State는 처음 방문하는 State가 됩니다. 이러한 환경에서 합리적인 의사결정을 하기 위해서는 현재 방문한 State와 유사한 다른 State에서의 기록을 토대로 일반화할 필요가 있습니다. 즉, Approximate Solution의 핵심 아이디어는 일반화입니다. State Space 중 일부의 Experience만을 토대로 좋은 Approximate Solution을 찾아야 합니다.&lt;/p&gt;

&lt;p&gt;다행스럽게도 이러한 일반화는 이미 많이 연구된 주제이며, 강화학습에 사용하기 위해 새로운 방법을 연구할 필요가 없습니다. 이번 장에서 다룰 대부분의 방법은 기존에 연구된 일반화 방법과 강화학습을 결합하는 것으로 이 문제를 해결합니다. 강화학습에서 필요한 일반화는 원하는 함수(주로 Value Function)의 표본을 토대로 전체 함수의 근사치를 구하기 위한 일반화이기 때문에, 이를 &lt;span style=&quot;color:red&quot;&gt;Function Approximation&lt;/span&gt;이라고도 합니다. Function Approximation은 Machine Learning, Neural Network, Pattern Recognition 및 Statistical Curve Fitting과 같은 &lt;strong&gt;Supervised Learning&lt;/strong&gt;의 한 예입니다. 이론적으로 이 분야에서 연구된 모든 방법은 강화학습에 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;그러나 Function Approximation을 사용한 강화학습에는 Nonstationarity, Bootstrapping, Delayed Target과 같은 기존의 Supervised Learning에서 발생하지 않는 여러가지 새로운 문제가 포함됩니다. 이것은 앞으로 5개의 장에 걸쳐 하나씩 소개할 예정입니다. 처음에는 On-policy로만 주제를 제한하며, 9장에서는 Policy가 주어져서 그것의 Value Function으로만 근사화되는 사례를 다루고, 10장에서는 근사를 이용해 Optimal Policy를 찾을 수 있는 Control 문제를 다룹니다. 11장부터는 Off-policy로 확장하여 12장에서는 $n$-step으로 확장한 Eligibility Trace를 소개하고 분석하며, 마지막 13장에서는 Optimal Policy를 직접 근사하고 Value Function을 근사할 필요가 없는 Control 방법인 Policy Gradient에 대해 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;이번 장에서는 먼저 On-policy 데이터에서 State-Value Function을 먼저 추정합니다. 주어진 Policy $\pi$를 사용하여 생성된 Experience에서 $v_{\pi}$를 근사하기 위해, 강화학습에서의 Function Approximation에 대해 설명합니다. 핵심적인 내용은 Approximate Value Function이 기존과 같은 Tabular가 아니라 Weight Vector $\mathbf{w} \in \mathbb{R}^d$를 매개변수로 갖는 함수로 표현된다는 것입니다. 이에 대한 표현으로, 주어진 Weight Vector $\mathbf{w}$와 주어진 State $s$에 대해, $\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)$를 사용합니다. Approximate Value Function $\hat{v}$를 구하는 방법에는 수많은 종류가 있으며, 이번 장에서 그 중 몇 가지를 살펴볼 예정입니다. 또한 Weight Vector $\mathbf{w}$ 원소의 수(즉, $\mathbf{w}$의 차원)는 State의 갯수보다 훨씬 적으며, 하나의 Weight를 변경해도 수많은 State의 Prediction Value가 변경됩니다. 또한 단 하나의 State가 Update되어도 이 변경 사항은 일반화되기 때문에 다른 State의 Value에도 영향을 주게 됩니다. 이러한 일반화는 학습을 강력하게 만들기도 하지만, 이렇게 관리하기 어렵게 만들기도 합니다.&lt;/p&gt;

&lt;p&gt;Function Approximation을 사용한 강화학습은 Agent가 부분적으로만 관찰할 수 있는 일부 State에도 적용할 수 있습니다. Value Function 자체가 매개변수화된 함수로 추정되기 때문에, State, 또는 State-Action 쌍이 입력되기만 한다면 그로부터 해당 Value가 계산되는 방식이기 때문입니다. 이것과 관련한 내용은 이 장보다 Section 17.3에서 간단하게 다룰 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;value-function-approximation&quot;&gt;Value-function Approximation&lt;/h2&gt;

&lt;p&gt;지금까지 이 책에서 다루었던 모든 Prediction 방법은 특정 State의 Value를 해당 State에 대한 &lt;strong&gt;Backed-up Value&lt;/strong&gt; 또는 &lt;strong&gt;Update Target&lt;/strong&gt;으로 표현하였습니다. 이제 각각의 개별 Update를 $s \mapsto u$로 표기하는데, $s$는 Updated State이고, $u$는 $s$의 Estimated Value가 이동하는 Update Target입니다. 예를 들어, Value Prediction을 위한 Monte Carlo Update는 $S_t \mapsto G_t$, TD(0) Update는 $S_t \mapsto R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t)$, 그리고 $n$-step TD Update는 $S_t \mapsto G_{t:t+n}$으로 표기할 수 있습니다. Dynamic Programming의 Policy Evaluation Update는 $s \mapsto \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t) \mid S_t = s \right]$로, 임의의 State $s$가 Update되지만, 다른 경우에는 Real Experience에서 발생한 State $S_t$가 Update됩니다.&lt;/p&gt;

&lt;p&gt;이 새로운 Update 표현 $s \mapsto u$는 State $s$에 대한 Estimated Value가 Update Target $u$와 더 유사해야 함을 의미합니다. 지금까지 실제 Update는 $s$의 Estimated Value에 대한 Table 값은 $u$를 향해 약간만 이동했으며, 다른 모든 State의 Estimated Value는 변하지 않았습니다. 이제부터는 Update에서 $s$의 Estimated Value가 변하면 다른 많은 State의 Estimated Value도 변경되도록 일반화합니다. 이렇게 입력-출력과 예시를 사용하여 모방하는 기계학습을 Supervised Learning이라고 하며, 출력이 $u$와 같은 숫자라면 이 과정을 &lt;span style=&quot;color:red&quot;&gt;Function Approximation&lt;/span&gt;이라고 합니다. Function Approximation화 방법은 근사하려는 함수의 입력-출력 예제를 통해 학습하며, Function Approximation가 완료되었을 때의 그 함수를 &lt;span style=&quot;color:red&quot;&gt;Estimated Value Function&lt;/span&gt;이라고 해석합니다.&lt;/p&gt;

&lt;p&gt;강화학습에서 Value Function Prediction을 위해 Function Approximation을 사용할 때, 원칙적으로는 Artificial Neural Network, Decision Tree 및 다양한 종류의 Multivariate Regression을 포함한 모든 Supervised Learning 방법을 사용할 수 있습니다. 그러나 모든 Function Approximation 방법이 강화학습에 적합한 것은 아닙니다. 강화학습에서는 Agent가 Environment, 또는 Environment Model과 상호작용하는 동안 학습이 온라인으로 수행되는 것이 중요하기 때문입니다. 이를 위해서는 점진적으로 얻어진 데이터를 효율적으로 학습할 수 있는 방법이 필요합니다. 또한 강화학습은 일반적으로 Nonstationary Target Function을 처리할 수 있는 Function Approximation 방법이 필요합니다. (즉, 시간이 지남에 따라 변경되는 Target Function) 예를 들어, Generalized Policy Iteration (GPI)에 기반한 Control에서 종종 $\pi$가 변경되는 동안 $q_{\pi}$를 학습하였습니다. Policy가 동일하게 유지되더라도 Dynamic Programming이나 Temporal-Difference Learning에서의 Bootstrapping 방법은 Target Value가 Nonstationary였습니다. 그렇기 때문에 Nonstationary 특성을 제대로 처리할 수 없는 방법은 강화학습에 적합하지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;the-prediction-objective-overlinetextve&quot;&gt;The Prediction Objective ($\overline{\text{VE}}$)&lt;/h2&gt;

&lt;p&gt;지금까지는 Prediction에 대한 명시적인 목표를 정하지 않았습니다. Tabular의 경우에는 학습된 Value Function이 Real Value Function과 정확히 같아질 수 있기 때문에 Prediction에 대한 성능 평가가 필요하지 않았습니다. 게다가 각각의 State에 대한 Update는 다른 State에 전혀 영향을 끼치지 않았습니다. 하지만 이제부터 다룰 근사적인 방법은 한 State의 Update가 다른 State에 영향을 미치는데다, 모든 State의 Value를 정확하게 구할 수도 없습니다. 왜냐하면 가정에 의해, Weight보다 훨씬 더 많은 State를 가지고 있으므로 한 State의 Estimated Value를 더 정확하게 만드는 행위는 항상 다른 State의 Estimated Value를 덜 정확하게 만들기 때문입니다. 따라서 어떤 State에 집중할 것인지 먼저 논의해보아야 합니다. State $s$에 대한 Error를 얼마나 중요하게 생각하는지 측정하기 위해, State Distribution $\mu(s) \ge 0, \sum_s \mu(s) = 1$을 정의합니다. 또한 State $s$에 대한 Error는 $\hat{v}(s, \mathbf{w})$와 $v_{\pi}(s)$ 차이의 제곱을 의미합니다. 이를 State Space에 대해 $\mu$만큼 Weight를 부여하면, Mean Square Value Error $\overline{\text{VE}}$는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\overline{\text{VE}}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) \left[ v_{\pi}(s) - \hat{v}(s, \mathbf{w}) \right]^2 \tag{9.1}\]

&lt;p&gt;식 (9.1)에 제곱근을 씌운 값은 근사한 Value Function이 Real Value Function과 얼마나 차이가 나는지를 대략적으로 측정하는데 사용됩니다. $\mu(s)$는 종종 State $s$에서 보낸 시간의 일부로 사용합니다. On-policy에서 이것은 On-policy Distribution이라고 하는데, 이번 장에서는 여기에 초점을 맞출 계획입니다. Continuing Task에서 On-policy Distribution은 $\pi$ 하에서의 Stationary Distribution입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The On-policy Distribution in Episodic Tasks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Episodic Task에서 On-policy Distribution은 Episode의 초기 State가 선택되는 방식에 따라 다릅니다. 한 Episode가 State $s$에서 시작될 확률을 $h(s)$라고 하고, 단일 Episode에서 State $s$에 소요된 평균 시간 단계 수를 $\eta(s)$라고 정의하면, Episode가 State $s$에서 시작하거나 이전 State $\bar{s}$에서부터 전이되었을 때 State $s$에 머문 시간은 다음과 같습니다.&lt;/p&gt;

\[\eta(s) = h(s) + \sum_{\bar{s}} \eta(\bar{s}) \sum_a \pi (a|\bar{s}) p(s|\bar{s}, a), \quad \text{for all } s \in \mathcal{S} \tag{9.2}\]

&lt;p&gt;이 식은 $\eta(s)$에 대해서도 풀 수 있습니다. On-policy Distribution은 다음과 같이 합이 1로 Normalize된 각 State에서 소요된 시간의 비율입니다.&lt;/p&gt;

\[\mu(s) = \frac{\eta(s)}{\sum_{s&apos;} \eta(s&apos;)}, \quad \text{for all } s \in \mathcal{S} \tag{9.3}\]

&lt;p&gt;이것은 Discounting을 고려하지 않은 경우입니다. 만약 Discounting ($\gamma &amp;lt; 1$)이 있는 경우에는 간단하게 식 (9.2)의 두 번째 항에 $\gamma$를 추가함으로써 해결할 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Continuing Task와 Episodic Task는 비슷하게 동작하지만, 엄밀하게 따지면 두 경우를 별개로 구분하여 각각 분석해야 합니다.&lt;/p&gt;

&lt;p&gt;다만 $\overline{\text{VE}}$가 강화학습에서 올바른 성능 평가의 목표인지는 명확하지 않습니다. Value Function을 학습하는 궁극적인 목적은 더 나은 Policy를 찾는 것인데, 이 목적을 위한 Optimal Value Function이 반드시 $\overline{\text{VE}}$를 최소화하지는 않기 때문입니다. 하지만 그럼에도 불구하고 $\overline{\text{VE}}$보다 더 나은 성능 평가의 척도가 없기 때문에 여기서는 $\overline{\text{VE}}$에 초점을 맞추도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;$\overline{\text{VE}}$의 관점에서 이상적인 목표는 모든 $\mathbf{w}$에 대해 $\overline{\text{VE}}(\mathbf{w}^{*}) \le \overline{\text{VE}}(\mathbf{w})$인 Global Optimum을 찾는 것입니다. 이것을 찾는 것은 Linear Approximation과 같은 간단한 Function Approximation에서는 가능하지만, Neural Network나 Decision Tree와 같은 복잡한 Function Approximation에서는 거의 불가능합니다. 따라서 복잡한 Function Approximation에서는 $\mathbf{w}^{*}$의 주변 $\mathbf{w}$에 대해 $\overline{\text{VE}}(\mathbf{w}^{*}) \le \overline{\text{VE}}(\mathbf{w})$를 만족하는 Local Optimum에 대신 수렴하게 만듭니다. 이정도가 non-Linear Approximation에서 할 수 있는 최대한이지만, 불행하게도 많은 경우에 그조차도 수렴한다는 보장이 없습니다. 몇몇 방법은 실제로 $\overline{\text{VE}}$가 무한대로 발산하기도 합니다.&lt;/p&gt;

&lt;p&gt;지금까지 두 Section에 걸쳐 Value Function의 Prediction을 위해 광범위한 강화학습 방법을 Function Approximation 방법과 결합하기 위한 아이디어를 설명했으며, 강화학습의 Update를 사용하여 Function Approximation을 위한 데이터를 생성했습니다. 또, Optimal Value Function Prediction을 위해 성능 평가의 척도인 $\overline{\text{VE}}$를 제안했습니다. Function Approximation 방법은 그 종류가 너무 많기 때문에 여기서 모두 고려할 수는 없고, 일단 여기에서는 Gradient에 기반한 방법, 특히 Linear Gradient Descent 방법에 중점을 두도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;stochastic-gradient-and-semi-gradient-methods&quot;&gt;Stochastic-gradient and Semi-gradient Methods&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 &lt;span style=&quot;color:red&quot;&gt;Stochastic Gradient Descent (SGD)&lt;/span&gt;를 기반으로 하는 Value Function Prediction 방법을 자세히 소개합니다. SGD는 모든 Function Approximation 방법 중 가장 널리 사용되는 방법 중 하나이며, 온라인 강화학습에 적합한 특징을 갖고 있습니다. Gradient Descent와 Stochastic Gradient Descent는 아래 포스트에도 소개되어 있으니 참고해주시면 좋겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/linear-models-2/&quot;&gt;[기계학습] 9. Linear Model II&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gradient Descent에서 Weight Vector $\mathbf{w} \doteq (w_1, w_2, \ldots, w_d)^{\sf T}$는 고정된 실수 값 요소를 갖는 Column Vector이고, $\hat{v}(s, \mathbf{w})$는 모든 State $s \in \mathcal{S}$에 대해 $\mathbf{w}$로 미분가능한 함수입니다. 이산적인 시간 단계 $t = 0, 1, 2, 3, \ldots$에 대한 Weight Vector $\mathbf{w}$의 값을 $\mathbf{w}_t$로 표기합니다. 또한 State $S_t$로부터 주어진 Policy에 따른 Real Value를 $S_t \mapsto v_{\pi} (S_t)$와 같이 정확하게 얻는다고 가정하겠습니다.&lt;/p&gt;

&lt;p&gt;먼저 식 (9.1)과 같이 $\overline{\text{VE}}$를 최소화하려는 것과 동일한 Distribution $\mu$를 가진 상황에서 State를 방문한다고 가정하겠습니다. 이 경우 가장 좋은 방법은 관찰된 데이터에서 오류를 최소화하는 것입니다. SGD는 각 데이터를 입력받은 후, 아래와 같이 해당 데이터의 오류를 줄이는 방향으로 Weight Vector $\mathbf{w}$를 조절합니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;\doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla \left[ v_{\pi}(S_t) - \hat{v}(S_t, \mathbf{w}_t) \right]^2 \tag{9.4} \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \left[ v_{\pi}(S_t) - \hat{v} (S_t, \mathbf{w}_t) \right] \nabla \hat{v}(S_t, \mathbf{w}_t) \tag{9.5}
\end{align}\]

&lt;p&gt;식 (9.4), (9.5)에서 $\alpha$는 Step-size Parameter이고, $\nabla f(\mathbf{w})$는 다음과 같이 Scalar 표현식인 $f(\mathbf{w})$를 Vector $\mathbf{w}$에 대해 편미분한 식입니다.&lt;/p&gt;

\[\nabla f(\mathbf{w}) \doteq \left( \frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, \cdots, \frac{\partial f(\mathbf{w})}{\partial w_d} \right)^{\sf T} \tag{9.6}\]

&lt;p&gt;식 (9.6)의 미분 Vector는 $\mathbf{w}$에 대한 $f$의 Gradient입니다. SGD는 Update가 완료될 때 &lt;strong&gt;Stochastic&lt;/strong&gt;이라고 하는데, 왜냐하면 확률적으로 선택된 단일 데이터(Sample)에 대해 Update하기 때문입니다. 많은 데이터에 대해 작은 단계를 수행한다면, 전체적인 효과는 $\overline{\text{VE}}$를 최소화하는 것과 같습니다.&lt;/p&gt;

&lt;p&gt;SGD를 포함한 Gradient Descent에서 Update 시 기울기 방향으로 작은 단계만을 이동하는데, 이것은 여러 데이터의 오류에 대한 균형을 맞추기 위함입니다. 극단적으로 데이터 하나가 들어올 때 그 데이터에서 오류가 0인 지점으로 (즉, 크게) 이동하면 해당 데이터에 대해 최소한의 오류만 가질 수 있으나, 그만큼 다른 데이터들에 대해 오류가 커지게 됩니다. 모든 State에 대해 오류가 0인 Value Function를 찾는 것은 목표도 아닐 뿐더러 목표로 할 수도 없습니다. 이 Step-size Parameter $\alpha$는 시간에 따라 감소하여 식 (2.7)의 조건을 충족시키도록 설계합니다. 이 조건을 만족하면 식 (9.5)의 SGD는 Local Optimum에 수렴합니다.&lt;/p&gt;

&lt;p&gt;이제 $t$번째 학습 데이터인 $S_t \mapsto U_t \in \mathbb{R}$을 따져보겠습니다. $U_t$는 $S_t$의 Real Value인 $v_{\pi}(S_t)$가 아니라 근사값입니다. 예를 들면 $U_t$는 $v_{\pi}(S_t)$에 노이즈가 추가된 버전일 수도 있고 $\hat{v}$를 사용하는 Bootstrapping 버전일 수도 있으며, 심지어는 경우에 따라 무작위 값일 수도 있습니다. 이런 경우에는 $v_{\pi}(S_t)$ 값을 알 수 없기 때문에 식 (9.5)를 수행할 수 없지만, $v_{\pi}(S_t)$ 대신 $U_t$를 사용하여 근사할 수도 있습니다. 이것은 State Value를 추정하기 위해 다음과 같은 일반적인 SGD 방법을 의미합니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ U_t - \hat{v}(S_t, \mathbf{w}_t) \right] \nabla \hat{v} (S_t, \mathbf{w}_t) \tag{9.7}\]

&lt;p&gt;만약 $U_t$가 Bias되지 않은 추정값인 경우(즉, $\mathbb{E} \left[ U_t \mid S_t = s \right] = v_{\pi} (s)$인 경우) 각각의 $t$에 대해 $\mathbf{w}_t$는 식 (2.7)과 같은 조건을 만족한다면 Local Optimum에 수렴하는 것이 보장됩니다. 예를 들어, Policy $\pi$에 따라 Environment(또는 Simulated Environment)와 상호작용에 의해 생성된 State를 가정하면, State의 Real Value $v_{\pi}$는 그 State 이후의 Reward에 대한 기대값이기 때문에 Monte Carlo의 Target인 $U_t \doteq G_t$는 그 정의에 의해 $v_{\pi}(S_t)$의 Bias되지 않은 추정값입니다. 이것을 사용한 식 (9.7)의 일반적인 SGD는 Local Optimum에 수렴하는 것이 보장됩니다. 따라서 Monte Carlo의 State Value 추정은 Gradient Descent을 통해 Local Optimum에 수렴하는 해법을 보장할 수 있습니다. Monte Carlo 버전에 대한 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하나 유의해야할 것은 $v_{\pi}(S_t)$의 Bootstrapping 추정이 식 (9.7)에서 Target $U_t$로 사용되는 경우, 동일한 수렴 보장을 얻을 수 없습니다. Bootstrapping의 Target은 $n$-step의 Return $G_{t:t+n}$ 또는 DP의 Target인 $\sum_{a, s’, r} \pi (a \mid S_t) p(s’,r, \mid S_t, a)\left[ r + \gamma \hat{v}(s’, \mathbf{w}_t \right]$처럼 Weight Vector $\mathbf{w}_t$의 현재 값에 의존하며, 이로 인해 Bias가 발생하고 Gradient Descent을 제대로 사용할 수 없습니다. 다행히 식 (9.4)와 (9.5)까지의 핵심 단계는 Target이 $\mathbf{w}_t$에 독립적입니다. 실제로 Bootstrapping 방법은 진정한 Gradient Descent의 인스턴스가 아니라고 밝혀졌으며(Barnard, 1993), Bootstrapping은 Estimated Value가 Weight Vector $\mathbf{w}_t$에는 영향을 끼치지만 Target에 대한 영향은 무시합니다. 즉, Gradient의 일부만 포함하기 때문에 Bootstrapping을 &lt;span style=&quot;color:red&quot;&gt;Semi-Gradient Method&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Semi-Gradient(=Bootstrapping) 방법은 Gradient 방법만큼 강력하게 수렴하지 않지만, 다음 Section에서 다룰 Linear 방법과 같은 특정한 경우에는 안정적으로 수렴합니다. 또한 어떤 경우에는 Semi-Gradient 방법의 장점이 유용한 경우도 있습니다. 대표적인 Semi-Gradient의 장점은 일반적으로 다른 방법보다 학습 속도가 빠르다는 것과 Episode가 끝날 때까지 기다릴 필요 없이 온라인으로 학습이 가능하다는 것입니다. 따라서 Continuing Task 같은 문제에 사용할 수 있는 이점이 있습니다. 기본적인 Semi-Gradient 방법은 $U_t \doteq R_{t+1} + \gamma \hat{v} (S_{t+1}, \mathbf{w})$를 Target으로 하는 Semi-Gradient TD(0)입니다. 이것의 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;State Aggregation은 각 그룹에 대해 하나의 Estimated Value(Weight Vector $\mathbf{w}$의 한 구성 요소)를 사용하여 State를 그룹화함으로써 Function Approximation을 일반화하는 간단한 형태입니다. State의 Value는 해당 그룹의 구성 요소로 추정되며, State가 Update되면 해당 구성 요소만 Update됩니다. State Aggregation은 Gradient $\nabla \hat{v} (S_t, \mathbf{w}_t)$가 State $S_t$ 그룹의 구성 요소에 대해 1이고 다른 구성 요소에 대해 0인 SGD (식 9.7)의 특별한 경우입니다.&lt;/p&gt;

&lt;h2 id=&quot;linear-methods&quot;&gt;Linear Methods&lt;/h2&gt;

&lt;p&gt;Function Approximation의 가장 중요하고 특별한 경우 중 하나는 Approximate Function $\hat{v}(\cdot, \mathbf{w})$가 Weight Vector $\mathbf{w}$에 대해 Linear인 경우입니다. Linear Method는 모든 State $s$에 대해 $\mathbf{w}$와 동일한 차원인 Real-valued Vector $\mathbf{x}(s) \doteq (x_1(s), x_2(s), \cdots, x_d(s))^{\sf T}$가 있으며, 다음과 같이 $\mathbf{w}$와 $\mathbf{x}(s)$ 사이의 내적으로 State의 Value를 근사화하는 방법입니다.&lt;/p&gt;

\[\hat{v} (s, \mathbf{w}) \doteq \mathbf{w}^{\sf T} \mathbf{x}(s) \doteq \sum_{i=1}^d w_i x_i (s) \tag{9.8}\]

&lt;p&gt;식 (9.8)과 같이 추정한 Value Function를 &lt;span style=&quot;color:red&quot;&gt;Linear in the Weights&lt;/span&gt;, 또는 단순히 &lt;span style=&quot;color:red&quot;&gt;Linear&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Vector $\mathbf{x}(s)$는 State $s$를 표현하는 &lt;span style=&quot;color:red&quot;&gt;Feature Vector&lt;/span&gt;라고 부릅니다. Vector $\mathbf{x}(s)$의 각 구성 요소인 $x_i (s)$는 함수 $x_i : \mathcal{S} \to \mathbb{R}$의 값입니다. 여기서는 Feature를 이러한 함수들의 전체 집단으로 간주하고, State에 대한 Value를 $s$에 대한 Feature라고 부릅니다. Linear Method에서 Feature는 Approximate Function들의 집합에 대한 Linear Basis를 형성하기 때문에 Basis Function입니다. State를 나타내기 위해 $d$차원 Feature Vector를 구성하는 것은 $d$ Basis Function의 집합을 선택하는 것과 동일합니다. Feature는 다양한 방법으로 정의될 수 있는데, 이는 다음 Section에서 자세히 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;다시 Function Approximation로 돌아와서, Linear Function의 근사를 SGD Update와 사용할 경우, Weight Vector $\mathbf{w}$에 대한 Approximate Value Function의 Gradient는 다음과 같습니다.&lt;/p&gt;

\[\nabla \hat{v}(s, \mathbf{w}) = \mathbf{x}(s)\]

&lt;p&gt;Linear의 경우 일반 SGD Update 식 (9.7)은 다음과 같이 간단하게 수정할 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \left[ U_t - \hat{v} (S_t, \mathbf{w}_t) \right] \mathbf{x}(S_t)\]

&lt;p&gt;Linear SGD는 매우 간단하기 때문에 수학적으로 분석하기 쉽습니다. 대부분의 학습 방법에서 유용한 수렴 결과는 거의 Linear나 그보다 간단한 Function Approximation 방법에 대한 것입니다. 특히 Linear의 경우 Optimum이 하나만 있기 때문에 Local Optimum에 수렴하는 것이 보장되기만 하면 Global Optimum, 또는 그 근처에 수렴하는 것이 보장됩니다. 예를 들어, 이전 Section에서 다룬 Gradient Monte Carlo 알고리즘은 $\alpha$가 시간이 지남에 따라 감소하는 일반적인 조건을 만족할 시 Linear Function Approximation에서 $\overline{\text{VE}}$의 Global Optimum에 수렴합니다.&lt;/p&gt;

&lt;p&gt;또한 Semi-Gradient TD(0) 알고리즘은 Linear Function Approximation에서도 수렴하지만, SGD의 일반적인 결과를 따르지 않기 때문에 별도의 Theorem이 필요합니다. 수렴되는 Weight Vector $\mathbf{w}$ 또한 Global Optimum이 아니라 Local Optimum에 가까운 지점이기 때문입니다. 특히 Continuing Task와 같은 특별한 경우를 고려해보면, 각 시간 $t$에서의 Update는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{w}_{t+1} &amp;amp;\doteq \mathbf{w}_t + \alpha \left( R_{t+1} + \gamma \mathbf{w}_t^{\sf T} \mathbf{x}_{t+1} - \mathbf{w}_t^{\sf T} \mathbf{x}_t \right) \mathbf{x}_t \tag{9.9} \\ \\
&amp;amp;= \mathbf{w}_t + \alpha \left( R_{t+1} \mathbf{x}_t - \mathbf{x}_t \left( \mathbf{x}_t - \gamma \mathbf{x}_{t+1} \right)^{\sf T} \mathbf{w}_t \right)
\end{align}\]

&lt;p&gt;식 (9.9)에서는 $\mathbf{x}_t = \mathbf{x}(S_t)$와 같이 표기를 간소화하여 사용했습니다. 시스템이 Steady State에 도달하면, 주어진 $\mathbf{w}_t$에 대해 예상되는 다음 Weight Vector를 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \mathbf{w}_{t+1} | \mathbf{w}_t \right] = \mathbf{w}_t + \alpha \left( \mathbf{b} - \mathbf{A} \mathbf{w}_t \right) \tag{9.10}\]

&lt;p&gt;식 (9.10)에서 $\mathbf{b}$와 $\mathbf{A}$는 다음과 같습니다.&lt;/p&gt;

\[\mathbf{b} \doteq \mathbb{E} \left[ R_{t+1} \mathbf{x}_t \right] \in \mathbb{R}^d \quad \text{and} \quad \mathbf{A} \doteq \mathbb{E} \left[ \mathbf{x}_t \left( \mathbf{x}_t - \gamma \mathbf{x}_{t+1} \right)^{\sf T} \right] \in \mathbb{R}^{d \times d} \tag{9.11}\]

&lt;p&gt;식 (9.10)에서 시스템이 수렴하는 경우, Weight Vector $\mathbf{w}_{\text{TD}}$로 수렴해야 합니다. 즉,&lt;/p&gt;

\[\begin{align}
\mathbf{b} - \mathbf{A} \mathbf{w}_{\text{TD}} &amp;amp;= \mathbf{0} \\ \\
\mathbf{b} &amp;amp;= \mathbf{A} \mathbf{w}_{\text{TD}} \\ \\
\mathbf{w}_{\text{TD}} &amp;amp;\doteq \mathbf{A}^{-1} \mathbf{b} \tag{9.12} \\ \\
\end{align}\]

&lt;p&gt;식 (9.12)와 같은 결과를 &lt;span style=&quot;color:red&quot;&gt;TD Fixed Point&lt;/span&gt;라고 합니다. 실제로 Linear Semi-Gradient TD(0)는 TD Fixed Point로 수렴합니다. 이 수렴성과 역행렬 $\mathbf{A}^{-1}$의 존재에 대한 증명은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of Convergence of Linear TD(0)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;식 (9.9)의 수렴을 보장하는 속성을 찾기 위해, 먼저 식 (9.10)을 다음과 같이 변형해보겠습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \mathbf{w}_{t+1} | \mathbf{w}_t \right] = \left( \mathbf{I} - \alpha \mathbf{A} \right) \mathbf{w}_t + \alpha \mathbf{b} \tag{9.13}\]

&lt;p&gt;식 (9.13)에서 행렬 $\mathbf{A}$는 Weight Vector $\mathbf{w}_t$를 곱하지만 $\mathbf{b}$는 곱하지 않습니다. 즉, 수렴하기 위해서는 $\mathbf{A}$만 중요합니다. 먼저, 행렬 $\mathbf{A}$가 Diagonal Matrix인 경우를 고려해보겠습니다. 만약 대각선 원소(=$A_{ij}$에서 $i = j$인 원소) 중에 하나라도 음수가 된다면 $\mathbf{I} - \alpha \mathbf{A}$의 대각선 원소가 1보다 커지고, 이로 인해 $\mathbf{w}_t$의 요소가 증폭되기 때문에 발산이 일어납니다. 반면에 $\mathbf{A}$의 대각선 원소가 모두 양수라면 $\alpha$는 가장 큰 원소보다 작게 선택될 수 있으므로, $\mathbf{I} - \alpha \mathbf{A}$는 0과 1 사이의 모든 대각선 원소가 있는 Diagonal Matrix입니다. 이 경우에는 Update 식의 첫 번째 항이 $\mathbf{w}_t$를 작게 만들기 때문에 안정적입니다. 일반적으로는 $\mathbf{A}$가 Positive Definite (=$y^{\sf T} \mathbf{A} y &amp;gt; 0$)일 때 $\mathbf{w}_t$가 0으로 감소합니다. 또한 Positive Definite 특성이 있다면 $\mathbf{A}$의 역행렬 $\mathbf{A}^{-1}$의 존재가 보장됩니다.&lt;/p&gt;

&lt;p&gt;Linear TD(0)의 경우 Continuing Task에서 $\gamma &amp;lt; 1$이라면 식 (9.11)의 행렬 $\mathbf{A}$는 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{A} &amp;amp;= \sum_s \mu (s) \sum_a \pi (a|s) \sum_{r, s&apos;} p (r, s&apos; | s, a) \mathbf{x}(s) \left( \mathbf{x}(s) - \gamma \mathbf{x}(s&apos;) \right)^{\sf T} \\ \\
&amp;amp;= \sum_s \mu (s) \sum_{s&apos;} p (s&apos;|s) \mathbf{x}(s) \left( \mathbf{x}(s) = \gamma \mathbf{x}(s&apos;) \right)^{\sf T} \\ \\
&amp;amp;= \sum_s \mu (s) \mathbf{x}(s) \left( \mathbf{x}(s) - \gamma \sum_{s&apos;} p(s&apos;|s) \mathbf{x}(s&apos;) \right)^{\sf T} \\ \\
&amp;amp;= \mathbf{X}^{\sf  T} \mathbf{D} (\mathbf{I} - \gamma \mathbf{P}) \mathbf{X}
\end{align}\]

&lt;p&gt;여기서 $\mu (s)$는 Policy $\pi$의 Stationary Distribution, $p(s’ \mid s)$는 Policy $\pi$에서 State $s$로부터 State $s’$으로의 Transition Probability, $\mathbf{P}$는 Transition Probability를 나타낸 행렬 $\lvert \mathcal{S} \rvert \times \lvert \mathcal{S} \rvert$, $\mathbf{D}$는 $\mu (s)$가 대각선 원소로 있는 Diagonal Matrix $\lvert \mathcal{S} \rvert \times \lvert \mathcal{S} \rvert$, 그리고 $\mathbf{X}$는 $\mathbf{x}(s)$가 행으로 있는 행렬 $\lvert \mathcal{S} \rvert \times d$ 입니다. 여기에 내부 행렬 $\mathbf{D} (\mathbf{I} - \gamma \mathbf{P})$는 행렬 $\mathbf{A}$의 Positive Definite 성질을 보장하는 핵심 부분입니다.&lt;/p&gt;

&lt;p&gt;이 핵심 행렬 $\mathbf{D} (\mathbf{I} - \gamma \mathbf{P})$은 모든 Column의 합계가 음수가 아니라면, Positive Definite가 보장됩니다. (Sutton, 1988) $\mathbf{D} (\mathbf{I} - \gamma \mathbf{P})$에서 대각선 원소는 양수이고, 그 외의 원소는 음수이므로 각 Row 합과 Column의 합이 양수라는 것을 보여야 합니다. $\mathbf{P}$가 Stochastic Matrix이고, $\gamma &amp;lt; 1$이므로 Row의 합은 모두 양수입니다. 행렬 $\mathbf{1}$을 모든 원소가 1인 Column Vector라고 할 때, 임의의 행렬 $\mathbf{M}$에 대해 Column의 합은 $\mathbf{1}^{\sf T} \mathbf{M}$으로 표현할 수 있습니다. $\boldsymbol{\mu}$를 $\mu (s)$의 $\mathcal{S}$-Vector라고 하면 $\boldsymbol{\mu} = \mathbf{P}^{\sf T} \boldsymbol{\mu}$가 성립하는데, $\mu$가 Stationary Distribution이기 때문입니다. 핵심 행렬의 Column 합은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
\mathbf{1}^{\sf T} \mathbf{D} (\mathbf{I} - \gamma \mathbf{P}) &amp;amp;= \boldsymbol{\mu}^{\sf T} - \gamma \boldsymbol{\mu}^{\sf T} \mathbf{P} \\ \\
&amp;amp;=\boldsymbol{\mu}^{\sf T} - \gamma \boldsymbol{\mu}^{\sf T} \mathbf{P} \\ \\
&amp;amp;=\boldsymbol{\mu}^{\sf T} - \gamma \boldsymbol{\mu}^{\sf T} \quad \text{(} \because \text{ } \boldsymbol{\mu} \text{ is the stationary distribution)} \\ \\
&amp;amp;= (1 - \gamma) \boldsymbol{\mu}^{\sf T}
\end{align}\]

&lt;p&gt;즉, 모든 구성 요소가 양수입니다. 따라서 이 핵심 행렬과 $\mathbf{A}$는 Positive Definite이며, On-policy TD(0)는 안정적입니다. 다만 Linear TD(0)가 확률 1로 수렴하는 것을 증명하기 위해서는 시간이 지남에 따라 $\alpha$가 작아진다는 것을 포함한 추가적인 조건들이 필요합니다. 여기서 그 부분은 생략하도록 하겠습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;TD Fixed Point에서 Continuing Task라면 $\overline{\text{VE}}$가 다음과 같이 가능한 가장 낮은 Error에 Bound하다는 것이 증명되었습니다.&lt;/p&gt;

\[\overline{\text{VE}}(\mathbf{w}_{\text{TD}}) \le \frac{1}{1 - \gamma} \min_{\mathbf{w}} \overline{\text{VE}}(\mathbf{w}) \tag{9.14}\]

&lt;p&gt;이것이 보여주는 것은 TD의 Asymptotic Error가 Monte Carlo Method에서 얻은 가능한 가장 작은 Error의 $\frac{1}{1 - \gamma}$배 이하라는 것입니다. $\gamma$는 종종 1에 가깝기 때문에 $\overline{\text{VE}}(\mathbf{w})$가 상당히 클 수 있으므로, TD의 경우 Asymptotic 성능이 상당히 저하될 가능성이 있습니다. 하지만 6장과 7장에서 배우듯, TD는 Monte Carlo Method에 비해 Variance가 크게 낮고 빠르기 때문에 각각의 장단점이 있습니다. 어떤 방법이 더 좋은지는 근사값과 문제의 환경, 그리고 학습이 얼마나 오래 지속되는지에 따라 다릅니다.&lt;/p&gt;

&lt;p&gt;식 (9.14)와 비슷한 Bound는 다른 On-policy Bootstrapping 방법에도 적용됩니다. 예를 들어, On-policy Distribution에 따라 Update된 식 (9.7)의 Linear Semi-Gradient DP 또한 TD Fixed Point로 수렴합니다. 다음 장에서 다룰 Semi-Gradient Sarsa(0)와 같은 1-step Semi-Gradient Action-Value 방법은 비슷한 Fixed Point와 Bound로 수렴합니다. Episodic Task의 경우에는 약간 다르지만 유사한 Bound가 있습니다. (Bertsekas and Tsitsiklis, 1996) Reward, Feature, 그리고 Step-size Parameter의 감소에 대한 여러 가지 기술적인 조건들도 있지만, 여기에서는 생략하겠습니다. 이것에 대한 자세한 내용은 (Tsitsiklis and Van Roy, 1997)의 논문에서 찾을 수 있습니다.&lt;/p&gt;

&lt;p&gt;지금까지 설명한 수렴 결과에 중요한 점은 On-policy Distribution에 따라 State가 Update된다는 것입니다. 다른 Distribution에 따른 Update의 경우 Function Approximation를 사용하는 Bootstrapping은 무한대로 발산할 수 있기 때문입니다. 이에 대한 예와 가능한 해결 방법은 11장에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;물론 지금까지 배운 내용을 1-step TD(0)에서만 사용할 수 있는 것은 아닙니다. 7장에서 배운 Tabular $n$-step TD 알고리즘을 Semi-Gradient Function Approximation로 확장한 Semi-Gradient $n$-step TD 알고리즘의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 알고리즘에서의 가장 중요한 식은 다음과 같이 식 (7.2)와 유사합니다. ($0 \le t &amp;lt; T$)&lt;/p&gt;

\[\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \left[ G_{t:t+n} - \hat{v}(S_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{v}(S_t, \mathbf{w}_{t+n-1}) \tag{9.15}\]

&lt;p&gt;이 때 $n$-step Return $G_{t:t+n}$은 다음과 같이 변형됩니다. ($0 \le t \le T - n$)&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \hat{v} (S_{t+n}, \mathbf{w}_{t+n-1}) \tag{9.16}\]

&lt;h2 id=&quot;feature-construction-for-linear-methods&quot;&gt;Feature Construction for Linear Methods&lt;/h2&gt;

&lt;p&gt;이전 Section에서 배운대로 Linear Method는 수렴을 보장하는 이점이 있습니다. 하지만 그 외에도 데이터 처리나 계산이 매우 효율적일 수 있다는 장점도 있습니다. &lt;strong&gt;~일 수 있다&lt;/strong&gt;라고 표현한 것처럼 모든 Linear Method가 그렇다는 것은 아니기 때문에 이번 Section에서는 다양한 Linear Feature를 하나하나 살펴보며 어떤 장점이 있는지, 그리고 강화학습에 얼마나 적합한지를 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;Linear 구조의 한계는 각각의 Feature에 대해 상호작용을 고려할 수 없다는 것입니다. 예를 들어, Pole-Balancing 문제에서 높은 Angular Velocity는 Angle에 따라 좋을 수도 있고 나쁠 수도 있습니다. Angle이 높은 상황에서 Angular Velocity가 높다면 Pole이 낙하할 위험이 크다는 것을 의미하고(나쁜 State), Angle이 낮은 상황에서 Angular Velocity가 높다면 Pole이 옳은 방향으로 가고 있음을 의미합니다(좋은 State). Linear 구조에서 이렇게 Feature 간에 상호작용을 고려하기 위해서는 추가적인 구현이 필요합니다. 이어지는 하위 Section에서는 이를 위한 다양한 방법을 소개합니다.&lt;/p&gt;

&lt;h3 id=&quot;polynomials&quot;&gt;Polynomials&lt;/h3&gt;

&lt;p&gt;많은 문제들에서 State는 Pole-Balancing 문제에서의 위치와 속도처럼 숫자로 표시됩니다. 이런 유형의 문제에서의 강화학습을 위한 Function Approximation는 Interpolation 및 Regression 작업과 많은 공통점이 있기 때문에, Interpolation 및 Regression에서 사용되는 Feature들 또한 강화학습에 사용할 수 있습니다. &lt;span style=&quot;color:red&quot;&gt;Polynomial&lt;/span&gt;은 Interpolation 및 Regression에서 사용되는 가장 단순한 Feature 중 하나로써, 간단하고 친숙하기 때문에 가장 먼저 소개합니다. 하지만 그 단순함 때문에 이후에 소개될 Feature들에 비해 성능은 좋다고 볼 수 없습니다.&lt;/p&gt;

&lt;p&gt;Polynomial Feature를 이해하기 쉽게 예를 들어 설명해보겠습니다. 임의의 강화학습 문제에서 두 개의 숫자 차원이 있는 State가 있다고 가정합니다. 이 강화학습에서 State $s$는 두 숫자 $s_1 \in \mathbb{R}$, $s_2 \in \mathbb{R}$로 이루어져 있으며, 이를 Feature Vector로 표현하면 $\mathbf{x}(s) = (s_1, s_2)^{\sf T}$로 나타낼 수 있습니다. 하지만 이렇게 표현하는 경우 두 개의 차원 간 상호작용을 고려할 수 없다는 것과, $s_1$과 $s_2$가 모두 0이라면 근사값도 0이어야 한다는 문제가 있습니다. 이를 해결하는 간단한 방법은 State의 Feature Vector $\mathbf{x}(s)$를 4차원으로 늘려 $\mathbf{x}(s) = (1, s_1, s_2, s_1 s_2)^{\sf T}$로 만드는 방법이 있습니다. 첫 차원의 1은 원래 State의 숫자를 &lt;strong&gt;Affine Function&lt;/strong&gt;으로 표현할 수 있고, 마지막 차원의 $s_1 s_2$을 사용하면 $s_1$과 $s_2$의 상호작용을 고려할 수 있습니다. 더 복잡한 상호작용을 고려하고 싶다면 Feature Vector의 차원을 더욱 늘려 $\mathbf{x}(s) = (1, s_1, s_2, s_1 s_2, s_1^2, s_2^2, s_1 s_2^2, s_1^2 s_2, s_1^2 s_2^2)^{\sf T}$와 같이 표현할 수도 있습니다. 이러한 Feature Vector는 복잡해보이지만, Weight에 기반한 Linear Approximation은 임의의 이차함수로 근사가 가능합니다. 이 예시를 일반화한다면 $k$개의 차원 간의 매우 복잡한 상호작용도 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Polynomial Feature 상호작용의 일반화&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;각각의 State $s$가 $s_i \in \mathbb{R}$인 $k$개의 숫자 $s_1$, $s_2$, $\ldots$, $s_k$로 이루어져있다고 가정한다. 이 $k$개 차원의 State Space에 대해, $n$차 Polynomial Basis Feature $x_i$는 다음과 같이 표현할 수 있다.&lt;/p&gt;

\[x_i = \prod_{j=1}^k s_j^{c_{i, j}} \tag{9.17}\]

&lt;p&gt;이 때 $c_{i, j} \in \{ 0, 1, \ldots, n \}$이며(단, $n \ge 0$인 정수), 이 Feature는 $(n + 1)^k$개의 서로 다른 Feature를 포함하는 $k$ 차원에 대해 $n$차 Polynomial Basis를 구성한다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;고차 Polynomial Base를 사용하면 더 복잡한 함수를 더 정확하게 근사할 수 있습니다. 그러나 $n$차 Polynomial Basis의 Feature 수는 차원 $k$에 따라 기하급수적으로 증가하기 때문에 일반적으로 Function Approximation를 위해서는 이것들의 부분 집합을 선택해야 합니다. 이것은 근사화할 함수에 대한 Prior Belief로 수행할 수 있으며, Polynomial Regression를 위해 개발된 여러 선택 방법을 적용하여 강화학습의 Incremental 및 Nonstationary 특성을 처리할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;fourier-basis&quot;&gt;Fourier Basis&lt;/h3&gt;

&lt;p&gt;또 다른 Linear Function Approximation은 &lt;span style=&quot;color:red&quot;&gt;Fourier Series&lt;/span&gt;에 기반하여 만들 수 있습니다. Fourier Series는 서로 다른 Frequency를 가진 Sine 및 Cosine Basis Feature의 Weighted Sum으로 표현합니다. Fourier Series와 Fourier Transform은 Approximate Function가 알려진 경우 Basis Function의 Weight는 간단한 식으로 표현되며, Basis Function이 충분하면 기본적으로 모든 함수를 정확하게 근사할 수 있습니다. 강화학습에서 Approximate Function은 알려지지 않았기 때문에 Fourier Basis Function은 사용하기 쉽고, 다양한 강화학습 문제에서 잘 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;먼저 1차원 환경을 고려해보겠습니다. $\tau$의 Period를 갖는 1차원 함수에서 일반적인 Fourier Series 표현은 Period를 $\tau$로 나눈 Sine과 Cosine 함수의 Linear 조합으로 나타납니다. 그러나 만약 Bounded Interval에서 정의된 Aperiodic Function을 근사하기 위해서는 $\tau$를 구간의 길이로 설정하여 Fourier Basis Function을 사용할 수 있습니다. 그렇게 되면 Aperiodic Function은 Sine 및 Cosine Feature의 Periodic Linear 조합의 한 Period가 됩니다.&lt;/p&gt;

&lt;p&gt;또한 $\tau$를 구간 길이의 2배로 설정하고, 구간의 절반인 $\left[ 0, \tau / 2 \right]$에서 근사값을 제한하면 Cosine Feature만 사용할 수 있습니다. Cosine Feature만 사용한다면 $y$축에 대칭인 Even Function으로 나타낼 수 있습니다. 따라서 Half-Period $\left[ 0, \tau / 2 \right]$에 대한 모든 함수는 Cosine Feature를 사용하여 근사할 수 있습니다. 마찬가지로 Sine Feature만을 사용하면 원점에 대해 대칭인 Odd Function을 만들 수 있습니다. 그러나 Half-Even Function이 Half-Odd Function보다 근사하기 쉽기 때문에 일반적으로 Cosine Feature만 사용하는 것이 좋습니다. 특히 Half-Odd Function은 종종 원점에서 불연속이기 때문입니다. 물론 일부 특정 상황에서는 Sine 및 Cosine Feature를 모두 사용할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 논리에 따라 $\tau = 2$로 설정하고 Feature는 $\tau$의 Half-Interval $\left[ 0, 1 \right]$에서 정의되도록 하면, 1차원 $n$차 Fourier Cosine Basis는 다음과 같은 $n + 1$ Feature로 표현됩니다.&lt;/p&gt;

\[x_i (s) = \cos (i \pi s), \quad s \in \left[ 0, 1 \right], \quad i = 0, \ldots n\]

&lt;p&gt;아래의 그림은 $i = 1, 2, 3, 4$인 경우 1차원 Fourier Cosine Feature $x_i$를 나타냅니다. (단, $x_0$은 상수 함수입니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 확장하여 Fourier Cosine Series를 다차원으로 일반화한 근사는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fourier Cosine Series의 일반화&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;각각의 State $s$가 $s_i \in \left[ 0, 1\right]$인 $k$개의 숫자로 이루어진 Vector $\mathbf{s} = (s_1, s_2, \ldots, s_k)^{\sf T}$로 이루어져있다고 가정한다. $n$차 Fourier Cosine Basis의 $i$번째 Feature는 다음과 같이 표현할 수 있다.&lt;/p&gt;

\[x_i (s) = \cos (\pi \mathbf{s}^{\sf T} \mathbf{c}^i) \tag{9.18}\]

&lt;p&gt;식 (9.18)에서 $\mathbf{c}^i = (c_1^i, \ldots, c_k^i)^{\sf T}$는 $j = 1, \ldots, k$와 $i = 1, \ldots, (n+1)^k$에 대해 $c_j^i \in \{ 0, \ldots, n \}$로 이루어져 있다. 이것은 $(n+1)^k$개의 가능한 정수 Vector $\mathbf{c}^i$에 대한 각각의 Feature를 정의한다. Vector $\mathbf{s}^{\sf T}$와 $\mathbf{c}^i$의 Inner Product $\mathbf{s}^{\sf T} \mathbf{c}^i$는 정수 $\{ 0, \ldots, n \}$를 $\mathbf{s}$의 각 차원에 할당하는 효과를 갖는다. 1차원의 경우 이 정수값은 해당 차원에서 Feature의 Frequency를 결정한다. 물론 이 Feature는 응용 프로그램에 따라 제한된 State Space에 맞게 이동 및 확장될 수 있다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;예를 들어 $\mathbf{s} = (s_1, s_2)^{\sf T}$인 경우를 고려해보겠습니다. (즉, $k = 2$) 여기서 $\mathbf{c}^i = (c_1^i, c_2^i)^{\sf T}$입니다. 아래의 그림은 6개의 Fourier Cosine Feature를 선택하여 보여줍니다. 각각의 그림은 이를 정의하는 Vector $\mathbf{c}^i$에 따라 어떻게 표현되는지 나타나 있습니다. $\mathbf{c}$에서 0이 의미하는 것은 해당 State가 차원에 따라 Feature가 일정함을 의미합니다. 따라서 만약 $\mathbf{c} = (0, 0)^{\sf T}$라면 Feature는 두 차원 모두 일정하다는 뜻입니다. 만약 $\mathbf{c} = (c_1, 0)^{\sf T}$이라면 Feature는 첫 번째 차원에서는 $c_1$에 따라 Frequency가 변하지만 두 번째 차원에서 일정합니다. 당연히 $\mathbf{c} = (0, c_2)^{\sf T}$라면 그 반대가 됩니다. 만약 $\mathbf{c} = (c_1, c_2)^{\sf T}$라면 Feature가 두 차원에 따라 달라지며 두 State 변수 간의 상호작용을 나타냅니다. $c_1$과 $c_2$의 값은 각 차원에 따른 Frequency를 결정하고 이들의 비율은 상호작용의 방향을 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fourier Cosine Feature를 사용할 때 식 (9.7)과 같은 학습 알고리즘을 사용한다면 각 Feature에 대해 다른 Step-size Parameter를 사용하는 것이 좋을 수도 있습니다. 만약 $\alpha$가 기본 Step-size Parameter라면 Feature $x_i$에 대해 Step-size Parameter $\alpha_i$를 $\alpha / \sqrt{(c_1^i)^2 + \cdots + (c_k^i)^2}$로 설정할 수도 있습니다. (Konidaris, Osentoski and Thomas, 2011) 단, $c_j^i = 0$인 경우는 $\alpha_i = \alpha$로 설정합니다.&lt;/p&gt;

&lt;p&gt;Sarsa의 Fourier Cosine Feature는 앞서 설명한 Polynomial이나 뒤에 나올 Radial Basis Function을 비슷한 여러 Basis에 비해 우수한 성능을 보일 수 있습니다. 그러나 Fourier Feature는 매우 높은 Frequency Basis Function이 포함되어 있지 않는 한, 불연속 지점 주변에서 &lt;strong&gt;Ringing&lt;/strong&gt;을 피하기 어렵기 때문에 불연속성에서 문제가 있습니다.&lt;/p&gt;

&lt;p&gt;$n$차 Fourier Basis의 Feature 수는 State Space의 차원에 따라 기하급수적으로 증가하지만, 해당 차원이 작은 경우라면(ex. $k \le 5$) 모든 $n$차 Fourier Feature를 사용할 수 있도록 $n$을 선택할 수 있습니다. 하지만 고차원 State Space의 경우에는 이러한 Feature의 하위 집합을 선택해야 합니다. Polynomial 때와 마찬가지로 근사화할 함수에 대한 Prior Belief를 사용하여 선택할 수 있으며, 몇몇 자동화된 선택 방법을 사용하여 강화학습의 Incremental 및 Nonstationary 특성을 처리할 수 있습니다. 이 때 Fourier Basis Feature의 장점은 $\mathbf{c}^i$ Vector를 설정하여 State 변수 간의 상호작용을 처리하고 근사 함수가 소음으로 간주될 수 있는 높은 Frequency 성분들을 걸러낼 수 있도록 $\mathbf{c}^i$ Vector의 값을 제한함으로써 Feature를 쉽게 선택할 수 있습니다. 반면에 Fourier Feature는 Global Property를 나타내는 전체 State Space에서 0이 아니기 때문에 Local Property를 나타내는 좋은 방법을 찾기 어려울 수 있습니다.&lt;/p&gt;

&lt;p&gt;아래의 그림은 1000개의 State를 가진 Random Walk 예제에서 Fourier와 Polynomial의 Basis를 비교하는 학습 곡선을 나타냅니다. 보시다시피 Fourier Basis의 성능이 월등히 좋기 때문에 일반적인 온라인 학습에는 Polynomial을 사용하지 않는 것이 좋습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;coarse-coding&quot;&gt;Coarse Coding&lt;/h3&gt;

&lt;p&gt;State 집합을 연속적인 2차원 공간에 표현하는 상황을 고려해보겠습니다. 이 경우 표현할 수 있는 방법 중 하나는 아래 그림처럼 State Space에 원과 같은 모양의 Feature를 생각해볼 수 있습니다. State가 원 안에 있으면 해당 Feature의 값은 1이고, 이 때 &lt;span style=&quot;color:red&quot;&gt;Present&lt;/span&gt;한다고 합니다. 반대로 그렇지 않다면 Feature의 값은 0이고, 이 때는 &lt;span style=&quot;color:red&quot;&gt;Absent&lt;/span&gt;라고 합니다. 이렇게 1-0으로 표현하는 Feature를 Binary Feature라고 합니다. 주어진 State에서 어떤 Binary Feature가 있는지는 해당 State가 속한 원을 나타냅니다. 이러한 방식으로 겹치는 Feature가 있는 State를 나타내는 것을 &lt;span style=&quot;color:red&quot;&gt;Coarse Coding&lt;/span&gt;이라고 합니다. 다만 꼭 아래 그림처럼 원 모양일 필요는 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Linear Gradient Descent을 사용한 근사라고 가정하면, 원의 크기와 밀도가 어떤 영향을 끼치는지 고려해봐야 합니다. 각각의 원은 학습에 의해 영향을 받는 단일 Weight(즉, $\mathbf{w}$의 구성 요소)입니다. State Space의 한 State에서 훈련을 하면, 해당 State를 포함하는 모든 원의 Weight가 영향을 받습니다. 따라서 식 (9.8)에 의해 근사 Value Function는 원의 합집합 내의 모든 State에 영향을 미치며, 한 점이 많은 State에 속해 있을수록 더 큰 영향을 미칩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 Coarse Coding의 여러 일반화 형태를 나타내고 있습니다. 왼쪽의 그림처럼 원의 작다면 일반화가 보다 좁은 영역에서 이루어지며, 중간의 그림처럼 원이 크다면 일반화가 보다 넓은 영역에서 이루어집니다. 게다가 Feature의 형태는 일반화의 특성에도 영향을 미칩니다. 오른쪽의 그림처럼 좁은 타원 형태라면 일반화도 그에 맞춰 영향을 받게 됩니다.&lt;/p&gt;

&lt;p&gt;큰 &lt;strong&gt;Receptive Field&lt;/strong&gt;가 있는 Feature는 광범위한 일반화를 제공하지만, 학습된 Feature를 대략적인 근사로 제한하기 때문에 일반화가 너무 대략적으로 나오는 것처럼 보일 수 있습니다. 초기 일반화는 실제로 Receptive Field의 크기와 모양에 의해 결정되지만, 궁극적인 추정 결과의 정확도는 전체 Feature 수에 의해 더 많이 영향을 받습니다. 이것은 다음 예제를 통해 자세하게 설명하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 9.3) Coarseness of Coarse Coding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이 예제는 Coarse Coding에서 Receptive Field의 크기에 따른 학습 효과를 보여줍니다. Coarse Coding 및 식 (9.7)에 기반한 Linear Function Approximation는 1차원 Square-wave 함수를 학습하는 데 사용되었습니다. 이 함수의 Target은 $U_t$로 설정되었고, 단 하나의 차원에서 Receptive Field는 원 대신 Interval이었습니다. 학습은 아래 그림과 같이 세 가지 다른 길이의 너비(Narrow, Medium, Broad)로 반복되었습니다. 또한 세 가지 모두 학습되는 Feature의 범위에 대해 약 50개의 동일한 Density를 가졌습니다. Training Data(Sample)는 이 범위에서 무작위로 균일하게 생성되었습니다. Step-size Parameter는 한 번에 존재했던 Feature의 수를 $n$이라 할 때 $\alpha = \frac{0.2}{n}$으로 설정되었습니다. 아래의 그림은 세 가지 경우에서 각각 학습했을 때의 Feature를 나타냅니다. 보시다시피 Feature의 너비는 학습 초기에 큰 영향을 끼칩니다. 하지만 최종 결과를 보시면 Feature의 너비에 그다지 큰 영향을 끼치지 않는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h3 id=&quot;tile-coding&quot;&gt;Tile Coding&lt;/h3&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Tile Coding&lt;/span&gt;은 Coarse Coding의 한 형태로써 다차원 연속 공간에서 유연하고 계산이 효율적인 특징을 가지고 있습니다. 교재에서는 이 방법을 현대 Sequential 디지털 컴퓨터에서 가장 실용적인 Feature라고 주장하고 있습니다.&lt;/p&gt;

&lt;p&gt;Tile Coding에서 Feature의 Receptive Field는 State Space의 Partition으로 그룹화됩니다. 이러한 각 Partition을 &lt;span style=&quot;color:red&quot;&gt;Tiling&lt;/span&gt;이라고 하며, Partition의 각 요소를 &lt;span style=&quot;color:red&quot;&gt;Tile&lt;/span&gt;이라고 합니다. 예를 들어, 2차원 State Space의 가장 단순한 Tiling은 아래 그림의 왼쪽에 나타난 것과 같이 균일한 격자입니다. 여기에서 Tile(=Receptive Field)은 사각형입니다. 이 단일 Tiling만 사용할 경우, 흰색 점으로 표시된 State는 Tile이 속하는 단일 Feature로 표현됩니다. 일반화는 동일한 Tile 내의 모든 State에 대해 완전하고, Tile 외부 State에는 존재하지 않습니다. 하나의 Tiling인 경우 Coarse Coding 보다는 &lt;strong&gt;State Aggregation&lt;/strong&gt;에 가깝습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Coarse Coding의 장점을 얻으려면 겹치는 Receptive Field가 필요한데, Tile Coding의 정의상 Partition의 Tile은 겹치지 않습니다. 이를 해결하기 위해서는 여러 Tiling을 사용하며, 각 Tiling은 Tile 너비의 일부로 설정합니다. 위 그림의 오른쪽은 4개의 Tiling을 사용한 경우입니다. 흰색 점으로 표시된 것과 같이 모든 State는 4개의 Tiling 각각에서 정확히 하나의 Tile에 속합니다. 이 4개의 Tile은 State에 도달할 때 활성화되는 4개의 Feature에 해당합니다. 특히, Feature Vector $\mathbf{x}(s)$는 각 Tiling에서 각각의 Tile에 대해 하나의 구성 요소를 갖습니다. 예를 들어, 위의 그림에서는 $4 \times 4 \times 4 = 64$개의 구성 요소가 있습니다. 이 중 State $s$가 속하는 Tile 4개를 제외하고는 모두 0의 값이 됩니다.&lt;/p&gt;

&lt;p&gt;Tile Coding의 실제 장점은 Partition과 함께 작동하기 때문에 한 번에 활성화되는 전체 Feature 수가 모든 State에서 동일하다는 것입니다. 각 Tiling에는 정확히 하나의 Feature가 있으므로 존재하는 Feature의 총 수는 항상 Tiling 수와 동일합니다. 이것으로 인해 Step-size Parameter $\alpha$를 쉽고 직관적인 방식으로 설정할 수 있습니다. 예를 들어 Tiling 수를 $n$이라 할 때, $\alpha = \frac{1}{n}$로 설정하면 정확히 1회 학습이 됩니다. 이 때 만약 데이터 $s \to v$가 학습된 경우, 이전 Estimated Value $\hat{v} (s, \mathbf{w}_t)$가 무엇이든 상관없이 새로운 Estimated Value는 $\hat{v} (s, \mathbf{w}_t) = v$가 됩니다. 다만 일반적으로 Target Output의 일반화나 &lt;strong&gt;Stochastic Variation&lt;/strong&gt;을 위해 이보다 더 천천히 변경하는 것을 선호하기 때문에, $\alpha = \frac{1}{10n}$로 선택한다면 한 번의 Update로 Target 까지 $\frac{1}{10}$만큼 이동하고, 인접한 State는 Tile 수에 비례하여 덜 이동합니다.&lt;/p&gt;

&lt;p&gt;Tile Coding은 또한 Binary Feature Vector를 사용하여 계산상의 이점을 얻습니다. 각각의 구성 요소가 0 또는 1이므로 식 (9.8)과 같은 Approximate Value Function에서 Weighted Sum을 계산하기 매우 쉽기 때문입니다. 식 (9.8)에서 곱셈 연산을 수행할 필요 없이, 구성 요소가 1인 항목들만 전부 더하면 끝입니다.&lt;/p&gt;

&lt;p&gt;일반화는 학습이 끝난 State가 아닌 다른 State가 공통된 Tile 수에 비례하여 동일한 Tile에 속하는 경우에 발생합니다. Tiling을 서로 Offset하는 방법의 선택조차도 일반화에 영향을 줍니다. 만약 Tiling의 Offset이 각 차원마다 균일하게 설정되어 있으면, 아래 그림의 위쪽과 같이 다른 State는 다른 방식으로 일반화됩니다. 위쪽의 8개의 그림은 각각 학습된 State에서 가까운 지점으로 일반화한 패턴을 보여줍니다. 이 예시에서는 8개의 Tiling이 있으므로 Tile 내 64개의 하위 영역이 명확하게 일반화되고, 모두 이 8개의 패턴 중 하나에 따릅니다. 문제는 Tiling이 균일할 때 대각선 요소의 영향력이 매우 강하다는 것입니다. (ex. 6번째와 8번째 그림의 비교) 만약 아래쪽 그림처럼 Tiling의 Offset이 비대칭적으로 설정되면 이러한 효과를 피할 수 있습니다. 이것이 바로 비대징 Offset이 Tile Coding에서 선호되는 이유입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모든 경우에서 Tiling은 각 차원의 Tile 너비 일부만큼 서로 Offset이 설정됩니다. Tile의 너비를 $w$라 하고 Tile의 수를 $n$으로 한다면 $\frac{w}{n}$이 기본 단위입니다. 작은 사각형 $\frac{w}{n}$ 안의 모든 State들은 같은 Tile이 활성화되고, 같은 Feature와 같은 근사값을 같습니다. State가 아무 방향으로 $\frac{w}{n}$ 만큼 이동하면 Feature가 하나의 구성 요소/Tile 만큼 변경됩니다. 균일하게 Offset이 설정된 Tiling은 정확히 이 단위 거리만큼 Offset이 설정됩니다. 2차원 공간에서 각 Tiling은 Displacement Vector에 의해 Offset이 설정되며, 이는 Tiling의 Offset이 이 Vector에서 $\frac{w}{n}$ 를 곱한 만큼 설정된다는 의미입니다. 예를 들어, 위의 그림 중 아래쪽(Tiling이 비대칭인 경우)은 Displacement Vector (1, 3)에 의해 Offset 됩니다. Tile Coding의 일반화에서 다양한 Displacement Vector의 영향은 많은 연구가 이루어졌고, 교재에도 이에 대한 설명이 나와있으나, 여기에서는 생략하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Tiling을 설계한다는 것은 Tiling의 수와 Tile의 모양을 선택하는 것과 같습니다. Tiling의 수는 Tile의 크기와 함께 Asymptotic Approximation의 Resolution과 Fineness를 결정합니다. 또한 Tile의 모양은 일반화에 대한 특징을 결정합니다. (Coarse Coding 부분 참고) 예를 들어, 위에서 다룬 정사각형 모양의 Tile은 각각의 차원에서 거의 동일하게 일반화됩니다. 아래 그림은 Tile의 다양한 모양을 나타내고 있습니다. 가운데의 수직 줄무늬 모양 Tile은 왼쪽에서 더 촘촘하고 얇게 디자인되어 있기 때문에 수평 차원을 기준으로 낮은 값일 수록 촘촘한 분류가 가능합니다. 오른쪽 그림의 대각선 줄무늬 Tiling은 보는 바와 같이 대각선을 따라 일반화를 합니다. 왼쪽과 같은 불규칙한 Tiling도 가능하지만, 실제로 많이 쓰는 방법은 아닙니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실제로 Tiling을 설계할 때는 다른 Tiling에 다른 모양의 Tile을 사용하는 것이 좋습니다. 예를 들어, 한 Tiling은 수직 줄무늬, 다른 Tiling은 수평 줄무늬를 사용하는 방법이 있습니다. 이것은 어느 차원에서든 일반화를 가능케 하지만, 줄무늬 모양의 Tiling 만으로는 수평 및 수직 좌표의 특정 조합이 고유한 값을 갖는다는 것을 학습할 수 없습니다. 이를 위해서는 직사각형 모양의 Tile이 필요합니다. 여러 Tiling을 조합하여 사용하면 이런 문제를 해결할 수 있습니다. 보통은 각 차원을 따라 일반화하는 것을 선호하지만, 조합에 따른 특정한 값을 학습하기 위해서는 별도의 방법이 필요합니다. (Sutton, 1996) Tiling의 선택은 일반화를 결정하며, 이 선택이 효과적으로 자동화될 수 있을 때까지 유연하고 사람들이 이해할 수 있는 방식으로 선택할 수 있는 것이 중요합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tile Coding에서 메모리 요구 사항을 줄이는 유용한 트릭 중 하나는 Hashing입니다. 이것은 큰 Tile을 훨씬 더 작은 Tile의 집합으로 규칙있게 축소하는 것입니다. Hashing은 State Space 전체에 무작위로 퍼진 비연속적이고 분리된 영역으로 구성된 Tile을 생성하지만, 완전한 Partition을 형성합니다. 예를 들어, 위의 그림과 같이 하나의 Tile은 4개의 하위 Tile로 구성될 수 있습니다. Hashing은 성능 손실이 거의 없이 메모리 요구 사항을 크게 낮출 수 있는 경우가 많습니다. 이것은 State Space의 작은 부분만 높은 Resolution이 필요하기 때문입니다. 이는 어느정도 &lt;strong&gt;Curse of Dimensionality&lt;/strong&gt;를 극복할 수 있다는 점에서 큰 의미가 있습니다. 보통 Tile Coding을 구현한 오픈 소스 프로그램들은 효율적인 Hashing을 적용하는 경우가 많습니다.&lt;/p&gt;

&lt;h3 id=&quot;radial-basis-functions&quot;&gt;Radial Basis Functions&lt;/h3&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Radial Basis Function (RBF)&lt;/span&gt;은 연속된 값들의 Feature에 대한 Coarse Coding의 일반화입니다. 각각의 Featrue가 0 또는 1이었던 이전과 다르게 RBF는 폐구간 $\left[ 0, 1 \right]$에 있는 어떤 값이라도 될 수 있으며, Feature가 존재하는 것에 대한 Degree를 반영합니다. 일반적인 RBF Feature $x_i$는 State $s$가 Feature의 Prototypical(또는 간단하게 중앙 State) $c_i$로부터 얼마나 떨어져 있는지 Feature의 상대적인 너비 $\sigma_i$에 의존하는 Gaussian $x_i(s)$입니다. 이는 수식으로 표현하면 다음과 같은 &lt;strong&gt;Gaussian(=Normal) Distribution&lt;/strong&gt;과 비슷합니다.&lt;/p&gt;

\[x_i (s) \doteq \exp \left( - \frac{|| s - c_i ||^2}{2 \sigma_i^2} \right)\]

&lt;p&gt;물론 Norm이나 거리를 측정하는 Metric은 주어진 State나 Action에 따라 적절하게 선택할 수 있습니다. 아래 그림은 1차원 공간에서 Euclidean Distance Metric의 예를 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;0과 1로만 값을 설정하던 Binary Featrue가 가지고 있지 않은 RBF의 주요 장점은 미분 가능한 근사 함수를 만든다는 것입니다. 사실 이것이 대부분 큰 의미는 없습니다. 다만 이것이 꽤 매력적인지 생각보다 많은 연구가 이루어졌으나 (An, 1991; Miller et al., 1991; An et al., 1991; Lane, Handelman and Gelfand, 1992) 대부분 Tile Coding에 비해 상당히 많은 추가적인 계산이 필요한데다 State 차원이 2개 이상인 경우 성능이 저하되는 경우가 많습니다. 특히 Tile의 가장자리는 높은 차원일수록 더 중요한데, RBF는 Tile의 가장자리가 잘 제어되지 않는 것이 증명되었습니다.&lt;/p&gt;

&lt;p&gt;RBF Network는 Feature에 대해 RBF를 사용하는 Linear Function Approximation입니다. 학습은 다른 Linear Function Approximation과 마찬가지로 식 (9.7)과 (9.8)로 정의됩니다. 또한 RBF Network에 대한 일부 학습 방법은 Feature의 중심과 너비를 변경하여 non-Linear Function Approximation으로도 사용할 수 있습니다. non-Linear Method는 Target Function을 훨신 더 정확하게 맞출 수 있다는 장점을 가지고 있습니다. RBF Network, 특히 non-Linear RBF Network의 단점은 더 큰 계산 복잡도 뿐만 아니라 학습하기 전의 Manual Tuning이 더 강력하고 효율적이라는 것입니다.&lt;/p&gt;

&lt;p&gt;RBF에 대한 더 자세한 내용은 다음 포스트를 참고해주시기 바랍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/radial-basis-functions/&quot;&gt;[기계학습] 16. Radial Basis Function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;selecting-step-size-parameters-manually&quot;&gt;Selecting Step-Size Parameters Manually&lt;/h2&gt;

&lt;p&gt;대부분의 SGD 방법에서는 적절한 Step-size Parameter $\alpha$를 선택해야 합니다. $\alpha$을 선택하는 규칙이 명확하게 주어져있다면 좋겠지만, 대부분의 경우 엔지니어가 직접 Manual하게 선택합니다. 알고리즘을 잘 이해하고, 설계한 의도대로 잘 동작하게 만드려면 Step-size Parameter의 역할에 대한 직관적인 감각을 키워야 합니다. 이번 Section에서는 이러한 내용을 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;이전에 배운 이론적인 고려 사항들은 불행히도 거의 도움이 되지 않습니다. 2장에서 다룬 식 (2.7)과 같은 &lt;strong&gt;Theory of Stochastic
Approximation&lt;/strong&gt;은 Step-size의 Sequence가 수렴을 보장하기 위해 어떤 식을 만족해야 하는지에 대한 조건을 제공하지만, 이것은 학습을 너무 느리게 만드는 경향이 있습니다. Tabular 방식의 Monte Carlo Method에서 표본 평균을 생성하는 고전적인 방법인 $\alpha = 1/t$는 Temporal Difference Learning나 Nonstationary 문제, 또는 Function Approximation에서 사용하기에 적합하지 않습니다. Linear Method의 경우 최적의 행렬 Step-size를 설정하는 Recursive Least-square 방법이 있으나, 이 방법은 Section 9.8에서 다룰 LSTD와 같은 Temporal Difference Learning에 사용하기에 적합하지 않습니다. 왜냐하면 시간 복잡도 $O(d^2)$를 가지는 Step-size Parameter나 지금까지 사용했던 Step-size Parameter들에 비해 $d$배 더 많은 Parameter가 필요하기 때문입니다. 따라서 이것들은 여기서 더 이상 사용하지 않겠습니다.&lt;/p&gt;

&lt;p&gt;Step-size Parameter를 직접 설정하는 방법에 대해 직관적인 느낌을 얻기 위해 잠시 Tabular 형식의 강화학습 방법을 다시 생각해보겠습니다. Tabular 강화학습에서 $\alpha = 1$로 설정한다면 하나의 학습 이후 Sample Error를 완전히 제거한다는 것을 알 수 있습니다. 하지만 이번 장 초반에 언급한 것과 같이 이것보다는 학습 속도가 느려야합니다. Tabular 강화학습에서 $\alpha = \frac{1}{10}$으로 설정할 경우 평균적으로 Target에 수렴하는데 대략 10개의 Experience가 필요하며, 같은 이유로 100개의 Experience에서 학습하기 위해서는 $\alpha = \frac{1}{100}$로 설정하면 된다는 것을 이해할 수 있습니다. 즉, 일반적으로 $\alpha = \frac{1}{\tau}$라면 Tabular 강화학습은 해당 State를 약 $\tau$번 Experience한 후에 수렴합니다.&lt;/p&gt;

&lt;p&gt;일반적인 Function Approximation에서는 각각의 State가 다른 모든 State와 유사할수도, 그렇지 않을 수도 있기 때문에 State를 얼마나 Experience해야 하는가에 대한 명확한 기준이 없습니다. 다만 Linear Function Approximation에서는 대략적인 규칙이 있습니다. 동일한 Feature Vector에 대해 $\tau$번 Experience으로 학습한다고 가정했을 때, Linear SGD 방법에서 많이 쓰이는 Step-size Parameter 설정법은 다음과 같습니다. (Rule of Thumb)&lt;/p&gt;

\[\alpha \doteq \left( \tau \mathbb{E} \left[ \mathbf{x}^{\sf T} \mathbf{x} \right] \right)^{-1} \tag{9.19}\]

&lt;p&gt;$\mathbf{x}$는 SGD에서의 Input Vector와 동일한 분포로 선택된 임의의 Feature Vector입니다. 이 방법은 Feature Vector의 길이가 크게 변하지 않는 경우에 가장 잘 작동합니다. 이상적으로 $\mathbf{x}^{\sf T} \mathbf{x}$는 상수입니다.&lt;/p&gt;

&lt;h2 id=&quot;nonlinear-function-approximation--artificial-neural-networks&quot;&gt;Nonlinear Function Approximation : Artificial Neural Networks&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Artificial Neural Network (ANN)&lt;/span&gt;은 non-Linear Function를 근사할 때 많이 사용됩니다. ANN에 대한 자세한 이론은 아래 포스트를 참고해주시기 바랍니다. 여기서는 ANN과 강화학습 사이의 관계에 대해서만 다루도록 하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/neural-networks/&quot;&gt;[기계학습] 10. Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음 그림은 기본적인 Feedforward ANN의 구조를 보여줍니다. 루프가 존재하지 않고, 출력이 입력에 영향을 주지도 않습니다. 그림에서는 4개의 입력이 있는 Input Layer, 2개의 출력이 있는 Output Layer, 그리고 2개의 Hidden Layer로 구성되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림과 같은 ANN은 원 모양의 Unit에 들어오는 입력을 &lt;strong&gt;Activation Function&lt;/strong&gt;에 넣고, 그 결과를 출력하는 구조로 되어있습니다. Activation Function은 다양한 종류가 사용되는데, $f(x) = 1 / (1 + e^{-x})$와 같은 Logistic Function을 사용하는 Sigmoid Function을 사용할 수도 있고, $f(x) = \max (0, x)$와 같은 Rectifier Function이 사용될 수도 있습니다. 이 때 입력으로 들어오는 값은 그대로 들어오는 것이 아니라 해당 입력이 얼마나 중요한지에 따라 Weight가 붙습니다.&lt;/p&gt;

&lt;p&gt;이러한 ANN의 학습 목적은 각각의 Weight에 대한 최적의 값을 결정하는 것입니다. Weight를 학습할 때 가장 많이 사용되는 알고리즘은 &lt;strong&gt;Backpropagation Algorithm&lt;/strong&gt;입니다. 순방향(입력 -&amp;gt; 출력) 경로로 현재의 Activation Function 및 Weight를 이용하여 출력을 계산하고, 해당 입력값에 대한 정답 출력값과 비교하여 그 차이를 역방향(출력 -&amp;gt; 입력)으로 반영하는 것입니다. 이 과정에서 편미분이 사용되며, Weight의 값이 Update 됩니다.&lt;/p&gt;

&lt;p&gt;Backpropagation Algorithm은 Hidden Layer가 많지 않은 ANN에서는 좋은 성능을 보이지만, Hidden Layer가 많은 경우에는 잘 작동하지 않습니다. 그 원인에는 여러가지가 있지만, 대표적으로 학습 데이터가 과도하게 반영되는 &lt;span style=&quot;color:red&quot;&gt;Overfitting&lt;/span&gt; 문제가 있으며, 이를 해결하는 방법으로 &lt;span style=&quot;color:red&quot;&gt;Regularization&lt;/span&gt;, &lt;span style=&quot;color:red&quot;&gt;Validation&lt;/span&gt; 등이 있습니다. 이에 대한 자세한 내용은 아래 포스트를 참고해주시기 바랍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/overfitting/&quot;&gt;[기계학습] 11. Overfitting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/regularization/&quot;&gt;[기계학습] 12. Regularization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/validation/&quot;&gt;[기계학습] 13. Validation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이후에도 교재에서는 ANN에서 Hidden Layer가 많은 경우 생길 수 있는 문제와 그 해결 방법을 소개합니다. 다만 여기에서는 그 내용을 굳이 전부 다룰 필요는 없다고 생각하기 때문에 생략하겠습니다. 일단은 강화학습에서 Function Approximation를 할 때, ANN을 통해 non-Linear Function Model로 근사가 가능하다 정도만 이해하고 넘어가시면 될 것 같습니다. ANN과 Deep Learning에 대해서는 추후 기회가 되면 포스팅하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;least-squares-td&quot;&gt;Least-Squares TD&lt;/h2&gt;

&lt;p&gt;지금까지의 Function Approximation 방법은 매개변수의 수에 비례한 시간 단계가 필요했습니다. 이번 Section에서는 다시 Linear Approximation 방법으로 돌아와서, 계산량이 조금 늘어나지만 더 나은 결과를 얻을 수 있는 Linear Approximation 방법을 소개하겠습니다.&lt;/p&gt;

&lt;p&gt;Section 9.4에서 언급했던 것처럼, TD(0)는 식 (9.12)와 같이 Linear Function Approximation은 다음과 같은 TD Fixed Point에 수렴합니다.&lt;/p&gt;

\[\mathbf{w}_{\text{TD}} \doteq \mathbf{A}^{-1} \mathbf{b}\]

&lt;p&gt;이 때 $\mathbf{A}$와 $\mathbf{b}$는 다음과 같습니다.&lt;/p&gt;

\[\mathbf{A} \doteq \mathbb{E} \left[ \mathbf{x}_t \left( \mathbf{x}_t - \gamma \mathbf{x}_{t+1} \right)^{\sf T} \right] \quad \text{and} \quad \mathbf{b} \doteq \mathbb{E} \left[ R_{t+1} \mathbf{x}_t \right]\]

&lt;p&gt;Section 9.4에서 이 식을 소개했을 때는 이것을 Iterative 방법으로 계산하였습니다. 여기서 소개할 &lt;span style=&quot;color:red&quot;&gt;Least-Square TD (LSTD)&lt;/span&gt;는 이와 다르게 $\mathbf{A}$와 $\mathbf{b}$의 Estimated Value를 먼저 계산한 다음, TD Fixed Point 식을 이용하여 $\mathbf{w}_{\text{TD}}$를 직접 계산합니다. 먼저 $\mathbf{A}$와 $\mathbf{b}$의 Estimated Value을 계산하는 $\widehat{\mathbf{A}}_t$와 $\widehat{\mathbf{b}}_t$의 식부터 소개하겠습니다.&lt;/p&gt;

\[\widehat{\mathbf{A}}_t \doteq \sum_{k=0}^{t-1} \mathbf{x}_k \left( \mathbf{x}_k - \gamma \mathbf{x}_{k+1} \right)^{\sf T} + \varepsilon \mathbf{I} \quad \text{and} \quad \widehat{\mathbf{b}}_t \doteq \sum_{k=0}^{t-1} R_{k+1} \mathbf{x}_k \tag{9.20}\]

&lt;p&gt;식 (9.20)에서 $\mathbf{I}$는 단위 행렬이고, 적당히 작은 $\varepsilon &amp;gt; 0$에 대해 $\widehat{\mathbf{A}}_t$는 항상 역행렬이 존재함을 보장합니다. $\widehat{\mathbf{A}}_t$와 $\widehat{\mathbf{b}}_t$의 식을 보면 이 식들을 $t$로 나누는 식이 빠져있음을 알 수 있습니다. 따라서 식 (9.20)은 사실 $\widehat{\mathbf{A}}_t$와 $\widehat{\mathbf{b}}_t$의 Estimated Value가 아니라 $\widehat{\mathbf{A}}_t \times t$와 $\widehat{\mathbf{b}}_t \times t$의 Estimated Value입니다. 하지만 아래와 같이 LSTD를 사용해서 $\mathbf{w}_t$를 계산할 때, $t$를 곱한 값은 어차피 상쇄되어 없어지기 때문에 굳이 미리 계산하지 않는 것입니다.&lt;/p&gt;

\[\mathbf{w}_t \doteq \widehat{\mathbf{A}}_t^{-1} \widehat{\mathbf{b}}_t \tag{9.21}\]

&lt;p&gt;이 방법은 Linear TD(0)를 데이터 관점에서 가장 효율적으로 계산할 수 있는 방법이지만, 그만큼 계산 비용이 더 많이 든다는 단점도 있습니다. 이전에 Semi-gradient TD(0)는 시간 복잡도와 공간 복잡도 모두 $O(d)$만 필요했습니다. LSTD의 계산이 얼마나 복잡한지 간단하게 따져보면, $\widehat{\mathbf{A}}_t$의 Update는 &lt;strong&gt;Outer Product&lt;/strong&gt;를 포함하기 때문에 시간 복잡도는 $O(d^2)$, 행렬을 저장하는데 필요한 공간 복잡도 또한 $O(d^2)$가 됩니다. 게다가 식 (9.21)을 계산하기 위해서는 $\widehat{\mathbf{A}}_t$의 역행렬을 먼저 계산해야 하는데, 이것을 계산하는데 드는 시간 복잡도가 $O(d^3)$입니다. 다행히도, $\widehat{\mathbf{A}}_t$의 역행렬을 $O(d^2)$의 시간 복잡도로 계산할 수 있는 방법이 있습니다.&lt;/p&gt;

\[\begin{align}
\widehat{\mathbf{A}}_t^{-1} &amp;amp;= \left( \widehat{\mathbf{A}}_{t-1} + \mathbf{x}_{t-1} (\mathbf{x}_{t-1} - \gamma \mathbf{x}_t)^{\sf T} \right)^{-1} \tag{from (9.20)} \\ \\
&amp;amp;= \widehat{\mathbf{A}}_{t-1}^{-1} - \frac{\widehat{\mathbf{A}}_{t-1}^{-1} \mathbf{x}_{t-1} (\mathbf{x}_{t-1} - \gamma \mathbf{x}_t)^{\sf T} \widehat{\mathbf{A}}_{t-1}^{-1}}{1 + (\mathbf{x}_{t-1} - \gamma \mathbf{x}_t)^{\sf T} \widehat{\mathbf{A}}_{t-1}^{-1} \mathbf{x}_{t-1}} \tag{9.22}
\end{align}\]

&lt;p&gt;위 식에서 초기값은 $\widehat{\mathbf{A}}_0 \doteq \varepsilon \mathbf{I}$로 정의됩니다. 식 (9.22)은 &lt;strong&gt;Sherman-Morrison Formula&lt;/strong&gt;를 사용한 것이며, 복잡해보이지만 시간 복잡도는 $O(d^2)$에 불과합니다. 이것을 사용해서 LSTD를 시간 복잡도 $O(d^2)$로 구현한 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;물론 이렇게 시간 복잡도를 $O(d^2)$로 줄인다고 해도 여전히 Semi-gradient TD보다 훨씬 느립니다. 이러한 시간 복잡도를 감수하더라도 LSTD를 사용하는 것이 그만큼의 가치가 있는지는 $d$가 얼마나 큰지, 학습 속도가 주어진 문제에서 얼마나 중요한지, 시스템의 나머지 부분에서 드는 비용이 어느정도인지에 따라 다릅니다. LSTD는 Step-size Parameter가 필요하지 않다는 장점이 분명 존재하지만, 대신 $\varepsilon$이 필요하며, 이것을 어떤 값으로 정해야하는지에 대한 문제가 존재합니다. $\varepsilon$가 너무 크면 학습 속도가 느려지고, 그렇다고 너무 작으면 역행렬을 계산할 때 문제가 생깁니다. 또한 Step-size Parameter가 없다는 것은 이전에 학습한 내용을 절때 잊지 않는다는 의미가 되는데, 일반적으로 이것이 좋을 수 있지만 강화학습이나 GPI에서 Policy $\pi$가 변하면 문제가 됩니다. 따라서 Policy가 계속 변할 수밖에 없는 Control 문제에서 LSTD를 사용하기 위해서는 이전에 학습한 내용을 잊을 수 있게 다른 메커니즘과 결합되어야만 합니다.&lt;/p&gt;

&lt;h2 id=&quot;memory-based-function-approximation&quot;&gt;Memory-based Function Approximation&lt;/h2&gt;

&lt;p&gt;지금까지는 Value Function을 근사화하기 위해 매개변수를 이용하여 접근하는 방법에 대해 다루었습니다. 이러한 접근 방법에서는 매개변수를 조정함으로써 Value Function을 근사하는 학습 알고리즘을 사용하였습니다. 각각의 Update $s \mapsto g$는 근사 오차를 줄이기 위해 매개변수를 조절하는 학습 알고리즘의 예시입니다. Update가 끝나면 학습에 사용되는 Training Data는 폐기될 수 있습니다. 대략적인 State의 Value(Query State)가 필요할 때는 학습 알고리즘에 의해 생성된 최신 매개변수를 사용한 Value Function을 토대로 평가합니다.&lt;/p&gt;

&lt;p&gt;이번 Section에서 다룰 &lt;span style=&quot;color:red&quot;&gt;Memory-based Function Approximation&lt;/span&gt;은 이와 다르게 매개변수를 Update하지 않고, Training Data가 도착할 때마다 메모리에 저장합니다. 그 후 Query State가 필요할 때마다 메모리에서 Training Data를 검색하여 Query State에 대한 Estimated Value를 계산하는 데 사용합니다. 이 방법은 Query State에 대해 시스템이 출력을 제공할 때까지 Training Data의 처리가 연기되기 때문에 &lt;span style=&quot;color:red&quot;&gt;Lazy Learning&lt;/span&gt;이라고도 합니다.&lt;/p&gt;

&lt;p&gt;Memory-based Function Approximation은 매개변수를 사용하지 않는 방법인 Nonparametric Method의 대표적인 예시입니다. 매개변수를 사용하는 방법과 달리 근사 함수가 Linear Function이나 Polynomial과 같은 고정된 종류로 제한되지 않고, Training Data 자체와 Query State에 대한 Estimated Value를 출력하기 위한 수단에 의해 결정됩니다. 더 많은 Training Data가 메모리에 축적될수록 Memory-based Function Approximation은 Target Fucntion을 더 정확하게 근사할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 때, 저장된 Training Data를 선택하는 방법과 Query에 응답하는 데 사용되는 방법에 따라 Memory-based Function Approximation의 종류를 나눌 수 있습니다. 다만 여기에서는 현재 Query State 근처에서만 Local Value Function를 근사하는 Local-learning 방법에 중점을 두겠습니다. 이 방법은 State가 메모리에서 Query State와 가장 가깝다고 판단되는 Training Data를 검색합니다. 이 때 얼마나 가깝다고 판단하는지는 State 간의 거리에 따르는데, 거리에 대한 기준은 문제마다 다양한 방식으로 정의될 수 있습니다. 어쨌든 적절한 Query State의 값을 찾아 제공한 후에는 Local Estimated Value는 삭제됩니다.&lt;/p&gt;

&lt;p&gt;Memory-based Function Approximation이 매개변수를 사용하는 근사 방법과 달리 함수의 형태가 정해지지 않기 때문에 많은 데이터가 축적될수록 정확도가 향상됩니다. 이러한 특징으로 인해 Memory-based Function Approximation은 강화학습에 적합하다는 좋은 장점이 있습니다. 예를 들어, Trajectory Sampling은 State Space의 많은 영역에 도달할 수 없거나, 거의 도달할 일이 없기 때문에 전역 근사가 필요하지 않은데, Memory-based Function Approximation을 사용한다면 State의 Local Neighbor에 초점을 맞춤으로써 불필요한 계산 낭비를 방지할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게 전역 근사를 피함으로써 얻어지는 또 다른 이점은 &lt;strong&gt;Curse of Dimensionality&lt;/strong&gt;에서 벗어난다는 것입니다. 예를 들어, $k$차원이 있는 State Space의 경우 전역 근사를 저장하는 Tabular 방법에는 $k$에 대해 지수적으로 비례한 메모리가 필요합니다. 하지만 Memory-based Function Approximation을 사용한다면 각각의 데이터는 $k$에 비례한 메모리만 필요하고, $n$개의 데이터를 저장한다면 $n$과 $k$에 대해 Linear에 비례한 메모리가 필요할 뿐입니다. 즉, 지수 함수가 들어가지 않습니다. 다만 이것은 저장하는데 필요한 데이터 양일 뿐, 누적된 데이터의 수가 많아질수록 Query에 응답하기 위해 걸리는 시간이 늘어나기 때문에 대규모 데이터베이스에서 사용하기에는 적합하지 않을 수도 있습니다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해 최근 Nearest Neighbor 검색을 빠르게 하는 방법이 연구중에 있습니다. 병렬 컴퓨터나 특수 목적 하드웨어를 사용하는 것도 하나의 접근 방식입니다. 소프트웨어적으로는 Training Data를 저장할 때 특별한 다차원 자료구조를 사용하는 것입니다. 대표적으로 $k$차원 공간을 Binary Tree의 노드가 배열된 영역을 재귀적으로 분할하는 $k$-$d$ Tree가 있습니다. $k$-$d$ Tree의 자세한 내용은 &lt;a href=&quot;https://en.wikipedia.org/wiki/K-d_tree&quot;&gt;Wikipedia&lt;/a&gt;에 자세하게 나와있습니다.&lt;/p&gt;

&lt;h2 id=&quot;kernel-based-function-approximation&quot;&gt;Kernel-based Function Approximation&lt;/h2&gt;

&lt;p&gt;이전 Section에서 설명한 Memory-based Function Approximation에서 Weighted Average과 Locally Weighted Regression과 같은 방법은 State $s’$와 Query State $s$의 거리에 따라 Weight를 부여하여 $s’ \mapsto g$과 같이 적절한 데이터베이스에 매칭됩니다. 이렇게 Weight를 할당하는 함수를 &lt;span style=&quot;color:red&quot;&gt;Kernel Function&lt;/span&gt;, 또는 그냥 &lt;span style=&quot;color:red&quot;&gt;Kernel&lt;/span&gt;이라고 부릅니다. Kernel Function $k : \mathbb{R} \to \mathbb{R}$는 State 사이의 거리에 Weight를 할당합니다. 정확하게는 Weight가 거리에 의존할 필요는 없고, State 간의 유사성을 계산하는데 필요한 측정 방식에 따라 의존합니다. 이 경우 $k(s, s’)$ ($k : \mathcal{S} \times \mathcal{S} \to \mathbb{R}$)은 $s$에 대한 Query 응답에 미치는 영향에 대해 $s’$에 대한 Weight가 됩니다.&lt;/p&gt;

&lt;p&gt;Kernel Regression은 메모리에 저장된 모든 학습 데이터의 Kernel Weighted Average를 계산하여 Query State에 결과를 할당하는 Memory-based Function Approximation입니다. $\mathcal{D}$가 저장된 학습 데이터의 집합이고, $g(s’)$가 저장된 학습 데이터 중 State $s’$의 Target Function이라고 하면 Kernel Regression에서 다음과 같이 State $s$의 Target Function을 근사할 수 있습니다.&lt;/p&gt;

\[\hat{v} (s, \mathcal{D}) = \sum_{s&apos; \in \mathcal{D}} k(s, s&apos;) g(s&apos;) \tag{9.23}\]

&lt;p&gt;가장 널리 사용되는 Kernel은 Section 9.5.5에 설명한 &lt;span style=&quot;color:red&quot;&gt;Gaussian Radial Basis Function (RBF)&lt;/span&gt;입니다. RBF는 정규분포처럼 많은 학습 데이터가 존재할 것 같은 지점이 중심이 되고, 학습 중 중심과 너비가 조절됨으로써 균형을 찾습니다. RBF 자체는 매개변수가 RBF의 Weight인 Linear Parametric 방법이며, 일반적으로 Stochastic Gradient Descent나 Semi-gradient Descent로 학습됩니다. RBF Kernel을 사용한 Kernel Regression은 이것과 다르게 Memory-based이고 학습할 매개변수가 없습니다. Query에 대한 응답은 식 (9.23)에 의해 계산됩니다.&lt;/p&gt;

&lt;p&gt;Kernel Regression을 실제로 구현하기 위해서는 여기서 다루지 않는 많은 문제가 해결되어야 합니다. 다만 Feature Vector $\mathbf{x}(s) = \left( x_1 (s), x_2 (s), \ldots, x_d (s) \right)^{\sf T}$로 표현되는 State와 Section 9.4에서 설명한 Linear Parametric Regression 방법은 다음과 같이 Feature Vector의 내적 계산으로 간단하게 Kernel Regression을 구현할 수 있습니다.&lt;/p&gt;

\[k(s, s&apos;) = \mathbf{x} (s)^{\sf T} \mathbf{x} (s&apos;) \tag{9.24}\]

&lt;p&gt;식 (9.24)와 같은 Kernel Regression은 동일한 학습 데이터로 학습한 경우 Linear Parametric 방법과 동일한 근사값을 생성합니다. 모든 Kernel Function이 식 (9.24)와 같이 Feature Vector의 내적으로 표현될 수 있는 것은 아니지만, 이렇게 표현할 수 있는 Kernel Function은 같은 결과를 생성하는 Parametric 방법에 비해 간단하게 계산할 수 있는 장점이 있습니다. 이것을 보통 &lt;span style=&quot;color:red&quot;&gt;Kernel Trick&lt;/span&gt; 이라고 부릅니다. Kernel에 대한 더 자세한 내용은 다음 포스트를 참고해주시기 바랍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/studies/kernel-methods/&quot;&gt;[기계학습] 15. Kernel Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;looking-deeper-at-on-policy-learning--interest-and-emphasis&quot;&gt;Looking Deeper at On-policy Learning : Interest and Emphasis&lt;/h2&gt;

&lt;p&gt;이번 장에서 지금까지 고려한 알고리즘들은 발생할 수 있는 모든 State를 동등하게 취급했습니다. 그러나 어떤 경우에는 특정한 몇몇 State에 집중해야할 수 있습니다. 예를 들어 Discounted Episodic Task에서 후기 State는 Discount로 인해 초기 State보다 Value가 낮으므로 초기 State를 정확하게 추정하는데 더 집중하는 것이 좋고, Action-Value Function을 학습할 때는 Value가 매우 낮은 나쁜 Action보다 Greedy Action을 더 정확하게 추정하는데 집중하는 것이 좋습니다. Function Approximation을 할 때 사용할 수 있는 자원은 한정되어있기 때문에 원하는 목표를 찾아 집중한다면 성능이 향상될 수 있습니다.&lt;/p&gt;

&lt;p&gt;지금까지 발생할 수 있는 모든 State를 동등하게 취급한 이유 중 하나는 On-policy Distribution에 따라 Update했기 때문입니다. 이제는 몇 가지 새로운 개념을 소개함으로써 일반화하고자 합니다.&lt;/p&gt;

&lt;p&gt;먼저 음이 아닌 Scalar Measure인 Random Variable $I_t$를 소개합니다. $I_t$는 &lt;span style=&quot;color:red&quot;&gt;Interest&lt;/span&gt;로 부르며 시간 $t$에서 State(또는 State-Action 쌍)를 정확하게 평가하는 데 얼마나 관심이 있는지에 대한 정도를 나타냅니다. 전혀 신경 쓰지 않는 State라면 Interest는 0이 되고, 가장 크게 신경쓰는 State라면 Interest는 1이 됩니다. (필요하다면 1이 아니라 더 큰 값을 최대값으로 사용해도 됩니다.) Interest $I_t$는 문제에 따라 다양하게 정의할 수 있습니다.&lt;/p&gt;

&lt;p&gt;둘째로, 또 다른 음이 아닌 Scalar Measure인 Random Variable인 $M_t$를 소개합니다. $M_t$는 &lt;span style=&quot;color:red&quot;&gt;Emphasis&lt;/span&gt;로 부르며 시간 $t$에서 수행한 학습을 얼마나 강조할지 나타냅니다. 예를 들어, 식 (9.15)에 Emphasis를 추가한다면 다음과 같이 수정할 수 있습니다.&lt;/p&gt;

\[\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha M_t \left[ G_{t:t+n} - \hat{v}(S_t, \mathbf{w}_{t+n-1}) \right] \nabla \hat{v}(S_t, \mathbf{w}_{t+n-1}), \quad 0 \le t &amp;lt; T \tag{9.25}\]

&lt;p&gt;또한 Emphasis와 Interest를 다음과 같이 재귀적으로 관계를 표현할 수 있습니다.&lt;/p&gt;

\[M_t = I_t + \gamma^n M_{t-n} \tag{9.26}\]

&lt;p&gt;만약 식 (9.26)을 계산 도중 $t$가 0보다 작아지는 경우 $M_t \doteq 0$으로 정의됩니다. 또한 이 방정식은 $G_{t:t+n} = G_t$, 즉 $n = T - t$, $M_t = I_t$와 같이 Episode의 마지막까지 포함되는 Monte Carlo로 간주됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 9.4) Interest and Emphasis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이번 예제는 Interest와 Emphasis가 얼마나 정확한 Value를 추정할 수 있는지 보여주는 예시입니다. 먼저, 다음과 같이 4개의 State를 가지고 있는 Markov Reward Process가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/9. On-policy Prediction with Approximation/RL 09-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예제에서 Episode는 맨 왼쪽 State에서 시작한 다음 최종 State에 도달할 때까지 각 단계마다 +1의 보상을 받은 후, 오른쪽 State로 넘어갑니다. 따라서 첫 번째 State의 Real Value는 4이고, 두 번째 State의 Real Value는 3이 되는 식입니다. (각 State의 Real Value는 State 바로 아래에 표시되어 있습니다.)&lt;/p&gt;

&lt;p&gt;하지만 추정된 Value는 매개변수에 의해 제약을 받기 때문에 대략적으로만 구할 수 있습니다. 여기서는 2개의 구성 요소로 이루어진 매개변수 Vector $\mathbf{w} = (w_1, w_2)^{\sf T}$가 주어졌으며, 각 State별 어떤 매개변수를 사용하는지는 State 원 안에 표시되어 있습니다. 처음 두 State는 매개변수가 $w_1$만 주어지므로 Real Value가 다르더라도 동일해야 합니다. 마찬가지로 세 번째와 네 번째의 추정값은 $w_2$만으로 주어졌기 때문에 동일해야 합니다. 만약 가장 왼쪽의 State만 정확하게 평가하는데 관심이 있다고 가정합니다. 그렇다면 State 위에 표시한 것처럼 Interest는 첫 번째 State에만 1이 할당되고 나머지 State에는 0이 할당됩니다.&lt;/p&gt;

&lt;p&gt;먼저 이 문제를 Gradient Monte Carlo 알고리즘을 사용해서 해결해보겠습니다. 만약 Interest와 Emphasis를 고려하지 않는다면 매개변수 Vector는 $\mathbf{w}_{\infty} = (3.5, 1.5)$로 수렴합니다. 그러므로 첫 번째 State의 값은 3.5가 됩니다. 반면에 Interest와 Emphasis를 사용한 방법은 첫 번째 State의 값을 정확하게 학습하여 $w_1$은 4로 수렴하고, $w_2$는 Update 되지 않습니다.&lt;/p&gt;

&lt;p&gt;다음으로 2-step Semi-gradient TD 방법을 사용해서 해결해보겠습니다. 마찬가지로 Interest와 Emphasis를 고려하지 않는다면 매개변수 Vector는 $\mathbf{w}_{\infty} = (3.5, 1.5)$로 수렴합니다. 만약 Interest와 Emphasis를 추가한다면 매개변수 Vector는 $\mathbf{w}_{\infty} = (4, 2)$로 수렴합니다. 이것은 첫 번째 State와 세 번째 State에 대해 정확한 Value를 계산하지만, 두 번째와 네 번째 State의 Update는 수행하지 않습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;강화학습을 인공지능이나 대규모 응용 프로그램에 적용하려면 일반화가 가능해야 합니다. 일반화를 하기 위해서는 각각의 Update를 Training Data로 취급하여 Supervised Learning을 이용한 &lt;strong&gt;Function Approximation&lt;/strong&gt;을 사용할 수 있습니다. 다행히, Supervised Learning에서 이미 연구된 많은 Function Approximation 방법이 있기 때문에, 기존 방법을 사용하여 이 문제를 해결할 수 있습니다.&lt;/p&gt;

&lt;p&gt;우선 가장 적합한 Supervised Learning 방법으로 생각되는 것은 Policy가 Weight Vector $\mathbf{w}$를 매개변수로 하는 &lt;strong&gt;Parameterized Function Approximation&lt;/strong&gt;을 사용하는 방법입니다. Weight Vector에는 많은 구성 요소가 있지만, State Space은 이보다 더 클 수밖에 없기 때문에 Approximate Solution을 찾아야 합니다. 이 때 On-policy Distribution $\mu$에서 Weight Vector $\mathbf{w}$를 사용하여 추정한 Value Function $v_{\pi_{\mathbf{w}}} (s)$가 Real Value Function와 얼마나 다른지를 측정하기 위해 &lt;strong&gt;Mean Square Value Error $\overline{\text{VE}}$&lt;/strong&gt;를 정의했습니다. $\overline{\text{VE}}$는 On-policy에서 서로 다른 Estimated Value Function의 우열을 매길 수 있는 명확한 판단 기준을 제공합니다.&lt;/p&gt;

&lt;p&gt;좋은 Weight Vector를 찾기 위해 가장 많이 사용되는 방법은 &lt;strong&gt;Stochastic Gradient Descent (SGD)&lt;/strong&gt;입니다. 특히 이번 장에서는 고정된 Policy가 있는 On-policy 경우에만 중점을 두었습니다. 이 경우에 사용하는 학습 알고리즘은 $n = \infty$일 때 사용되는 &lt;strong&gt;Gradient Monte Carlo&lt;/strong&gt; 알고리즘, 그리고 $n = 1$일 때 사용되는 &lt;strong&gt;Semi-gradient TD(0)&lt;/strong&gt; 알고리즘입니다. Semi-gradient TD는 엄밀하게 말하면 Gradient 방법이 아니지만, 이러한 Bootstrapping 방법에서 Weight Vector가 Update 대상에 나타나지만 Gradient 계산에서는 고려되지 않으므로 Semi-gradient 방법이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;그럼에도 불구하고 Feature에 Weight를 곱해 각각 더한 Linear Function Approximation 같은 특수한 경우라면 Semi-gradient 방법은 좋은 Estimated Value를 얻을 수 있습니다. Linear Function Approximation은 간단하지만 적절한 Feature가 선택되었을 때 잘 동작한다는 장점이 있습니다. 이 때, Feature를 선택하는 것은 강화학습 시스템에 사전 지식을 추가하는 가장 중요한 방법 중 하나입니다. &lt;strong&gt;Polynomial&lt;/strong&gt;을 선택할 경우, 온라인 학습 환경에서 일반화하기 힘든 단점이 있습니다. 온라인 학습을 원할 경우 &lt;strong&gt;Fourier-based&lt;/strong&gt;나 &lt;strong&gt;Coarse Coding&lt;/strong&gt;을 선택하는 것이 좋습니다. &lt;strong&gt;Tile Coding&lt;/strong&gt;은 Coarse Coding의 한 종류로, 계산이 효율적이고 유연한 것이 특징입니다. &lt;strong&gt;Radial Basis Function&lt;/strong&gt;은 부드럽게 변화하하는 1차원, 혹은 2차원 작업에 적합합니다. &lt;strong&gt;Least Square TD&lt;/strong&gt;는 가장 데이터적으로 효율적인 Linear TD 추정 방법이지만 계산량이 매우 많은 단점이 있습니다. non-Linear Method에는 SGD와 Backpropagation이 결합한 &lt;strong&gt;Artificial Neural Network&lt;/strong&gt;가 있습니다. 이 방법은 최근 &lt;strong&gt;Deep Reinforcement Learning&lt;/strong&gt;이라는 이름으로 활발히 연구되고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Linear Semi-gradient $n$-step TD&lt;/strong&gt;는 일반적인 조건에서 모든 $n$에 대해 최적의 오차 범위 내에 있는 $\overline{\text{VE}}$에 수렴하는 것이 보장됩니다. $\overline{\text{VE}}$는 $n$이 클수록 더 작아지며(=범위가 좁아지며), $n \to \infty$일 때 0에 수렴합니다. 하지만 $n$이 커질수록 학습 속도가 매우 느려지는 문제가 있습니다.&lt;/p&gt;

&lt;p&gt;다음 장에서는 근사를 사용하는 On-policy Control에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;9장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">Part II : Approximate Solution Methods</summary></entry><entry><title type="html">Planning and Learning with Tabular Methods</title><link href="http://localhost:4000/studies/planning-and-learning-with-tabular-methods/" rel="alternate" type="text/html" title="Planning and Learning with Tabular Methods" /><published>2022-04-20T00:00:00+09:00</published><updated>2022-04-20T00:00:00+09:00</updated><id>http://localhost:4000/studies/planning-and-learning-with-tabular-methods</id><content type="html" xml:base="http://localhost:4000/studies/planning-and-learning-with-tabular-methods/">&lt;p&gt;지금까지 배운 강화학습 방법에서 Dynamic Programming, Heuristic Search와 같이 Model이 필요한 방법을 &lt;span style=&quot;color:red&quot;&gt;Model-based&lt;/span&gt;라고 하고, Monte Carlo Method, Temporal Difference Learning과 같이 Model 없이 사용할 수 있는 방법을 &lt;span style=&quot;color:red&quot;&gt;Model-free&lt;/span&gt;라고 합니다. 해법을 구할 때 Model-based 방법은 &lt;strong&gt;Planning&lt;/strong&gt;을 사용하지만 Model-free 방법은 &lt;strong&gt;Learning&lt;/strong&gt;을 사용하는 차이점이 있습니다. 그러나 두 방법은 Value Function을 계산하는 과정에서 유사점이 있습니다. 두 방법 모두 미래에 발생하는 이벤트를 토대로 Value Function을 추정하기 때문입니다. 이번 장에서는 Model-based와 Model-free를 통합한 새로운 방법을 제안합니다. 지난 장에서 Monte Carlo Method과 TD(0)를 $n$-step TD로 통합하는 방법을 보여드렸는데, 이번 장의 통합 과정도 이와 유사합니다. 특히 서로 다른 두 방법이 어디까지 통합될 수 있는지에 집중할 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;models-and-planning&quot;&gt;Models and Planning&lt;/h2&gt;

&lt;p&gt;Environment Model은 Agent의 Action에 Environment가 어떻게 반응할지 예측하는 데 사용할 수 있는 모든 것을 의미합니다. State와 Action이 주어졌을 때, Model은 다음 State와 받게 되는 Reward를 예측합니다. Model이 Stochastic이라면 다음 State와 받게 되는 Reward는 여러 종류가 있어서 확률에 따라 다음 State와 받게 되는 Reward가 정해집니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Distribution Model&lt;/span&gt;이라고 부릅니다. 또 다른 Model로 확률에 따라 단 하나의 Sample만 생성하는 &lt;span style=&quot;color:red&quot;&gt;Sample Model&lt;/span&gt;도 있습니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 12개의 주사위를 던져 그 합을 Modeling한다고 가정해봅시다. Distribution Model은 발생할 수 있는 모든 합을 구한 다음, 각 합계가 발생할 확률을 계산합니다. 반면에 Sample Model은 이 Probability Distribution에 따라 생성된 Sample을 다수 생성하여 확률을 계산하는 것입니다. Distribution Model을 토대로 Sample을 생성할 수도 있기 때문에 더 우수한 방법이지만, 실제 문제에서는 Distribution Model을 생성하는 것이 쉽지 않기 때문에 얻기 쉬운 Sample Model을 더 많이 사용합니다. 예를 든 12개의 주사위 합을 Modeling하는 것만 쳐도, 모든 경우의 수와 확률을 구하는 것은 쉽지 않습니다. 오히려 12개의 주사위를 굴리는 시뮬레이션 프로그램을 작성하는게 훨씬 쉽습니다. 물론 Sample Model을 토대로 가능한 합계와 그 확률을 추산하는 것은 오류가 발생하기 쉽다는 단점도 있습니다.&lt;/p&gt;

&lt;p&gt;Model을 구했다면 그 Model을 사용하여 시뮬레이션을 할 수도 있습니다. 초기 State와 Action이 주어지면 Sample Model은 가능한 다음 State 및 Reward 등을 생성하고, Distribution Model은 발생할 확률에 따라 Weight를 부여한 모든 가능한 State와 Reward를 생성합니다. 따라서 시작점만 주어진다면 Model을 통해 Episode 전체를 생성할 수 있습니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Simulated Experience&lt;/span&gt;라고 합니다. 이렇게 Model을 기반으로 Modeling된 Environment와 상호작용하기 위한 Policy를 생성 및 개선하는 모든 프로세스를 &lt;span style=&quot;color:red&quot;&gt;Planning&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인공지능에서 Planning은 두 가지 접근 방식으로 나뉘어 있습니다. 이 책에서 다루는 접근 방식은 &lt;span style=&quot;color:red&quot;&gt;State Space Planning&lt;/span&gt;이라고 하는데, Optimal Policy을 만드는 최적의 경로를 위한 &lt;strong&gt;State Space&lt;/strong&gt;를 탐색하는 방법입니다. 이 때 Action은 State에서 다른 State로 전환하는 데 사용되며, Value Function은 State에 의해 계산됩니다. 또 다른 방법으로 &lt;span style=&quot;color:red&quot;&gt;Plan Space Planning&lt;/span&gt;이 있는데, 이것은 이름처럼 &lt;strong&gt;Plan Space&lt;/strong&gt;를 탐색하는 방법입니다. Operators를 사용하여 Plan을 다른 Plan으로 전환하고, Value Function은 Plan Space에 기반하여 정의됩니다. Plan Space Planning은 강화학습 문제의 초점인 &lt;strong&gt;Stochastic Sequential Decision Problem&lt;/strong&gt;에 적용하기 어렵기 때문에 여기서는 더이상 언급하지 않습니다. (참고 : Russell and Norvig, 2010)&lt;/p&gt;

&lt;p&gt;이번 장에서 사용하는 기본 아이디어로 &lt;u&gt;(1) 모든 State Space Planning은 Policy를 개선하기 위해 Value Function 계산을 중간 단계에 포함&lt;/u&gt;하고, &lt;u&gt;(2) Simulated Experience이나 백업 연산으로 Value Function을 계산&lt;/u&gt;합니다. 이 구조를 Diagram으로 표현하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 방법에 가장 적합한 것은 &lt;strong&gt;Dynamic Programming&lt;/strong&gt;입니다. State Space를 Sweep한 다음 각 State에 대해 가능한 Transition Distribution을 생성하고, 각 Distribution이 백업된 값을 계산한 다음 State의 추정 Value를 Update하면 됩니다. 물론 Dynamic Programming 외에 다른 State Space Planning 방법을 못쓰는 것은 아닙니다. 다른 방법 또한 이번 장에서 살펴볼 예정입니다.&lt;/p&gt;

&lt;p&gt;Planning과 Learning의 핵심은 Backup Update를 통한 Value Function의 추정입니다. 두 방법의 차이점으로 Planning은 Model에서 생성된 Simulated Experience를 사용하는 반면, Learning은 Environment에서 생성된 Real Experience를 사용합니다. 이 차이로 인해 Experience가 얼마나 유연하게 생성될 수 있는지, 성능이 어떻게 평가되는지와 같은 또 다른 차이점을 만들어내긴 하지만, Value Function을 추정한다는 공통된 부분을 통해 많은 아이디어와 알고리즘이 두 방법에 동일하게 쓰일 수 있습니다. 특히, 다수의 Learning 알고리즘은 Planning에도 유용하게 사용할 수 있습니다. 예를 들어, 다음 알고리즘은 1-step Tabular Q-learning 알고리즘을 Planning에 도입한 예시입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 장의 두 번째 주제는 작고 점진적인 단계로 Planning하는 것의 이점입니다. 이를 통해 계산 낭비가 거의 없이 언제든지 Planning을 중단하거나 리디렉션 할 수 있으며, 이는 Planning과 Learning을 효율적으로 융합하기 위한 핵심 요구 사항입니다. 주어진 문제가 너무 방대하여 정확하게 풀 수 없는 경우, 작은 단계로 나누어 Planning하는 것이 효율적인 접근 방식이 될 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;dyna-integrated-planning-acting-and-learning&quot;&gt;Dyna: Integrated Planning, Acting, and Learning&lt;/h2&gt;

&lt;p&gt;Planning이 일어나는 동안, Planning Agent가 Environment와 상호작용하는 동안 얻게 되는 새로운 정보는 Model을 변경할 수 있습니다. 만약 의사 결정과 Model 학습이 모두 상당한 계산량이 필요하다면, 사용 가능한 계산 자원을 적절히 배분할 필요가 있습니다. 이 문제를 해결하기 위해 이번 Section에서는 주요 기능을 통합하는 간단한 구조인 Dyna-Q를 제안합니다.&lt;/p&gt;

&lt;p&gt;Planning Agent가 Real Experience를 하기 위해서는 최소한 다음 두 가지 조건이 필요합니다. 첫째로 Real Environment와 더 정확하게 일치하도록 Model을 개선하는 데 사용할 수 있어야 하고(&lt;strong&gt;Model Learning&lt;/strong&gt;), 둘째로 이전 장들에서 논의한 강화학습 방법을 사용하여 Value Function과 Policy를 개선하는데 사용할 수 있어야 합니다(&lt;strong&gt;Direct Reinforcement Learning&lt;/strong&gt;). 이들간의 관계는 아래 Diagram을 참고해주시기 바랍니다. 각 화살표는 누구에게 영향을 끼치는지 나타냅니다. Experience가 어떻게 Model을 통해 Value Function과 Policy를 개선할 수 있는지 확인하시기 바랍니다. Model을 학습하고, Model을 Planning 함으로써 Value Function과 Policy를 개선하는 것을 &lt;span style=&quot;color:red&quot;&gt;Indirect Reinforcement Learning&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Direct RL과 Indirect RL 모두 각자의 장단점이 있습니다. Indirect RL은 제한된 양의 Experience을 최대한 활용하기 때문에 Environment와의 상호작용이 적더라도 더 나은 Policy를 유도할 수 있습니다. 반면 Direct RL은 간단하고, Model을 설계할 때 발생하는 Bias에 영향을 받지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dyna-Q에는 이 Diagram에 표시된 모든 프로세스가 포함되어 있습니다. 위의 그림을 통해 그 구조를 확인할 수 있습니다. 중앙의 Real Experience를 기준으로 왼쪽을 보시면, Real Experience를 토대로 Direct RL을 수행하고 오른쪽을 보시면 Real Experience를 토대로 Model을 학습한 다음, Simulated Experience를 토대로 Planning을 수행하는 것을 볼 수 있습니다. 여기에서 사용하는 Direct RL/Planning은 일반적으로 같은 강화학습 방법을 사용하기 때문에 &lt;span style=&quot;color:red&quot;&gt;Final Common Path&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;원칙적으로 Dyna에서 Planning, Acting, Model-Learning, 그리고 Direct RL은 동시에 병렬적으로 수행됩니다. 하지만 실제로 구현하는 컴퓨터는 직렬로 구현되어 있기 때문에, 각각의 시간 단계에서 발생하는 순서를 지정해야 합니다. Dyna-Q에서 Acting, Model-Learning, Direct RL에는 계산이 거의 필요하지 않고, 약간의 시간만 소요된다고 가정합니다. 남은 계산 자원과 시간은 모두 Planning에 할당합니다. 전체 Tabular Dyna-Q의 Pseudocode는 아래와 같습니다. 이 알고리즘에서 $Model(s, a)$는 주어진 State-Action 쌍 $(s, a)$에 대해 Model이 예상한 다음 State 및 Reward를 나타냅니다. Direct RL은 (d), Model-Learning은 (e), 그리고 Planning은 (f) 단계에서 구현되어 있습니다. 어렵게 생각하실 필요 없이 (e)와 (f)를 생략하면 그냥 1-step Q-learning과 동일한 알고리즘입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 8.1) Dyna Maze&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이번에는 간단한 예제를 통해 Dyna를 더 자세히 살펴보겠습니다. 아래 그림의 오른쪽 위를 보시면 간단한 GridWorld가 있습니다. State는 각각의 칸, Action은 동서남북 4방향으로 주어져 있습니다. 만약 갈 수 없는 방향인 경우 Agent의 State는 바뀌지 않습니다. Reward는 Goal에 도착하면 +1, 그 외에는 모두 0으로 주어집니다. Goal에 도착하면 Episode가 종료되고 다시 Start 지점으로 돌아가 새 Episode를 시작합니다. Discount는 $\gamma = 0.95$로 주어져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제에서 초기의 Action-Value는 모두 0, Step-size Parameter $\alpha = 0.1$, Exploration Parameter $\epsilon = 0.1$으로 설정되어 있습니다. 위의 그래프는 이 문제에 대한 평균 학습 곡선을 나타내고 있습니다. Planning이 하나도 없는 Direct RL은 처음에 아무런 정보가 없기 때문에 굉장히 많은 Step을 거치고 나서야 Goal에 도달한 것을 알 수 있습니다. Step 수가 너무 커서 그래프에 표시되지 않았으나, 약 1700 Step이 소요되었습니다. 그 이후로 빠르게 Step 수가 줄어들긴 합니다만, 수렴하기까지 약 25개의 Episode가 필요함을 알 수 있습니다. 그러나 5개의 Planning만 추가해도 수렴 속도가 눈에 띄게 빨라지며, 50개의 Planning이 추가되었을 때는 3개의 Episode만에 수렴함을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;어째서 Planning을 추가했을 때 학습이 빠르게 수렴하는지는 위의 그림을 보면 알 수 있습니다. 6장에서 배운대로 1-step Q-learning은 Episode의 마지막 부분에서만 Q 값이 Update되기 때문에 수렴하기까지 속도가 오래 걸립니다. 그러나 Planning을 사용하면 Planning Step이 클 수록 하나의 Episode에서 많은 Q 값이 동시에 학습되기 때문에 그만큼 수렴이 빨라집니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;when-the-model-is-wrong&quot;&gt;When the Model Is Wrong&lt;/h2&gt;

&lt;p&gt;Example 8.1에서 보여드린 Dyna Maze 예제에서와 같이 Model의 Update는 매우 수월했습니다. 즉, Model에 대한 사전 정보 없이, 이상적으로 정확한 Model이 생성되었습니다. 그러나 이렇게 항상 우리가 원하는 대로 Model이 생성될 수는 없습니다. Model이 불완전할 수 있을 뿐만 아니라 정확하지 않을 수도 있습니다. 이렇게 되는 원인은 Environment가 Stochastic으로 주어진 경우라던가, 제한된 적은 수의 Sample만 관찰된다던가, 불완전한 근사 함수를 토대로 학습된다던가, 아니면 심지어 Environment가 바뀌었는데 그것을 관찰하지 못한 경우가 있을 수도 있습니다. 만약 Model에 이렇게 문제가 생긴다면, Planning 단계에서 Suboptimal Policy를 계산할 수 있습니다. 어떤 경우에는 Planning에 의해 계산된 Suboptimal Policy로 Model에 대한 오류를 빠르게 발견하고 수정할 수 있습니다. 다음 예제를 통해 이것이 어떤 의미인지 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 8.2) Blocking Maze&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 예제는 비교적 사소한 Modeling 오류를 복구하는 문제입니다. 위의 그림 중 왼쪽 상단의 미로는 시작 지점 S부터 목표 지점 G 사이에 긴 벽이 존재합니다. 1000개의 시간 단계가 지나면 오른쪽과 같이 벽의 위치를 바꿉니다. 그래프는 이 문제에서 Dyna-Q와 이를 수정한 Dyna-Q+에 대한 평균 누적 Reward를 나타냅니다. 1000 시간 단계 후에 평평한 부분은 Agent가 벽 아래에서 어느 방향으로 가야 목표 지점에 도달할지 찾는 시간입니다. 이 구간이 지나면 다시 두 Agent는 목표 지점으로 갈 수 있는 최단 경로를 찾아 다시 Reward를 얻게 됩니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;이렇게 Environment가 급격하게 변해 기존의 Policy를 사용하지 못하는 경우에는 누적 Reward의 차이만 있을 뿐 두 방법 모두 새로운 Policy를 찾을 수 있습니다. 문제는 Environment에서 기존의 답보다 더 좋은 새로운 답이 생기는 경우에 발생합니다. 그 예시를 다음 예제에서 보여드리겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 8.3) Shortcut Maze&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 예제는 왼쪽에 출구가 있고, 시작 지점 S부터 목표 지점 G 사이에 긴 벽이 있는 상황입니다. 3000 시간 단계 후에, 이번에는 오른쪽에 새로운 출구가 생깁니다. 당연히 눈으로 보았을 때는 오른쪽 출구를 통해 목표 지점 G에 가는 것이 더 빠른 경로입니다. 그러나 아래의 그래프를 보시면 Dyna-Q 알고리즘은 오른쪽에 새로운 출구를 발견하지 못해 계속 기존의 Policy를 고수하는 것을 알 수 있습니다. Dyna-Q+ 알고리즘은 이와 다르게 결국 새로운 출구를 찾기는 하나 상당 수의 시간 단계가 소요되는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;여기서 발생할 수 있는 또 다른 문제는 Exploration과 Exploitation 간의 조율입니다. Planning에서 Exploration은 Model을 개선하는 시도를 의미하지만, Exploitation은 현재 Model에서 주어진 최적의 방식으로 행동하는 것을 의미합니다. 강화학습에서의 Exploration vs Exploitation과 마찬가지로 완벽하게 조율할 수 있는 해법은 없지만, 간단한 Heuristic 방법으로 어느 정도 해결할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이전 예제에 언급된 Dyna-Q+가 바로 이러한 Heuristic 방법을 사용합니다. Dyna-Q+의 Agent는 Environment와의 실제 상호작용에서 State-Action 쌍이 마지막으로 선택된 이후 경과된 시간 단계를 측정합니다. 선택된 지 오래 지날수록 Model이 정확해지지 않을 가능성이 크기 때문에, 해당 State-Action 쌍을 선택하도록 유도하기 위해 보너스 Reward를 제공합니다. 예를 들어, 어떤 특정한 State-Action 쌍이 $r$의 Reward를 제공한다고 가정했을 때, 이 State-Action 쌍이 시간 단계 $\tau$ 만큼 선택되지 않는다면, 해당 State-Action 쌍의 Reward는 $r + \kappa \sqrt{\tau}$가 되는 구조입니다. 물론 이렇게 Exploration을 강제로 유도하는 것은 그만큼의 손해가 발생하나, 이전 예제와 같이 Environment가 변하는 경우에는 감수할만한 가치가 충분히 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;prioritized-sweeping&quot;&gt;Prioritized Sweeping&lt;/h2&gt;

&lt;p&gt;Dyna의 Agent에서 Simulated Transition은 이전에 경험했던 모든 State-Action 쌍에서 무작위로 균일하게 선택되어 시작됩니다. 그러나 균일한 선택이 반드시 최선은 아닙니다. 상황에 따라 특정 State-Action 쌍에 초점을 맞추는 것이 Planning에 더 효율적일 수 있습니다. 예를 들어, Example 8.1에서 WITHOUT PLANNING을 보시면 첫 번째 Episode에서의 학습은 Goal 지점 직전에서만 이루어졌습니다. 두 번째 Episode가 시작할 때, Goal 지점의 직전 State-Action을 제외하고는 여전히 초기화된 0의 값만 가지고 있습니다. 즉, 이런 상황에서 대부분의 State-Action 쌍은 방문해봤자 값이 변하지 않기 때문에 Update를 하는 의미가 없습니다. 그렇기 때문에 초기 몇 번의 Episode에서의 Update는 낭비나 다를 바 없습니다.&lt;/p&gt;

&lt;p&gt;이러한 Update 낭비를 해결하는 방법으로, 역방향 학습을 떠올려볼 수 있습니다. Goal 지점 직전 부분에서 값이 변했다면, 그 부분부터 역으로 진행하는 방법으로 낭비 없는 Update를 할 수 있기 때문입니다. 이것을 Planning에 도입한 것을 &lt;span style=&quot;color:red&quot;&gt;Backward Focusing&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;Backward Focusing을 사용해 낭비없이 많은 State-Action 쌍을 Update할 수 있지만, 모든 State-Action 쌍에 대해 유용하지 않을 수 있습니다. 어떤 State-Action 쌍의 값은 많이 Update 될 수도 있지만, 그렇지 않은 State-Action 쌍 또한 분명히 존재할 수 있기 때문입니다. 특히 Stochastic Environment에서 이럴 가능성이 높습니다. 따라서 이 때는 각 State-Action 쌍마다 우선순위를 정하여 우선순위가 높은 State-Action 쌍을 많이 방문하게 만드는 것이 좋습니다. 이렇게 우선순위에 따라 Update 하는 것을 &lt;span style=&quot;color:red&quot;&gt;Prioritized Sweeping&lt;/span&gt;이라고 합니다. 이 때, Update로 변경된 값이 클수록 우선순위가 높습니다. Update를 한 후, 값의 변화가 크다면 그만큼 적게 방문했다는 뜻이기 때문입니다. 이러한 방식으로 Update를 하게 되면 수렴할 때까지 효율적으로 전파가 가능해집니다. Deterministic Environment에서 Prioritized Sweeping에 대한 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 8.4) Prioritized Sweeping on Mazes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다시 미로 예제로 돌아가봅시다. 위의 그래프를 보시면 미로 문제에서 Prioritized Sweeping은 Dyna-Q 방법보다 수렴속도가 5~10배 정도 빨라짐을 알 수 있습니다. 두 방법 모두 Planning 단계를 $n=5$로 동일하게 설정한 결과입니다. (Peng and Williams (1993))&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Stochastic Environment에 Prioritized Sweeping을 적용하는 것도 간단합니다. 각 State-Action 쌍을 경험한 횟수와 그로부터 이어지는 다음 State가 무엇이었는지를 직접 세면 됩니다. 즉, 가능한 모든 다음 State와 발생 확률을 고려하여 우선순위를 정하는 것입니다.&lt;/p&gt;

&lt;p&gt;지금까지 Prioritized Sweeping의 장점을 위주로 소개하였으나, Prioritized Sweeping는 Planning을 효율적으로 개선하는 방법 중 하나일 뿐 최선의 방법은 아닙니다. Prioritized Sweeping의 대표적인 한계는 Expected Update를 사용한다는 것입니다. 이것은 Stochastic Environment에서 확률이 낮은 Transition에 대해 많은 계산을 낭비할 수 있습니다. 이에 반해 Sample Update는 Sampling으로 인한 분산을 감안하더라도 더 적은 계산으로 Value Function을 실제와 가깝게 추정할 수 있습니다. Expected Update와 Sample Update의 비교는 다음 Section에서 자세히 다루겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;expected-vs-sample-updates&quot;&gt;Expected vs. Sample Updates&lt;/h2&gt;

&lt;p&gt;지금까지 배운 내용의 대부분은 Value Function을 추정하기 위한 Update 방법이었습니다. 1-step Update로 한정해서 생각해보면 크게 3가지의 요소를 토대로 분류할 수 있습니다. 첫 번째 요소는 State의 Value를 Update하는지, 아니면 Action의 Value를 Update하는지에 대한 여부입니다. 두 번째 요소는 Optimal Policy에 대한 Value를 추정하는지, 아니면 주어진 임의의 Policy에 대한 Value를 추정하는지에 대한 여부입니다. 이 두 요소를 조합했을 때 4가지 종류의 Value Function $q_{*}$, $v_{*}$, $q_{\pi}$ 그리고 $v_{\pi}$로 간단하게 표기할 수 있습니다. 마지막 세 번째 요소는 발생할 수 있는 모든 가능한 이벤트를 고려하는 Expected Update인지, 아니면 발생할 수 있는 단일 Sample을 고려한 Sample Update인지에 대한 여부입니다. 이 3가지 요소를 조합하면 총 8가지의 경우의 수가 나오며, 그 중 7개는 아래 그림처럼 정리할 수 있습니다. 한 가지가 빠진 이유는 그다지 유용한 방법이 아니기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림과 같은 7가지 분류는 Learning 뿐만 아니라 Planning에도 적용할 수 있습니다. 예를 들어, 이전 Section에서 다루었던 Dyna-Q는 $q_{*}$ + Sample Update를 사용하지만, $q_{*}$ + Expected Update, 또는 $q_{\pi}$ + Sample Update를 사용할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;6장에서 1-step Sample Update를 소개할 때, Expected Update의 대안으로 소개하였습니다. 정확한 Model이 없으면 Expected Update가 불가능하기 때문에, Environment나 Sample Model의 정보를 사용하여 Sample Update를 수행할 수 있기 때문입니다. 이런 관점으로 볼 때, 만약 Sample Update와 Expected Update를 모두 사용할 수 있는 문제라면 Expected Update가 더 낫다는 것을 알 수 있습니다. Expected Update는 Sampling Error로 인한 문제가 발생하지 않기 때문에 확실히 더 낫다고 볼 수 있지만, 더 많은 계산이 필요합니다. 대부분의 문제에서 사용할 수 있는 계산 자원은 한계가 있기 때문에 두 방법을 적절하게 평가하기 위해서는 이 점을 반영해야 합니다.&lt;/p&gt;

&lt;p&gt;구체적으로, $q_{*}$를 추정할 때 Expected Update와 Sample Update를 각각 고려해보겠습니다. State와 Action은 이산적, Approximate Value Function은 Tabular-Q, 그리고 Transition Function은 $\hat{p}(s’, r \mid s, a)$라고 가정하겠습니다. 이 때, Expected Update는 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[Q(s, a) \gets \sum_{s&apos;, r} \hat{p}(s&apos;, r | s, a) \left[ r + \gamma \max_{a&apos;} Q(s&apos;, a&apos;) \right] \tag{8.1}\]

&lt;p&gt;Sample Update 버전은 아래와 같이 Q-learning과 유사한 형태로 표현됩니다. $\alpha$는 step-size parameter입니다.&lt;/p&gt;

\[Q(s, a) \gets Q(s, a) + \alpha \left[ R + \gamma \max_{a&apos;} Q(S&apos;, a&apos;) - Q(s, a) \right] \tag{8.2}\]

&lt;p&gt;Expected Update와 Sample Update의 차이는 State와 Action을 토대로 가능한 다음 State가 여러 개가 존재할 수 있는 Stochastic Environment에서 발생합니다. 만약 Deterministic Environment이라면, $\alpha = 1$로 설정했을 때 Expected Update와 Sample Update는 동일합니다. Stochastic Environment에서 Sample Update는 Sampling Error의 영향을 받지만, 한번에 하나의 다음 State만 고려하기 때문에 계산이 적고, Expected Update는 모든 가능한 다음 State를 고려하기 때문에 Sampling Error가 존재하지 않지만 그만큼 많은 계산량을 필요로 합니다. 가능한 다음 State의 개수를 &lt;span style=&quot;color:red&quot;&gt;Branching Factor&lt;/span&gt; $b$라고 하는데, Expected Update는 Sample Update보다 대략 $b$배 만큼 더 많은 계산이 필요합니다.&lt;/p&gt;

&lt;p&gt;Expected Update를 하는데 충분한 계산 자원과 시간이 있는 경우, Expected Update는 Sampling Error가 없기 때문에 $b$번의 Sample Update보다 좋은 추정 결과를 보입니다. 그러나 많은 State-Action 쌍이 있는 큰 문제에서는 충분한 계산 자원이나 시간이 없기 때문에 Sample Update가 더 나은 추정 결과를 보이게 됩니다. 그렇다면 어떤 상황에서 Expected Update를 사용할 지, Sample Update를 사용할 지를 생각해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프는 다양한 $b$의 값을 토대로 Expected Update와 Sample Update를 비교한 결과입니다. Expected Update가 $b$만큼 연산을 했다는 것은 모든 State-Action 쌍을 고려했다는 것이므로 $b$ 이후에 RMS error는 0이 됩니다. Sample Update는 대략 $\sqrt{\frac{b-1}{bt}}$ 그래프의 모양으로 RMS error가 감소합니다. $t$는 수행된 Sample Update의 수 입니다. 이 그래프로 알 수 있는 점은 적당히 큰 $b$의 경우 Sample Update의 오류가 생각보다 빨리 줄어든다는 것입니다. 이러한 결과는 Sample Update가 확률론적 분기 요인이 크고 State가 너무 많아 정확하게 풀 수 없는 문제에서는 Expected Update보다 우수할 수 있음을 시사합니다.&lt;/p&gt;

&lt;h2 id=&quot;trajectory-sampling&quot;&gt;Trajectory Sampling&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 Update를 분류하는 두 가지 방법을 비교합니다. 첫 번째는 Dynamic Programming의 고전적인 접근 방법으로 전체 State(또는 State-Action 쌍)에 대해 Sweep을 수행하고 각 State(또는 State-Action)의 Value를 Update합니다. 이 방법은 한 번의 Sweep당 한 번의 Update를 수행하는데, Sweep에 오랜 시간이 걸릴 수 있는 대규모 문제에 적합하지 않을 수 있습니다.&lt;/p&gt;

&lt;p&gt;두 번째는 특정한 Distribution에 따라 State, 또는 State-Action을 Sampling하는 것입니다. Dyna-Q와 같이 Uniform Sampling을 할 수도 있지만, 문제에 따라서 Uniform Sampling은 적합하지 않을 수도 있습니다. Sampling을 하는 또 다른 방법으로는 On-policy Distribution에 따라 Sampling하는 것입니다. 즉, 현재의 Policy를 따를 때 관찰할 수 있는 Distribution에 따라 Sampling 및 Update를 하는 것입니다. 이 방법의 장점은 단순히 현재 Policy에 따라 Model과 상호 작용함으로써 쉽게 생성할 수 있다는 것입니다. Episodic Task나 Continuing Task 모두 상관없이 사용할 수 있으며, Model을 기반으로 시뮬레이션함으로써 Update를 수행합니다. 이 방법을 &lt;span style=&quot;color:red&quot;&gt;Trajectory Sampling&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;On-policy Distribution에 초점을 맞추면 중요하지 않은 수많은 State(또는 State-Action) 쌍이 무시되기 때문에 유리할 수도 있지만, 같은 이유로 오래 Update되지 않은 부분이 계속 영향을 끼치기 때문에 불리할 수도 있습니다. 이 책에서는 이것을 실험을 통해 경험적으로 평가하였습니다. 실험에서는 식 (8.1)과 같은 1-step Expected Tabular Update를 사용하여 Uniform Distribution과 On-policy Distribution으로 나누어 시뮬레이션했습니다. 두 방법 모두 $\epsilon = 0.1$로 설정한 $\epsilon$-greedy policy를 사용하였으며, Discount는 무시하였습니다. 모든 State에서 Branching Factor $b$는 동일하게 설정하였으며, 모든 State에서 Episode가 종료될 확률은 0.1입니다. 각 Transition에 대한 Reward는 Expectation이 0, Variance가 1인 Normal Distribution을 따르도록 설정하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-15.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프 중 윗부분은 1000개의 State와 Branching Factor를 각각 1, 3, 10으로 설정한 Environment에서 200개의 Sample에 대한 평균 결과를 나타냅니다. 각 Policy의 우수성은 Expected Update가 완료된 Update의 수에 대한 함수로 표현합니다. 모든 경우에서 On-policy Distribution에 따른 Sampling은 Uniform Distribution에 비해 초기에 더 빨리 좋은 성능을 보여주는 것을 알 수 있습니다. 아래쪽의 그래프는 10000개의 State에서 Branching Factor가 1인 경우에서의 실험입니다. 두 그래프를 토대로 State의 개수나 Branching Factor에 상관 없이 항상 On-policy Distribution에 따른 Sampling은 Uniform Distribution의 Sampling에 비해 초기에 우수한 성능을 보임을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;그러나 위의 그래프를 다시보면, Branching Factor가 큰 경우에는 장기적으로 보았을 때 Uniform Distribution의 Sampling이 약간 더 우수한 성능을 보입니다. 즉, 단기적으로는 On-policy Distribution에 따른 Sampling이 빠르게 수렴값에 가까워지지만, 장기적으로는 Uniform Distribution의 Sampling이 더 우수한 성능을 보일 수도 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;real-time-dynamic-programming&quot;&gt;Real-time Dynamic Programming&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Real-time Dynamic Programming (RTDP)&lt;/span&gt;은 Dynamic Programming의 Value Iteration을 On-policy Trajectory Sampling으로 구현한 버전입니다. Dynamic Programming은 Sweep 기반 Policy Iteration과 밀접하게 관련되어 있기 때문에 RTDP를 통해 On-policy Trajectory Sampling이 어떤 이점이 있는지 쉽게 비교할 수 있습니다. RTDP는 식 (4.10)에 정의한 대로 Expected Tabular Value Iteration을 통해 실제, 또는 시뮬레이션 된 Trajectory에서 방문한 State의 Value를 Update합니다.&lt;/p&gt;

&lt;p&gt;RTDP는 Section 4.5에서 언급한 Asynchronous DP의 한 종류입니다. RTDP에서 Update 순서는 실제, 또는 시뮬레이션 된 Trajectory에서 방문한 순서에 의해 결정됩니다.&lt;/p&gt;

&lt;p&gt;만약 Trajectory가 지정된 시작 State의 집합에서만 시작할 수 있고, 주어진 Policy에 대한 Prediction에만 관심이 있는 경우, On-policy Trajectory Sampling을 통해 주어진 Policy로 도달할 수 없는 State들은 완전히 무시할 수 있습니다. 주어진 Policy를 평가하는게 아니라 Optimal Policy를 찾는 것이 목표인 Control에서 Optimal Policy로 도달할 수 없는 State가 있는 경우 그 State로 가는 Action을 선택할 필요가 없습니다. 이렇게 Optimal Policy와 관련 없는 State에 대해 임의의 Action을 지정하거나, 정의하지 않을 수도 있는 Policy를 &lt;span style=&quot;color:red&quot;&gt;Optimal Partial Policy&lt;/span&gt;라고 합니다. 표현이 조금 헷갈리지만, 아래 그림을 보시면 무슨 뜻인지 이해하실 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-16.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러나 Sarsa와 같은 On-policy Trajectory Sampling Control에서 Optimal Partial Policy를 찾기 위해서는 일반적으로 모든 State-Action 쌍을 무한히 방문해야 합니다. 이것은 Section 5.3과 같은 Assumption of Exploring Starts를 사용할 수도 있지만, Prediction과 달리 Control에서는 Optimal Policy으로 수렴하기 위해 State, 또는 State-Action 쌍 Update를 중지할 수 없습니다.&lt;/p&gt;

&lt;p&gt;이와 다르게 RTDP의 가장 큰 장점은 합리적인 조건을 만족하는 일부 문제에 대해 모든 State를 무한히 자주 방문하지 않거나, Optimal Policy와 관련 없는 일부 State를 전혀 방문하지 않고도 Optimal Policy를 찾는 것이 보장된다는 것입니다. 이는 State가 매우 많아 단일 Sweep도 수행하기 힘든 문제를 해결할 때 큰 장점이 될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 8.6) RTDP on the Racetrack&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;구체적인 예제를 통해 Dynamic Programming과 RTDP를 비교해보겠습니다. 아래의 그림과 같이 자동차가 Starting line에서 Finish line까지 도달하는 것이 목표인 Grid World 문제가 있습니다. Starting line에서 Finish line까지 도달하기 위해서는 자동차가 오른쪽으로 턴하는 Action이 반드시 필요한데, 트랙을 벗어나지 않으면서 가능한 빨리 Finish line에 도달해야합니다. 각 Episode는 자동차가 Finish line에 도달하면 종료됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-17.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;State의 집합은 자동차의 속도가 되는데, Starting line에서 속도는 0입니다. 속도의 제한은 딱히 없으므로 State의 집합은 무한대라고 볼 수 있습니다. 하지만 0부터 시작하는 속도가 갑자기 무한대에 가까워질수는 없으니, 실질적으로 도달할 수 있는 속도는 유한하다고 볼 수 있습니다. Reward는 모든 단계에서 -1이고, 만약 자동차가 트랙의 경계에 부딪히면 Starting line의 무작위 지점으로 이동하고 Episode가 계속됩니다.&lt;/p&gt;

&lt;p&gt;예제 중 왼쪽의 그림은 시작 State에서 도달할 수 있는 State가 총 9,115개입니다. 하지만 실제로 Optimal Policy와 관련이 있는 State는 그 중 599개 뿐입니다. 이 문제를 Dynamic Programming과 RTDP로 각각 수행했을 때 평균적인 결과는 아래 표와 같습니다. 두 방법 모두 초기 값은 동일합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-18.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dynamic Programming은 각 State 집합 전체를 Sweep하는 Value Iteration이며, Sweep 한 번에 하나의 State가 Update됩니다. Dynamic Programming에서는 State를 Update했을 때 그 변동값이 $10^{-4}$ 미만일 때 수렴된 것으로 판단하였고, RTDP는 20회 이상의 Episode에서 Finish Line을 통과한 시간이 비슷한 시간 단계일 때 수렴한 것으로 판단하였습니다.&lt;/p&gt;

&lt;p&gt;두 방법 모두 평균적으로 14~15 시간 단계를 소요하는 Optimal Policy를 생성하였지만, RTDP는 Dynamic Programming이 수행한 Update 수의 절반만 사용했습니다. 이것이 바로 On-policy Trajectory Sampling의 결과입니다. Dynamic Programming은 각각의 Sweep에서 모든 State의 Value를 Update했지만, RTDP는 Optimal Policy와 관련된 일부의 State만 Update하였습니다. RTDP는 평균적으로 100회 이하로 방문한 State의 98.45%를, 10회 이하로 방문한 State의 80.51%의 Value를 Update했습니다. 또한 약 290개의 State의 Value는 전혀 Update하지 않았습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;RTDP의 또 다른 장점은 Value Function이 Optimal Value Function $v_{*}$에 접근함에 따라 Agent가 Policy를 따라 사용하는 Trajectory가 Optimal Policy에 가까워진다는 것입니다. 이것은 Value Iteration 또한 알고리즘이 종료되기 직전의 Value Function은 $v_{*}$에 가깝고 이 때의 Greedy Policy도 Optimal Policy에 가깝지만, Value Iteration이 종료되기 전의 Greedy Policy가 Optimal Policy와 가까운지 확인하는 것은 Dynamic Programming에 포함되어 있지 않으며, 그것을 구현하기 위해서는 상당한 추가 계산이 필요합니다.&lt;/p&gt;

&lt;p&gt;이 RTDP 예제를 통해 Trajectory Sampling의 몇 가지 장점을 알 수 있었습니다. 기존의 Value Iteration은 모든 State의 Value를 계속 Update했지만, RTDP는 목표와 관련된 State의 집합에만 초점을 맞췄습니다. 이 초점은 학습이 진행될수록 점점 더 좁아지며, 결국에는 Optimal Policy를 구성하는 State에만 초점을 맞추도록 수렴됩니다. RTDP는 Sweep 기반 Value Iteration에 비해 약 50%의 계산만 사용하고도 Control을 성공적으로 수행하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;planning-at-decision-time&quot;&gt;Planning at Decision Time&lt;/h2&gt;

&lt;p&gt;지금까지 이번 장의 내용은 Model에서 얻은 Simulated Experience를 기반으로 Policy나 Value Function을 개선하기 위해 Planning을 사용했습니다. 현재 State에서 Action을 선택하기 전에, 현재 State를 포함한 이전의 많은 State에서 Action을 선택하는데 Planning을 사용했습니다. 이런 개념으로, Planning은 현재 State에만 집중되지 않습니다. 이렇게 사용되는 Planning을 &lt;span style=&quot;color:red&quot;&gt;Background Planning&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;Planning을 사용하는 또 다른 방법은 매번 새로운 State $S_t$를 방문한 후, Planning을 시작하고 완료하는 것입니다. 즉, State $S_{t+1}$을 방문한 후, Planning을 시작하여 $A_{t+1}$을 생성하는 방식입니다. 이런 사용 방법은 State의 Value만 사용할 수 있고, Model로부터 예측한 다음 State의 Value를 토대로 Action을 선택합니다. 이 방법은 Planning이 특정 State에 초점을 맞추게 됩니다. 이 방법을 &lt;span style=&quot;color:red&quot;&gt;Decision-time Planning&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;Decision-time Planning은 Value와 Policy가 현재 State와 Action의 선택에 따라 달라지므로 Planning 과정에서 생성된 Value와 Policy는 현재 State에서 Action을 선택한 후 삭제됩니다. 이것이 문제처럼 보일 수 있지만, 일반적인 Environment에서는 매우 많은 State가 있고, 한번 State를 방문한 후 오랫동안 같은 State로 돌아갈 가능성이 거의 없기 때문에 큰 문제는 아닙니다. 방문하는 State마다 Planning을 수행하기 때문에, Decision-time Planning은 빠른 응답이 필요하지 않은 Environment에서 유용합니다. 예를 들면, 체스 게임에서는 한 번의 Action에 몇 초 ~ 몇 분의 계산 시간이 허용될 수 있습니다. 다만 Routing과 같이 즉각적으로 빠른 응답이 필요한 Environment에서는 Background Planning을 사용하여 각 State에 대해 신속하게 적용할 수 있는 Policy를 계산하는 것이 더 좋습니다.&lt;/p&gt;

&lt;h2 id=&quot;heuristic-search&quot;&gt;Heuristic Search&lt;/h2&gt;

&lt;p&gt;인공지능의 고전적인 State-space Planning 방법은 Heuristic Search로 알려진 Decision-time Planning 방법입니다. Heuristic Search에서는 만나는 State마다 Tree를 생성합니다. Tree의 Leaf node에 추정한 Value Function를 적용하고, 현재 State를 Root node로 두어 Leaf node부터 Root node까지 거꾸로 올라가는(Backup) 방식으로 탐색을 합니다. 이 과정은 현재 State에 대한 State-Action 쌍이 담긴 노드에서 중지되며, 계산한 Backup 값 중 현재 State에서 가장 좋은 Action을 선택한 다음 나머지 Backup 값은 폐기합니다.&lt;/p&gt;

&lt;p&gt;기존의 Heuristic Search에서는 추정한 Value Function가 변경되었을 때 Backup된 값을 저장하지 않았습니다. 왜냐하면 기본적으로 Value Function은 사람이 설계하는 것이고, 검색을 한다고 해서 그 값이 변경되지는 않기 때문입니다. 하지만 Heuristic Search 도중에 계산된 Backup 값이나, 지금까지 배운 다른 방법들을 사용하여 Value Function을 개선해볼 수는 있습니다.&lt;/p&gt;

&lt;p&gt;Heuristic Search를 통해 더 나은 Action을 선택하는 방법은 그만큼 더 Tree를 깊게 생성해서 검색하는 것입니다. 만약 완벽한 Model(그리고 불완전한 Value Function)이 주어진다면 깊게 검색할수록 일반적으로 더 나은 Policy를 만들 수 있습니다. 극단적으로 Episode 끝까지 탐색할 수 있다면 불완전한 Value Function일지라도 Optimal Action을 선택할 수 있습니다. 하지만 더 깊게 검색할수록 더 많은 계산이 필요하기 때문에 그만큼 응답 시간이 느려진다는 단점도 있습니다. (이것에 대한 구체적인 예시는 뒤에 나오는 Section 16.1의 TD-Gammon을 참고해주시기 바랍니다)&lt;/p&gt;

&lt;p&gt;Heuristic Search에서 가장 중요한 것이 현재 State임을 간과해서는 안됩니다. Heuristic Search는 검색 트리가 현재 State에서 즉시 사용할 수 있는 Action이나 후속 State에 집중되어 있기 때문입니다. 가령, 계산 자원과 메모리 자원은 임박한 이벤트에 우선적으로 할당되어야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-19.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Update의 Distribution은 현재 State와 가능한 후속 State에 초점을 맞추기 위해 지금까지와 유사한 방식으로 변경할 수 있습니다. 제한적이지만 Heuristic Search를 사용하여 Search Tree를 구성한 다음, 위의 그림과 같이 상향식으로 1-step Update를 수행할 수 있습니다. Update가 이러한 방식으로 이루어지고 테이블 형식으로(Tabular) 사용된다면 &lt;strong&gt;Depth-first Heuristic Search&lt;/strong&gt;와 정확히 동일한 Update라고 볼 수 있습니다. 모든 State Space 검색은 이렇게 다수의 1-step Update를 결합하는 것으로 볼 수 있습니다. 즉, 깊게 검색할수록 성능이 향상하는 이유는 multi-step을 사용하기 때문이 아니라, 현재 State로부터 바로 다음에 있는 State와 Action에 대한 Update의 집중 때문입니다. Action을 결정하는 것에 대해 많은 양의 계산을 할당함으로써 그렇지 않은 Update보다 더 나은 의사 결정을 수행할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;rollout-algorithms&quot;&gt;Rollout Algorithms&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Rollout Algorithm&lt;/span&gt;은 현재 State에서 시작하여 Simulated Trajectory에 적용된 Monte Carlo Control을 기반으로 하는 Decision-time Planning 알고리즘입니다. 현재 State에서 가능한 Action 각각에서 시작하여 주어진 Policy를 따라 Simulated Trajectory의 Reward를 평균화하여 주어진 Policy에 대한 Action Value를 추정합니다. 추정한 Action Value가 충분히 정확하다고 판단되면 가장 높은 추정값을 갖는 Action이 선택되고, 그 Action의 결과로 생성된 다음 State에서 이 과정이 반복됩니다. Rollout 이라는 이름이 붙은 이유는 주사위를 여러번 던져서 주사위의 Value를 추정하는 것에서 유래했습니다. (Tesauro and Galperin, 1997)&lt;/p&gt;

&lt;p&gt;5장에서 설명한 Monte Carlo Control 알고리즘과 달리 Rollout Algorithm의 목표는 주어진 Policy $\pi$에 대해 Optimal Action Value Function $q_{*}$를 찾거나, Action Value Function $q_{\pi}$를 추정하는 것이 아닙니다. 그 대신 주어진 Policy에 대해 현재 State에 대한 Action Value의 Monte Carlo Prediction을 계산합니다. (이것을 Rollout Policy라고 부릅니다) Decision-time Planning 알고리즘으로써, 사용한 Action Value의 추정값은 바로 폐기합니다. Rollout Algorithm은 모든 State-Action 쌍에 대한 결과를 Sampling할 필요가 없고, State 공간이나 State-Action 공간에 대한 함수를 근사화할 필요가 없기 때문에 구현하기가 비교적 간단한 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;Rollout Algorithm의 목표는 Rollout Policy를 개선하는 것입니다. 그러나 이 과정에서 중요한 Trade-off가 있는데, Rollout Policy를 향상시킬수록 시뮬레이션하는데 더 많은 시간이 필요하다는 것입니다. Rollout Algorithm은 Decision-time Planning 방법으로써 엄격한 시간 제약을 충족해야 합니다. Rollout Algorithm에 필요한 계산 시간은 &lt;u&gt;① 각 결정에 대해 평가해야하는 Action의 수&lt;/u&gt;, &lt;u&gt;② 유용한 Sample Reward를 얻는 데 필요한 Simulated Trajectory의 시간 단계 수&lt;/u&gt;, &lt;u&gt;③ Rollout Policy가 결정을 내리는 데 걸리는 시간&lt;/u&gt;, 그리고 &lt;u&gt;④ 우수한 Monte Carlo Action Value 추정치를 얻기 위해 필요한 Simulated Trajectory의 수&lt;/u&gt;가 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 요소들의 균형을 맞추는 것은 Rollout 방법을 사용할 때 중요하지만, 이를 조금 완화할 수 있는 몇 가지 방법이 있습니다. 첫째로, Monte Carlo 시행은 서로 독립적이기 때문에 여러 개의 프로세서에서 이 시행들을 병렬적으로 실행할 수 있습니다. 둘째로, 5장에서 Episode를 부분 종료한 테크닉을 활용하는 것입니다. (Tesauro and Galperin, 1997)이 제안한 것처럼 Monte Carlo 시뮬레이션을 모니터링하여 최적이 될 것 같지 않은 Action을 미리 제거할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;Rollout Algorithm은 Value나 Policy에 대한 장기적인 기억을 유지하지 않기 때문에 일반적으로 학습 알고리즘으로 간주하지 않습니다. 그러나 Rollout Algorithm은 이 책에서 다루는 강화학습의 일부 특징을 활용합니다. Rollout Algorithm은 Sample Trajectory의 Return을 평균화하는 과정에서 Environment의 Sample Model과 상호작용합니다. 이런 식으로 전체 Sweep을 사용하지 않고 Expected Update 대신 Sample에 의존하여 Model의 필요성을 피하는 강화학습의 특성을 갖고있기 때문입니다. 마지막으로 Rollout Algorithm은 추정된 Action Value에 대해 Greedy Action을 선택함으로써 Policy Improvement 속성을 이용한다는 점이 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-tree-search&quot;&gt;Monte Carlo Tree Search&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Monte Carlo Tree Search (MCTS)&lt;/span&gt;는 최근에 나온 Decision-time Planning의 가장 핫한 예시입니다. MCTS는 기본적으로 이전 Section에서 설명한 Rollout Algorithm에 기반하지만, 시뮬레이션에서 보다 높은 Reward를 받는 Trajectory로 연속적으로 유도하기 위해, Monte Carlo 시뮬레이션에서 얻은 Value 추정값을 누적하는 수단을 추가하였습니다. MCTS는 2016년 AlphaGo가 이세돌을 꺾는 데 결정적인 역할을 한 알고리즘입니다. 물론 MCTS는 게임에 국한된 알고리즘은 아닙니다.&lt;/p&gt;

&lt;p&gt;MCTS가 실행되는 순서는 기존과 조금 다른데, 주어진 State에서 후속 State가 먼저 발생한 후 그 State를 만들기 위한 Action을 선택하는 방식입니다. Rollout Algorithm에서와 같이 각각의 실행은 현재 State에서 시작해서 최종 State로 실행되기까지 많은 Trajectory를 시뮬레이션하는 반복 프로세스입니다. MCTS의 핵심 아이디어는 이전 시뮬레이션에서 높은 평가를 받은 Trajectory의 초기 부분에 집중하여, 연속적인 시뮬레이션을 수행하는 것입니다. MCTS를 구현할 때, 다음 Action을 선택할 때 사용한 추정한 Value Function이나 Policy를 보존하는 경우가 많으나, 그렇게 하지 않아도 문제는 없습니다.&lt;/p&gt;

&lt;p&gt;대부분 Simulated Trajectory의 Action들은 단순한 Policy에 의해 생성되는데, Rollout Algorithm에 사용되는 Policy이기 때문에 Rollout Policy라고 합니다. Rollout Policy와 Model 모두 많은 계산이 필요하지 않다면 짧은 시간에 많은 Simulated Trajectory를 생성할 수 있습니다. 모든 Tabular Monte Carlo Method에서 State-Action 쌍의 Value는 해당 쌍의 Simulated Reward의 평균으로 추정됩니다. Monte Carlo의 Value Prediction은 아래 그림과 같이 현재 State를 Root로 하는 Tree 구조를 가지며, 도달할 가능성이 가장 높은 State-Action 쌍의 하위 집합만 유지됩니다. MCTS는 Simulated Trajectory의 결과를 기반으로 최적의 State가 될 수 있는 후보들을 노드에 추가하여 Tree를 확장합니다. 이러한 State에 대해 Action 중 일부는 Value 추정값을 가지고 있으므로 Exploration과 Exploitation를 잘 조절하여 Tree Policy라는 Policy를 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-20.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 MCTS의 기본 버전입니다. 각 단계에 대해 조금 더 자세히 설명하면,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Selection&lt;/strong&gt; : Root Node에서 시작하여 Tree의 가장자리에 연결된 Action Value를 기반으로 Tree Policy를 사용해 Tree를 순회하며 Leaf Node를 선택합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Expansion&lt;/strong&gt; : 일부 반복 과정 중에 탐색되지 않은 Action을 선택함으로써, 선택한 노드에서 도달한 하나 이상의 자식 노드를 추가하여 Leaf Node를 확장합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Simulation&lt;/strong&gt; : Rollout Policy를 사용하여 선택한 노드, 또는 새로 추가된 자식 노드 중 하나에서 전체 Episode를 시뮬레이션합니다. 결과는 Tree Policy에 의해 먼저 선택된 Action과 Rollout Policy에 의해 Tree 이후의 Action이 포함된 Monte Carlo 시행입니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Backup&lt;/strong&gt; : 시뮬레이션된 Episode에 의해 생성된 Return은 MCTS의 반복 과정에서 Tree Policy가 수행한 Action Value를 Update하거나, 초기화하기 위해 백업됩니다. Rollout Policy에 의해 Tree 바깥에서 방문한 State 및 Action에 대한 Value는 저장되지 않습니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;MCTS는 시간적으로, 그리고 계산 리소스가 허용하는 만큼 이 4단계를 계속 반복합니다. 그 후 마지막으로 이 반복 과정을 통해 누적된 통계에 의해 Root Node의 Action이 선택됩니다. 이 때 Action을 선택하는 기준은 여러 가지가 있는데, 예를 들어 Root Node에서 선택할 수 있는 모든 Action 중 가장 Value가 큰 Action을 선택할 수도 있고, 또는 가장 안좋은 후속 State를 피할 수 있는 Action을 선택할 수도 있습니다. 어쨌든 이렇게 Action을 선택해서 다음 State에 도달하면, MCTS를 다시 실행하여 다음 Action을 선택합니다. 이 때, 다음 State에 도달하고 나면 이전에 계산했던 Tree는 모두 폐기됩니다.&lt;/p&gt;

&lt;p&gt;MCTS가 어떻게 이렇게 우수한 결과를 도출할 수 있는지는 이 책에서 설명하는 강화학습의 원리와 연관시키면 이해할 수 있습니다. 기본적으로 MCTS는 Root Node에서 시작하는 시뮬레이션이 적용된 Monte Carlo Control을 기반으로 하는 Decision-time Planning 알고리즘입니다. 즉, 이전 Section에서 설명한 Rollout Algorithm의 한 종류입니다. 따라서 Online, Incremental, Sample 기반의 Value Prediction 및 Policy Improvement의 장점을 그대로 보유하고 있습니다. 이 외에도 Tree에 Action-Value 추정값을 저장하고, 강화학습의 Sample Update를 사용하여 Update합니다. 이것은 Monte Carlo 시행을 할 때, 초기에 높은 Reward를 갖는 Trajectory를 따르는 경로에 집중하는 효과가 있습니다. 또한 Tree를 점진적으로 확장함으로써 이러한 Trajectory에서 방문한 State-Action 쌍의 추정값을 효과적으로 저장 및 조회할 수 있습니다. 따라서 MCTS는 탐색에 과거 경험을 사용하는 이점을 가지면서 Action-Value Function을 전체적으로 근사해야하는 문제를 피할 수 있습니다.&lt;/p&gt;

&lt;p&gt;MCTS의 이러한 성공은 강화학습 뿐만 아니라 인공지능 전반에 큰 영향을 끼쳤으며, 현재에도 여러 응용 프로그램에 사용하기 위해 수정 및 확장을 연구하고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary-of-the-chapter&quot;&gt;Summary of the Chapter&lt;/h2&gt;

&lt;p&gt;Planning에는 Environment Model이 필요합니다. Distribution Model은 다음 State의 확률과 가능한 Action에 대한 Reward로 구성됩니다. Sample Model은 이러한 확률에 따라 생성된 단일 Transition 및 Reward를 생성합니다. Dynamic Programming은 가능한 모든 다음 State 및 Reward에 대한 기대값을 계산하는 Expected Update를 사용하기 때문에 Distribution Model이 필요합니다. 반면에 대부분의 강화학습 알고리즘에서는 Sample Update를 사용하므로 Sample Model을 사용합니다. Sample Model은 일반적으로 Distribution Model보다 훨씬 쉽게 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 장에서는 최적의 Action을 Learning하는 것과 Planning하는 것 사이에 관계를 강조하였습니다. 두 방법 모두 동일하게 Value Function을 추정하고, 작은 Backup 계산을 점진적으로 Update합니다. 이 공통점을 사용해 Learning과 Planning 모두 동일한 Value Function Prediction을 Update하도록 설계함으로써 간단하게 통합할 수 있습니다. 또한 어떤 Learning 방법이라도 Real Experience가 아니라 Model에 의한 Simulated Experience에 적용하기만 하면 Planning에 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Incremental Planning 방법을 Acting 및 Model Learning과 통합하는 것은 간단합니다. Planning, Acting, Model Learning 간에는 순환적으로 상호작용하며, 이를 통합하는 가장 간단한 방식은 모든 프로세스를 비동기화하여 병렬식으로 수행하는 것입니다. 이 방법은 프로세스 간의 계산 자원을 효율적으로 분배할 수 있다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;또한 이 장에서 State Space에서 Planning 방법에 대해 다루었습니다. 첫째로 Update 크기의 변화입니다. Update가 작을수록 Planning이 더 Incremental 될 수 있습니다. 가장 작은 Update는 Dyna와 같은 1-step Sample Update입니다. 둘째로 Update의 Distribution, 즉 검색의 초점입니다. Prioritized Sweeping은 최근에 값이 변경된 State의 역방향에 초점을 맞춥니다. On-policy Trajectory Sampling은 Agent가 Environment를 제어할 때 만날 수 있는 State, 또는 State-Action 쌍에 초점을 둡니다. 이를 통해 Prediction이나 Control과 관련이 없는 State Space의 일부를 건너뛸 수 있습니다. Real-time Dynamic Programming은 Value Iteration의 On-policy Trajectory Sampling 버전으로써 기존 Sweep 기반의 Policy Iteration에 비해 몇 가지 장점을 보여줍니다.&lt;/p&gt;

&lt;p&gt;Planning은 Agent가 Environment와 상호작용할 때 실제로 발생하는 State에도 초점을 맞출 수도 있습니다. 이것의 가장 중요한 형태는 Planning이 Action을 선택하는 프로세스의 일부로 수행될 때입니다. 인공지능 분야에서 연구되는 고전적인 Heuristic Search가 그 예시입니다. 그 외에 Online, Incremental, Sample에 기반한 Value Prediction 및 Policy Improvement의 이점을 제공하는 Rollout Algorithm과 Monte Carlo Tree Search가 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary-of-part-i--dimensions&quot;&gt;Summary of Part I : Dimensions&lt;/h2&gt;

&lt;p&gt;8장을 마지막으로 이 책의 1부가 끝납니다. 1부에서는 강화학습의 여러 방법들이 가지고 있는 공통적인 아이디어를 위주로 제시했습니다. 각각의 아이디어는 방법을 변형시킬 수 있는 Dimension으로 볼 수 있습니다. 이러한 Dimension의 집합은 가능한 방법들이 펼쳐진 공간으로 볼 수 있습니다. 이번 Section에서는 방법 공간의 Dimension 개념을 사용하여 지금까지 배운 강화학습의 관점을 요약합니다.&lt;/p&gt;

&lt;p&gt;지금까지 배운 강화학습의 방법에는 세 가지 공통된 핵심 아이디어가 있습니다. 첫째로, 모든 방법이 Value Function을 추정했습니다. 둘째로, 모든 방법이 실제로 움직인 Trajectory 또는 가능한 State의 Trajectory를 따라 값을 Backup했습니다. 마지막으로 모든 방법이 Generalized Policy Iteration의 전략에 기반했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/8. Planning and Learning with Tabular Methods/RL 08-21.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림을 보시면 방법을 분류하는 가장 핵심적인 두 가지 Dimension이 나와 있습니다. 두 Dimension은 Value Function을 개선하는데 사용되는 Update의 종류와 관련되어 있습니다. 가로 방향은 Sample Update인지 Expected Update인지에 대한 여부입니다. Expected Update는 Distribution Model이 필요한 반면, Sample Update에는 Sample Model만 필요하거나 아예 Model이 없는 Real Experience에서도 수행할 수 있습니다. 세로 방향은 Update의 깊이, 즉 Bootstrapping에 대한 정도를 나타냅니다. 위 그림의 네 꼭지점을 보시면 그 중 세 지점에는 Value Function을 추정하는 방법인 Dynamic Programming, Temporal Difference Learning, 그리고 Monte Carlo Method가 나와 있습니다. Temporal Difference Learning과 Monte Carlo Method 사이에는 그림에는 나와있지 않지만 $n$-step TD 방법이 있으며, Bootstrapping을 많이 할수록(즉, $n$의 값을 커질수록) Monte Carlo에 가까워집니다.&lt;/p&gt;

&lt;p&gt;Dynamic Programming은 1-step Expected Update를 포함하기 때문에 맨 오른쪽 상단 끝에 위치합니다. 오른쪽 하단 끝은 Expected Update가 매우 깊어 마지막 State까지 실행되는 극단적인 경우입니다. 이것은 사실상 모든 경우의 수를 체크하는 방법이기 때문에 &lt;span style=&quot;color:red&quot;&gt;Exhaustive Search&lt;/span&gt;라고 부릅니다. 이 극단적인 방법들 사이에는 Heuristic Search 방법과 제한된 깊이까지 검색하고 Update 하는 방법이 포함됩니다. Dynamic Programming과 Temporal Difference Learning 사이에는 Expected Update와 Sample Update를 혼합하는 방법, 그리고 단일 Update에서 Sample과 Distribution을 혼합하는 방법이 포함됩니다.&lt;/p&gt;

&lt;p&gt;이 두 가지 Dimension 외에 마지막으로 논의할 수 있는 세 번째 Dimension은 On-policy와 Off-policy에 대한 구분입니다. On-policy는 현재 사용하는 Policy에 대한 Value Function을 학습하지만, Off-policy는 가장 좋다고 판단되는 다른 Policy로부터 현재 Policy의 Value Function을 학습합니다. Policy를 생성하는 것은 일반적으로 탐색의 필요성으로 인해 현재 가장 좋다고 생각하는 것과 다릅니다. 이 세 번째 Dimension은 위의 그림에 직접적으로 표시되지는 않았으나, 그 그림의 평면에서 수직으로 표현된다고 보시면 됩니다. 이 외에도 이 교재에서는 아래와 같이 여러 가지 다른 Dimension을 구분해놓았습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Definition of Return&lt;/strong&gt; : Episodic Task인가, 또는 Continuing Task인가? 그리고 Discount 되는가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Action value vs State value vs Afterstate value&lt;/strong&gt; : 어떤 종류의 Value를 추정해야 하는가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Action Selection/Exploration&lt;/strong&gt; : Exploration과 Exploitation을 적절히 조절하기 위해 어떻게 Action을 선택해야 하는가? (ex. $\epsilon$-greedy, soft-max, upper confidence bound)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Synchronous vs Asynchronous&lt;/strong&gt; : 모든 State에 대한 Update를 동시에 수행할 것인가, 아니면 순서에 따라 하나씩 수행할 것인가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Real vs Simulated&lt;/strong&gt; : Real Experience으로 Update할 것인가? 아니면 Simulated Experience으로 Update할 것인가? 또는 둘 다인가? 둘 다라면 어떤 방법으로 융합하는가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Location of Updates&lt;/strong&gt; : 어떤 State, 또는 State-Action 쌍을 Update 하는가? (Model이 없는 방법은 실제로 발생한 State와 State-Action 쌍만 가능하지만 Model에 기반한 방법은 임의로 선택할 수 있음)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Timing of Updates&lt;/strong&gt; : Update는 Action 선택의 일부로 수행하는가? 아니면 나중에 수행하는가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory for Updates&lt;/strong&gt; : Update된 값을 얼마나 유지해야 하는가? Heuristic Search에서와 같이 Action을 선택할 동안에만 유지하고 폐기할 것인가, 아니면 영구적으로 유지할 것인가?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;물론 이러한 Dimension의 구분은 완전하지 않고, 서로 독립적이지도 않습니다. 각각의 개별 알고리즘들은 이 외에도 여러 차이가 있으며, 많은 알고리즘은 여러 개의 Dimension에 걸쳐 있습니다. 예를 들면, Dyna 방법은 Real Experience와 Simulated Experience를 모두 사용하여 동일한 Value Function에 영향을 끼칩니다. 물론 이와 다르게 여러 개의 Value Function을 사용하는 것도 합리적인 방법입니다. 이 Section에서 논하고자 하는 것은 가능한 방법들의 분류 방식을 소개하고 새로운 방법이 나왔을 때, 또는 새로운 방법을 만들고자 할 때의 아이디어를 정리하는 것입니다.&lt;/p&gt;

&lt;p&gt;하지만 1부에서 언급하지 않은 가장 중요한 Dimension이 하나 있습니다. 2부에서는 Function Approximation이라는 새로운 아이디어를 본격적으로 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;8장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">지금까지 배운 강화학습 방법에서 Dynamic Programming, Heuristic Search와 같이 Model이 필요한 방법을 Model-based라고 하고, Monte Carlo Method, Temporal Difference Learning과 같이 Model 없이 사용할 수 있는 방법을 Model-free라고 합니다. 해법을 구할 때 Model-based 방법은 Planning을 사용하지만 Model-free 방법은 Learning을 사용하는 차이점이 있습니다. 그러나 두 방법은 Value Function을 계산하는 과정에서 유사점이 있습니다. 두 방법 모두 미래에 발생하는 이벤트를 토대로 Value Function을 추정하기 때문입니다. 이번 장에서는 Model-based와 Model-free를 통합한 새로운 방법을 제안합니다. 지난 장에서 Monte Carlo Method과 TD(0)를 $n$-step TD로 통합하는 방법을 보여드렸는데, 이번 장의 통합 과정도 이와 유사합니다. 특히 서로 다른 두 방법이 어디까지 통합될 수 있는지에 집중할 예정입니다.</summary></entry><entry><title type="html">$n$-step Bootstrapping</title><link href="http://localhost:4000/studies/n-step-bootstrapping/" rel="alternate" type="text/html" title="$n$-step Bootstrapping" /><published>2022-04-06T00:00:00+09:00</published><updated>2022-04-06T00:00:00+09:00</updated><id>http://localhost:4000/studies/n-step-bootstrapping</id><content type="html" xml:base="http://localhost:4000/studies/n-step-bootstrapping/">&lt;p&gt;이번 장에서는 5장에서 배운 Monte Carlo Method과 6장에서 배운 Temporal Difference (TD)를 융합하여 만든 새로운 방법을 소개합니다. Monte Carlo Method는 항상 Episode가 끝난 후에야 학습이 가능했고, TD는 1단계만 관찰하면 학습이 가능했습니다. 그렇다면 그 사이의 단계인 $n$번째 단계까지 관찰한 다음 학습을 하게 되면 조금 더 일반화된 학습이 가능하지 않을까하는 아이디어가 떠오르게 됩니다.&lt;/p&gt;

&lt;p&gt;이렇게 $n$개의 시간 단계 동안 관찰한 후 학습에 반영하는 것을 &lt;span style=&quot;color:red&quot;&gt;$n$-step Bootstrapping&lt;/span&gt;이라고 합니다. TD은 경험이 즉각적으로 학습에 반영되지만, 때때로 조금 더 장기적인 관점에서 바라봐야하는 문제가 있습니다. 이런 관점에서 $n$-step Bootstrapping은 12장에서 배울 Eligibility Traces의 기반이 됩니다.&lt;/p&gt;

&lt;p&gt;이번 장 역시 이전 장들과 마찬가지로, 먼저 Prediction을 알아본 다음에 Control을 다루는 순서로 진행됩니다. 즉, 먼저 $n$-step Bootstrapping으로 $v_{\pi}$를 추정한 후, Optimal Policy를 찾기 위한 Control 방법을 논의할 예정입니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-td-prediction&quot;&gt;$n$-step TD Prediction&lt;/h2&gt;

&lt;p&gt;Policy $\pi$를 사용하여 생성된 Sample Episode에서 $v_{\pi}$를 추정할 때, Monte Carlo Method는 Episode가 끝날 때까지 해당 State부터 관찰된 전체 Reward의 합인 Return을 기반으로 업데이트하고, 1-step TD는 1개의 Reward만 관찰한 후 업데이트합니다. 이번에는 이 두 극단적인 방법의 중간점으로 $n$개의 Reward를 관찰한 후 업데이트를 수행하는 $n$-step TD에 대해 알아보겠습니다. 1-step TD, $n$-step TD, 그리고 Monte Carlo Method의 차이는 아래의 Backup Diagram을 보시면 쉽게 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step 업데이트를 1-step TD와 마찬가지로 TD라고 부르는 이유는, 1-step TD처럼 이후의 추정이 어떻게 달라지는지에 따라 이전의 추정이 변하기 때문입니다. 다만 그 추정이 1-step 후가 아니라 $n$-step 후일 뿐입니다.&lt;/p&gt;

&lt;p&gt;이들간의 차이를 수식으로 비교해봅시다. 먼저 Monte Carlo Method에서 Return $G_t$를 계산하는 식은 다음과 같았습니다.&lt;/p&gt;

\[G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T\]

&lt;p&gt;위 식에서 $T$는 Episode의 마지막 시간 단계입니다. Monte Carlo Method에서는 Return을 각 시간 단계마다 받는 Reward에 Discount $\gamma$를 곱한 값의 합으로 계산합니다.&lt;/p&gt;

\[G_{t:t+1} \doteq R_{t+1} + \gamma V_t (S_{t+1})\]

&lt;p&gt;반면에 1-step TD에서는 첫 번째 Reward와, 다음 State의 추정 Value에 Discount를 곱한 값의 합으로 계산됩니다. $V_t$는 시간 단계 $t$에서 추정한 $v_{\pi}$를 의미하며, $G_{t:t+1}$는 시간 단계 $t$부터 $t+1$까지 얻은 수익을 의미합니다. 이 개념을 확장하면 2-step의 Return은 다음과 같음을 알 수 있습니다.&lt;/p&gt;

\[G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1} (S_{t+2})\]

&lt;p&gt;위와 같은 방법으로 $n$-step의 Return을 만들면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1} (S_{t+n}) \tag{7.1}\]

&lt;p&gt;위 식에서 $n \ge 1$이고 $0 \le t &amp;lt; T - n$이라는 조건이 있습니다. 만약 $t + n \ge T$라면 $n$-step이 끝나기 전에 Episode가 끝난다는 이야기이므로 $T$ 이후의 항은 모두 0으로 처리하면 됩니다.&lt;/p&gt;

&lt;p&gt;1-step에서 시간 단계 $t+1$에 도달해야 $t$에서의 Value Function을 업데이트 할 수 있던 것처럼, $n$-step 또한 시간 단계 $t+n$에 도달해야만 $t$에서의 Value Function을 업데이트 할 수 있습니다. 그렇기 때문에 $n$-step에서 가장 처음 학습을 하는 시간 단계는 $t+n$이 됩니다. 이 점을 반영하여 $V$의 업데이트 식을 정의하면 다음과 같습니다.&lt;/p&gt;

\[V_{t+n} (S_t) \doteq V_{t+n-1} (S_t) + \alpha \left[ G_{t:t+n} - V_{t+n-1} (S_t) \right], \quad 0 \le t &amp;lt; T \tag{7.2}\]

&lt;p&gt;이 때, $S_t$ 이외의 State에서는 Value Function이 변하지 않습니다. 즉, 모든 $s \ne S_t$에 대해서 $V_{t+n} (s) = V_{t+n-1} (s)$입니다. 이것을 $n$-step TD라고 부릅니다. $n$-step TD의 모든 Episode에서는 처음 $n-1$ 시간 단계까지는 아무것도 변하지 않는데, 이를 보완하기 위해 각 Episode가 끝난 후 다음 Episode가 시작되기 전에 동일한 수의 추가적인 업데이트가 이루어집니다. 완전한 Pseudocode를 보시면 이것이 어떤 의미인지 이해가 되실 겁니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step Return은 Value Function $V_{t+n-1}$를 사용하여 $R_{t+n}$ 이후에 누락된 Reward를 보완합니다. $n$-step Return의 장점은 최악의 상황에서 $V_{t+n-1}$보다 $v_{\pi}$ 추정값이 더 낫다는 것입니다. 다시말해, $n$-step Return에서 가장 큰 오차는 $V_{t+n-1}$의 가장 큰 오차보다 $\gamma^n$ 배 만큼 작거나 같습니다. 이것을 수식으로 표현하면 다음과 같습니다.&lt;/p&gt;

\[\max_s \left| \mathbb{E}_{\pi} \left[ G_{t:t+n} | S_t = s \right] - v_{\pi} (s) \right| \le \gamma^n \max_s \left| V_{t+n-1} (s) - v_{\pi} (s) \right| \tag{7.3}\]

&lt;p&gt;식 (7.3)은 모든 $n \ge 1$에 대해 성립합니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Error Reduction Property of $n$-step Returns&lt;/span&gt;라고 합니다. 이 수식 덕분에 모든 $n$-step TD 방법이 적절한 조건 하에 올바른 추정값으로 수렴한다는 것이 보장됩니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-sarsa&quot;&gt;$n$-step Sarsa&lt;/h2&gt;

&lt;p&gt;이번에는 $n$-step과 Sarsa를 결합한 Control을 배우도록 하겠습니다. 새로 배우는 Sarsa와 구분하기 위해, 이전 장에서 배운 Sarsa를 1-step Sarsa, 또는 Sarsa(0)으로 표기하고, 이번 장에서 배우는 새로운 방법은 &lt;span style=&quot;color:red&quot;&gt;$n$-step Sarsa&lt;/span&gt;로 부르겠습니다. $n$-step Sarsa의 기본 개념은 아래의 Backup Diagram과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 기반으로 $n$-step Return을 재정의하면 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} \gamma^n Q_{t+n-1} (S_{t+n}, S_{t+n}), \quad n \ge 1, 0 \le t &amp;lt; T-n \tag{7.4}\]

&lt;p&gt;물론 이전 Section에서 배운 대로 $t + n \ge T$라면 $G_{t:t+n} \doteq G_t$입니다. 식 (7.4)를 알고리즘에 맞게 수정하면 다음과 같습니다.&lt;/p&gt;

\[Q_{t+n} (S_t, A_t) \doteq Q_{t+n-1} (S_t, A_t) + \alpha \left[ G_{t:t+n} - Q_{t+n-1} (S_t, A_t) \right] \quad 0 \le t &amp;lt; T \tag{7.5}\]

&lt;p&gt;$n$-step Return처럼 식 (7.5)도 모든 $s \ne S_t$, $a \ne A_t$에 대해 $Q_{t+n} (s, a) = Q_{t+n-1}$입니다. 즉, 학습하고 있는 State-Action 쌍을 제외하고는 $Q$ 값이 변하지 않습니다. 그렇기 때문에 $n$-step Sarsa라고 부르는 것입니다. $n$-step Sarsa의 완전한 Pseudocode은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step Sarsa의 장점은 1-step Sarsa보다 학습 속도가 빠르다는 점입니다. 아래 그림은 Grid World에서의 예시를 보여주고 있습니다. 첫 번째 그림과 같은 Episode에 대해, 1-step Sarsa는 가운데 그림처럼 마지막 State-Action 쌍에 대해서만 $Q$ 값의 업데이트가 일어납니다. 하지만 세 번째 그림을 보시면 10-step Sarsa는 Episode 뒤 10개의 State-Action 쌍이 모두 업데이트가 되는 장점이 있습니다. 즉, 하나의 Episode에서 더 많은 것을 학습할 수 있다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step과 Expected Sarsa를 결합하는 방법도 동일합니다. 이에 대한 Backup Diagram은 $n$-step Sarsa의 Backup Diagram 맨 오른쪽에 나타나 있습니다. 주의할 점은 시간 단계 $t$부터 $t+n$까지 모두 평균값을 사용하는 것이 아니라, 마지막 단계에서만 평균값을 사용합니다. &lt;span style=&quot;color:red&quot;&gt;$n$-step Expected Sarsa&lt;/span&gt;의 Return 식은 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n \bar{V}_{t+n-1} (S_{t+n}) \quad t+n &amp;lt; T \tag{7.7}\]

&lt;p&gt;마찬가지로 $t + n \ge T$인 경우에는 $G_{t:t+n} \doteq G_t$입니다. 또한 $\bar{V}_t (s)$는 State $s$의 &lt;strong&gt;Expected Approximate Value&lt;/strong&gt;이고, 다음과 같이 정의됩니다.&lt;/p&gt;

\[\bar{V}_t (s) \doteq \sum_a \pi (a|s) Q_t (s, a) \quad \text{for all } s \in \mathcal{S} \tag{7.8}\]

&lt;p&gt;식 (7.8)은 지금 뿐만이 아니라 다른 장에서도 사용되기 때문에 기억해두시는 것이 좋습니다. 또한 식 (7.8)에서 만약 State $s$가 마지막 State라면 $\bar{V}_t (s) $는 0으로 정의됩니다.&lt;/p&gt;

&lt;h2 id=&quot;n-step-off-policy-learning&quot;&gt;$n$-step Off-policy Learning&lt;/h2&gt;

&lt;p&gt;이번에는 Off-policy 학습의 $n$-step 버전을 배워보겠습니다. $n$-step 방법에서는 Return이 $n$ 단계에 걸쳐 생성됩니다. Off-policy에서 중요한 점은 Target Policy $\pi$와 Behavior Policy $b$가 구분되는 것인데, 이 때 두 Policy 간의 차이를 보정하기 위해 &lt;strong&gt;Importance Sampling&lt;/strong&gt;을 사용하였습니다. 그렇다면 $n$-step 버전에서 두 Policy 간의 Weight를 어떻게 처리하는지 다음 식을 통해 살펴보겠습니다.&lt;/p&gt;

\[V_{t+n} (S_t) \doteq V_{t+n-1} (S_t) + \alpha \rho_{t:t+n-1} \bigg[ G_{t:t+n} - V_{t+n-1} (S_t) \bigg] \quad 0 \le t &amp;lt; T \tag{7.9}\]

&lt;p&gt;식 (7.9)에서 $\rho_{t:t+n-1}$가 바로 Importance Sampling Ratio입니다. 5장에서 배운대로 Importance Sampling Ratio는 다음과 같이 계산합니다.&lt;/p&gt;

\[\rho_{t:h} \doteq \prod_{k=t}^{\min (h,T-1)} \frac{\pi (A_k | S_k)}{b (A_k | S_k)} \tag{7.10}\]

&lt;p&gt;만약 Policy $\pi$에 의해 선택되지 않는 Action의 경우(즉, $\pi (A_k \mid S_k) = 0$)에는 $n$-step Return에 Weight를 0으로 주고 완전히 무시해야 합니다. 반대로 Policy $\pi$가 Policy $b$보다 더 높은 확률로 선택되는 Action의 경우에는 Weight가 증가합니다. 만약 두 Policy $\pi$와 $b$가 동일한 경우 Importance Sampling Ratio는 정확히 1이 됩니다. 따라서 식 (7.9)와 같은 업데이트 식이 유도된 것입니다.&lt;/p&gt;

&lt;p&gt;이와 같은 방법으로, $n$-step Sarsa 또한 다음과 같이 Off-policy 방법으로 대체할 수 있습니다.&lt;/p&gt;

\[Q_{t+n} (S_t, A_t) \doteq Q_{t+n-1} (S_t, A_t) + \alpha \rho_{t+1:t+n} \left[ G_{t:t+n} - Q_{t+n-1} (S_t, A_t) \right] \tag{7.11}\]

&lt;p&gt;여기서 Importance Sampling Ratio는 식 (7.9)의 $n$-step TD 보다 한 단계 늦게 시작하고 끝납니다. 왜냐면 Sarsa는 Q-learning과 달리 다음 Action을 선택한 후에 학습을 하기 때문입니다. 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$n$-step Expected Sarsa의 Off-policy 버전은 Importance Sampling Ratio만 약간 변형시키면 됩니다. $\rho_{t+1:t+n-1}$ 대신 $\rho_{t+1:t+n}$로 바꾼 다음 나머지는 동일합니다. (물론 Return 식은 식 (7.7)을 사용합니다)&lt;/p&gt;

&lt;h2 id=&quot;per-decision-methods-with-control-variates&quot;&gt;Per-decision Methods with Control Variates&lt;/h2&gt;

&lt;p&gt;이전 Section에서 제시한 $n$-step Off-policy 방법은 간단하고 개념적으로 명확하지만, 그렇게 효율적인 방법은 아닙니다. 보다 정교하게 접근하는 방법은 Section 5.9에서 잠깐 소개한 Per-decision Importance Sampling을 사용해야 합니다. 이 접근 방식은 식 (7.1)과 같은 $n$-step Return을 다음과 같은 Recursive Form으로부터 시작합니다.&lt;/p&gt;

\[G_{t:h} = R_{t+1} + \gamma G_{t+1:h} \quad t&amp;lt;h&amp;lt;T \tag{7.12}\]

&lt;p&gt;이 때, $G_{h:h} \doteq V_{h-1}(S_h)$입니다. Importance Sampling Ratio를 고려할 때, 만약 Policy $\pi$에서 시간 $t$에 방문하지 않는 Action에 대해서 $\rho_t$는 0입니다. 이렇게 계산하면 $n$-step Return 또한 0이되고, Target Policy에 대해 Variance가 높아지는 문제가 있습니다. 그렇기 때문에 여기서는 이보다 정교한 새로운 방법을 제안하며, Off-policy의 $n$-step Return을 다음과 같이 재정의합니다.&lt;/p&gt;

\[G_{t:h} \doteq \rho_t (R_{t+1} + \gamma G_{t+1:h}) + (1 - \rho_t) V_{h-1} (S_t) \quad t&amp;lt;h&amp;lt;T \tag{7.13}\]

&lt;p&gt;식 (7.12)와 동일하게 $G_{h:h} \doteq V_{h-1}(S_h)$입니다. 식 (7.13)은 기존과 다르게 $\rho_t$가 0이라고 할지라도 Return이 0이 되지 않는 대신 기존 추정치인 $V_{h-1} (S_t)$와 동일해집니다. Importance Sampling Ratio가 0이라는 뜻은 Sample을 무시하라는 뜻이므로 추정치를 변경하지 않게 바꾼 것입니다. 이 때, 식 (7.13)에서 오른쪽 두 번째 항을 &lt;span style=&quot;color:red&quot;&gt;Control Variate&lt;/span&gt;라고 합니다. Importance Sampling Ratio의 기대값이 1이라면 추정값과 상관이 없으므로 Control Variate의 기대값은 0이 됩니다. 또한 식 (7.13)과 같은 Off-policy의 $n$-step Return 정의는 식 (7.1)과 같은 On-policy Return 정의를 엄격하게 일반화했다고 볼 수 있습니다. 실제로 식 (7.13)에서 $\rho_t$를 항상 1이라고 가정하면 식 (7.1)과 동일합니다. 식 (7.13)의 업데이트 식은 식 (7.2)를 그대로 사용하면 됩니다.&lt;/p&gt;

&lt;p&gt;Action-Value 버전에서는 첫 번째 Action이 Importance Sampling에서 영향을 끼치지 않기 때문에 $n$-step Return의 Off-policy 정의는 약간 다릅니다. 식 (7.7)과 같이 Expectation Form으로도, 식 (7.12)와 같은 Recursive Form으로도 표현할 수 있기 때문에 두 가지 표현 방법을 모두 보여드리도록 하겠습니다. 다음 식은 Control Variate를 포함하였습니다.&lt;/p&gt;

\[\begin{align}
G_{t:h} &amp;amp;\doteq R_{t+1} + \gamma \left( \rho_{t+1} G_{t+1:h} + \bar{V}_{h-1} (S_{t+1}) - \rho_{t+1} Q_{h-1} (S_{t+1}, A_{t+1}) \right) \\ \\
&amp;amp;= R_{t+1} + \gamma \rho_{t+1} \left( G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1}) \right) + \gamma \bar{V}_{h-1} (S_{t+1}), t &amp;lt; h \le T \tag{7.14}
\end{align}\]

&lt;p&gt;Recursive Form에서 $h &amp;lt; T$인 경우 $G_{h:h} \doteq Q_{h-1} (S_h, A_h)$로, $h \ge T$라면 $G_{T-1:h} \doteq R_T$로 끝납니다. 식 (7.5)와 결합하여 추정 알고리즘을 만들면 Expected Sarsa와 유사한 형태가 나옵니다.&lt;/p&gt;

&lt;p&gt;지금까지 사용한 Importance Sampling은 Off-policy 학습을 가능하게 하지만 높은 Variance를 유발하기 때문에 Step-size parameter를 작게 설정해야 합니다. 다만 이로 인해 학습 속도는 느려질 수밖에 없습니다. 즉, Off-policy 방법은 On-policy 방법보다 학습 속도가 느립니다. 물론 이를 개선하기 위한 여러 연구가 진행되고 있습니다. 이번 Section에서 다룬 Control Variate가 그 예 중 하나이며, 이 외에도 &lt;strong&gt;Autostep&lt;/strong&gt; (Mahmood, Sutton, Degris and Pilarski, 2012), &lt;strong&gt;Tian&lt;/strong&gt; (Karampatziakis and Langford, 2010), &lt;strong&gt;Mahmood&lt;/strong&gt;(2017; Mahmood and Sutton, 2015) 등이 방법이 제안되었습니다. 다음 Section에서는 또 다른 방법인 Importance Sampling을 사용하지 않는 Off-policy 학습 방법에 대해 다루어 보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;off-policy-learning-without-importance-sampling--the-n-step-tree-backup-algorithm&quot;&gt;Off-policy Learning Without Importance Sampling : The n-step Tree Backup Algorithm&lt;/h2&gt;

&lt;p&gt;6장에서 1-step  Q-learning과 Expected Sarsa를 배울 때 Importance Sampling을 사용하지 않는 방법에 대해 배웠습니다. 이를 $n$-step으로 확장한 방법으로 Tree-backup Algorithm이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tree-backup Algorithm의 기본 아이디어는 위의 Backup Diagram과 같습니다. Diagram의 각 단계에는 3가지 Sample State와 Reward, 그리고 2개의 Sample Action이 있습니다. 이것은 초기 State, Action 쌍인 $(S_t, A_t)$ 이후에 발생하는 이벤트를 나타냅니다. 각 State에서 가운데 Action을 제외한 나머지 Action은 선택되지 않은 Action입니다. 선택되지 않은 Action은 Sample 데이터가 없기 때문에 Bootstrap하고 Target Policy의 추정값을 업데이트하는데 사용합니다. 이것이 Backup Diagram처럼 나무 모양과 비슷하기 때문에 &lt;span style=&quot;color:red&quot;&gt;Tree-backup&lt;/span&gt;이라고 불립니다.&lt;/p&gt;

&lt;p&gt;Tree-backup의 업데이트는 Tree의 Leaf Node로부터 추정된 Action-Value로부터 시작합니다. 각 Leaf Node는 Target Policy $\pi$로부터 선택될 확률에 비례하는 Weight를 기반으로 Target에 기여합니다. 따라서 첫 번째 Level의 Action은 $\pi (a \mid S_{t+1})$의 Weight로 기여하지만, 실제로 취한 Action $A_{t+1}$은 전혀 기여하지 않습니다. $\pi (A_{t+1} \mid S_{t+1})$은 두 번째 Level의 Action Value에 Weight를 부여하는데 사용됩니다. 즉, 선택되지 않은 두 번째 Level Action의 Weight는 $\pi (A_{t+1} \mid S_{t+1}) \pi (a’ \mid S_{t+2})$가 됩니다. 마찬가지로 세 번째 Level Action의 Weight는 $\pi (A_{t+1} \mid S_{t+1}) \pi (a’ \mid S_{t+2}) \pi (a’’ \mid S_{t+3})$이 됩니다. 쉽게 말해, Backup Diagram에서 각 Leaf Node가 의미하는 것은 Root State로부터 주어진 Policy $\pi$ 하에 해당 Action이 선택될 확률을 의미합니다.&lt;/p&gt;

&lt;p&gt;3-step Tree-backup 업데이트는 각 State별로 가능한 모든 Action을 고려하는 과정과, 주어진 Policy에 따라 Action을 수행하는 과정이 포함되기 때문에 6개의 단계로 나눌 수 있습니다. 이것을 일반적인 $n$-step Tree-backup Algorithm으로 만들어보겠습니다. 먼저, 1-step의 Return은 다음과 같이 Expected Sarsa와 같은 모양으로 표현할 수 있습니다.&lt;/p&gt;

\[G_{t:t+1} \doteq R_{t+1} + \gamma \sum_a \pi(a | S_{t+1}) Q_t (S_{t+1}, a), \quad t &amp;lt; T-1 \tag{7.15}\]

&lt;p&gt;같은 방식으로 2-step Tree-backup의 Return은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
G_{t:t+2} &amp;amp; \doteq R_{t+1} + \gamma \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) Q_{t+1} (S_{t+1}, a) \\ \\
&amp;amp;+ \gamma \pi (A_{t+1} | S_{t+1}) \left( R_{t+2} + \gamma \sum_a \pi (a | S_{t+2}) Q_{t+1} (S_{t+2}, a) \right) \\ \\
&amp;amp;= R_{t+1} + \gamma \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) Q_{t+1}(S_{t+1}, a) + \gamma \pi (A_{t+1} | S_{t+1}) G_{t+1:t+2}, \quad t &amp;lt; T-2
\end{align}\]

&lt;p&gt;이를 반복하여 일반적인 $n$-step Tree-backup Return의 Recursive Form은 다음과 같습니다.&lt;/p&gt;

\[G_{t:t+n} \doteq R_{t+1} + \gamma \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) Q_{t+n-1} (S_{t+1}, a) + \gamma \pi (A_{t+1} | S_{t+1}) G_{t+1:t+n}   \tag{7.16}\]

&lt;p&gt;단, $t &amp;lt; T - 1, n \ge 2$ 입니다. 만약 $n = 1$인 경우 식 (7.15)와 같으며, 예외적으로 $G_{T-1:t+n} \doteq R_T$입니다. 이것은 $n$-step Sarsa에서 다음과 같이 업데이트됩니다.&lt;/p&gt;

\[Q_{t+n} (S_t, A_t) \doteq Q_{t+n-1} (S_t, A_t) + \alpha \left[ G_{t:t+n} - Q_{t+n-1} (S_t, A_t) \right], \quad - \ge t &amp;lt; T\]

&lt;p&gt;물론 학습에 사용되지 않는 모든 State $s \ne S_t$, 모든 Action $a \ne A_t$에 대해서는 Q 값이 변하지 않습니다. (즉, $Q_{t+n} (s, a) = Q_{t+n-1} (s, a)$) 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-unifying-algorithm-n-step-qsigma&quot;&gt;A Unifying Algorithm: $n$-step $Q(\sigma)$&lt;/h2&gt;

&lt;p&gt;지금까지 우리는 $n$-step Sarsa, $n$-step Tree-backup, 그리고 $n$-step Expected Sarsa에 대해 공부했습니다. 이 셋의 가장 큰 차이는 Sample로 인한 Importance Sampling의 여부입니다. $n$-step Sarsa는 매 단계마다 Importance Sampling Ratio를 보정해주어야 하고, $n$-step Expected Sarsa 또한 마지막을 제외한 모든 단계에서 Importance Sampling Ratio 보정이 필요합니다. $n$-step Tree-backup 알고리즘은 Importance Sampling이 필요없는 것이 특징입니다. 이번 Section에서는 이 3개의 알고리즘을 통합하는 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기본적인 아이디어는 위의 그림에서 네 번째 Backup Diagram과 같습니다. 매 단계 Sarsa 처럼 처리할 것인지, 아니면 Tree-backup 처럼 처리할 것인지의 여부를 따로 정하는 것입니다. Expected Sarsa 처럼 처리하기 위해서는 마지막 단계를 Tree-backup 처럼 처리하면 됩니다.&lt;/p&gt;

&lt;p&gt;단계 별로 어떻게 처리할 것인지는 $\sigma$의 값에 따라 달라집니다. $t$ 단계에서의 $\sigma$ 값을 $\sigma_t \in [0, 1]$라고 표기합니다. 만약  $\sigma = 1$인 경우 Sarsa와 동일해지고(=Full Sampling), $\sigma = 0$인 경우 Tree-backup과 동일해집니다(=Pure Expectation). 이 새로운 알고리즘을 &lt;span style=&quot;color:red&quot;&gt;$n$-step $Q(\sigma)$&lt;/span&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;이제 $n$-step $Q(\sigma)$를 수식으로 표현해보겠습니다. 식 (7.16)의 Tree-backup $n$-step Return을 horizon $h = t + n$과 식 (7.8)의 $\bar{V}$로 표현합니다.&lt;/p&gt;

\[\begin{align}
G_{t:h} &amp;amp;= R_{t+1} + \gamma \sum_{a \ne A_{t+1}} \pi (a | S_{t+1}) Q_{h-1} (S_{t+1}, a) + \gamma \pi (A_{t+1} | S_{t+1}) G_{t+1:h} \\ \\
&amp;amp;= R_{t+1} + \gamma \bar{V}_{h-1} (S_{t+1}) - \gamma \pi (A_{t+1} | S_{t+1}) Q_{h-1}(S_{t+1}, A_{t+1}) + \gamma \pi (A_{t+1} | S_{t+1}) G_{t+1:h} \\ \\
&amp;amp;= R_{t+1} + \gamma \pi (A_{t+1} | S_{t+1}) \left( G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1}) \right) + \gamma \bar{V}_{h-1} (S_{t+1})
\end{align}\]

&lt;p&gt;그 이후로는 Importance Sampling Ratio $\rho_{t+1}$을 $\pi (A_{t+1} \mid S_{t+1})$로 대체한 것을 제외하고는 식 (7.14)와 같이 Control Variate가 있는 $n$-step Sarsa와 동일합니다. $Q(\sigma)$는 다음과 같이 표현할 수도 있습니다.&lt;/p&gt;

\[\begin{align}
G_{t:h} &amp;amp; \doteq R_{t+1} + \gamma \left( \sigma_{t+1} \rho_{t+1} + (1 - \sigma_{t+1}) \pi (A_{t+1} | S_{t+1}) \right) \\ \\
&amp;amp;\times \left( G_{t+1:h} - Q_{h-1} (S_{t+1}, A_{t+1}) \right) + \gamma \bar{V}_{h-1} (S_{t+1}) \tag{7.17}
\end{align}\]

&lt;p&gt;이 때 $t &amp;lt; h \le T$ 입니다. Recursive Form에서 $h &amp;lt; T$인 경우 $G_{h:h} \doteq Q_{h-1} (S_h, A_h)$가 되고, $h = T$인 경우 $G_{T-1:T} \doteq R_T$가 됩니다. 그 후 식 (7.11) 대신 Importance Sampling Ratio가 없는 식 (7.5)를 사용하여 $n$-step Sarsa 업데이트를 사용합니다. Importance Sampling Ratio 자체가 $n$-step Return에 포함되기 때문입니다. $Q(\sigma)$의 완전한 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서는 1-step TD와 Monte Carlo Method의 중간으로 볼 수 있는 &lt;strong&gt;$n$-step TD&lt;/strong&gt;에 대해 배웠습니다. 이렇게 극단적인 두 방법을 적절히 조절하여 중간 정도의 방법을 사용하는 것은 때때로 좋은 성능을 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/7. n-step bootstrapping/RL 07-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Section 7.6에서 사용했던 그림을 다시 가져와보면 이번 장에서 배운 내용이 요약되어 있습니다. &lt;strong&gt;$n$-step Sarsa&lt;/strong&gt;나 &lt;strong&gt;Expected Sarsa&lt;/strong&gt;는 기본적으로 Return을 계산할 때 Importance Sampling Ratio를 반드시 고려해야하며, 그 방법을 회피하기 위해 &lt;strong&gt;$n$-step Tree-backup&lt;/strong&gt;을 고안하였습니다. 마지막으로는 이것들을 일반화할 수 있는 &lt;strong&gt;$n$-step $Q(\sigma)$&lt;/strong&gt;를 제안하였습니다.&lt;/p&gt;

&lt;p&gt;이러한 $n$-step 방법의 단점은 지난 장에서 배운 1-step 방법보다 각 시간 단계당 더 많은 계산이 필요할 뿐만 아니라 더 많은 메모리가 필요하다는 것입니다. 12장에서는 이 단점을 최소한으로 줄여 $n$-step TD를 구현하지만, 아무리 줄여도 항상 1-step보다는 계산량과 필요 메모리량이 많다는 한계가 있습니다. 하지만 이러한 단점을 감안하더라도 $n$-step 방법이 가지는 장점이(대표적으로 빠른 학습) 있기 때문에 고려할만한 가치는 있습니다.&lt;/p&gt;

&lt;p&gt;Off-policy $n$-step은 추후 배울 Eligibility Traces보다 복잡하지만 개념적으로 명확하다는 장점이 있습니다. 이 장에서는 Off-policy $n$-step TD에 대해 2가지 방법으로 접근하였습니다. 첫 번째 방법은 기존에 배운 &lt;strong&gt;Importance Sampling&lt;/strong&gt;을 이용하는 방법이었습니다. 이것은 개념적으로 간단하지만 Variance가 크다는 단점이 있습니다. 따라서 만약 Target Policy와 Behavior Policy가 매우 큰 차이가 나는 경우라면 이 방법은 적합하지 않습니다. 두 번째 방법으로 &lt;strong&gt;Tree-backup&lt;/strong&gt; 방법을 제안하였습니다. 이것은 Stochastic Target Policy를 갖는 Q-learning을 $n$-step으로 확장한 개념입니다. Importance Sampling을 포함하지 않지만, Target Policy와 Behavior Policy의 차이가 클 경우 $n$이 크더라도 Bootstrapping이 간단하다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;7장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장에서는 5장에서 배운 Monte Carlo Method과 6장에서 배운 Temporal Difference (TD)를 융합하여 만든 새로운 방법을 소개합니다. Monte Carlo Method는 항상 Episode가 끝난 후에야 학습이 가능했고, TD는 1단계만 관찰하면 학습이 가능했습니다. 그렇다면 그 사이의 단계인 $n$번째 단계까지 관찰한 다음 학습을 하게 되면 조금 더 일반화된 학습이 가능하지 않을까하는 아이디어가 떠오르게 됩니다.</summary></entry><entry><title type="html">Temporal-Difference Learning</title><link href="http://localhost:4000/studies/temporal-difference-learning/" rel="alternate" type="text/html" title="Temporal-Difference Learning" /><published>2022-03-30T00:00:00+09:00</published><updated>2022-03-30T00:00:00+09:00</updated><id>http://localhost:4000/studies/temporal-difference-learning</id><content type="html" xml:base="http://localhost:4000/studies/temporal-difference-learning/">&lt;p&gt;이번 장은 강화학습의 핵심 아이디어인 &lt;span style=&quot;color:red&quot;&gt;Temporal-Difference (TD) Learning&lt;/span&gt;을 다루게 됩니다. TD Learning은 Environment에 대한 정확한 Model 없이 경험을 통해 학습한다는 Monte Carlo의 아이디어와 Bootstrap 하지 않고 학습된 다른 추정치를 기반으로 추정치를 업데이트한다는 Dynamic Programming 아이디어를 결합하여 만들어졌습니다. 이번 장의 시작은 이전 장들과 같이 주어진 Policy $\pi$에 대한 Value Function $v_{\pi}$를 추정하는 문제로부터 시작하며, Optimal Policy을 찾는 Control 문제에서도 이전과 같이 GPI를 변형하여 접근합니다. 이전 장들과의 주요 차이점은 Prediction 문제에 대한 접근 방식입니다.&lt;/p&gt;

&lt;p&gt;이번 장 이후로 나오는 대부분의 주제는 Dynamic Programming, Monte Carlo, TD Learning과 관련이 있으며, 특히 7장에서는 TD Learning과 Monte Carlo 방법의 중간 개념임 $n$-step Bootstrapping을 소개하고, 12장에서는 이것들을 매끄럽게 통합할 수 있는 TD($\lambda$)를 소개합니다.&lt;/p&gt;

&lt;h2 id=&quot;td-prediction&quot;&gt;TD Prediction&lt;/h2&gt;

&lt;p&gt;TD Learning과 Monte Carlo Method는 모두 경험을 사용하여 Value Function을 추정합니다. Monte Carlo 방법은 State를 방문 후, Return의 값을 알 수 있을 때까지 기다린 다음 그 값을 토대로 $V(S_t)$를 추정합니다. 간단한 Every-visit Monte Carlo Method의 $V(S_t)$ 업데이트 식은 이전 장에서 배운 대로 다음과 같습니다.&lt;/p&gt;

\[V(S_t) \gets V(S_t) + \alpha \left[ G_t - V(S_t) \right] \tag{6.1}\]

&lt;p&gt;이전에 배운 대로 Return $G_t$는 시간 $t$ 이후에 얻는 기대 Reward이고, $\alpha$는 Step-size parameter 입니다. 만약 $\alpha$가 상수라면 식 (6.1)을 &lt;span style=&quot;color:red&quot;&gt;Constant-$\alpha$ Monte Carlo&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method에서는 $G_t$를 얻기 위해 Episode가 끝날 때까지 기다려야 하지만, TD Learning에서는 다음 시간 단계까지만 기다리면 된다는 차이가 있습니다. 시간 $t+1$에서 얻은 Reward $R_{t+1}$과 추정된 Value Function $V(S_{t+1})$을 사용하여 $V(S_t)$를 업데이트 할 수 있습니다. 가장 간단한 TD의 업데이트 식은 다음과 같습니다.&lt;/p&gt;

\[V(S_t) \gets V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right] \tag{6.2}\]

&lt;p&gt;식 (6.2)와 같은 업데이트 식을 $TD(0)$, 또는 1-step TD 라고 부릅니다. 이렇게 부르는 이유는 추후 12장에서 다룰 $TD(\lambda)$와 7장에서 다룰 $n$-step TD의 특수한 형태이기 때문입니다. 아래는 $TD(0)$ 업데이트를 사용한 Value Function 추정 방법의 완전한 Pseudocode입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TD(0) 알고리즘은 부분적으로 기존 추정값을 기반으로 업데이트 하기 때문에 DP와 같은 Bootstrapping 방법이라고 부를 수 있습니다. 또한 3장에서 다루었던 Value Function의 추정 식을 다시 가져와보면,&lt;/p&gt;

\[\begin{align}
v_{\pi} &amp;amp; \doteq \mathbb{E}_{pi} \left[ G_t | S_t = s \right] \tag{6.3} \\ \\
&amp;amp;= \mathbb{E}_{pi} \left[ R_{t+1} + \gamma G_{t+1} | S_t = s \right] \tag{from (3.9)} \\ \\
&amp;amp;= \mathbb{E}_{pi} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s \right] \tag{6.4}
\end{align}\]

&lt;p&gt;식 (6.3)은 Monte Carlo Method가 Target으로 하는 추정값이고, 식 (6.4)는 DP가 Target으로 하는 추정값입니다. 두 식이 추정값인 이유는 Monte Carlo Method에서는 실제 Expected Return이 아닌 Sample Return이 사용되기 때문이고, DP에서는 $v_{\pi}(S_{t+1})$가 알려져 있지 않아 $V(S_{t+1})$를 대신 사용하기 때문입니다. TD의 Target은 Sample Return을 사용하며 역시 $V(S_{t+1})$를 사용하기 때문에 추정값이 됩니다. 즉, TD는 Monte Carlo Method와 DP를 결합한 것으로 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 TD(0)에 대한 Backup Diagram입니다. 맨 위의 흰 점은 State 노드를 의미하며, 이에 대한 추정값은 바로 다음 State만 사용하기 때문에 Backup Diagram이 간단하게 표현됩니다. TD와 Monte Carlo 방법은 Sample Update라고 부르는데, 그 이유는 현재 State에서 이어지는 Action과 Reward를 사용하여 원래의 State(또는 State-Action 쌍)를 업데이트하기 때문입니다.&lt;/p&gt;

&lt;p&gt;마지막으로 TD(0)에서 0이 의미하는 것은 $S_t$의 추정값과 $R_{t+1} + \gamma V(S_{t+1})$의 추정값 사이의 차이입니다. 이것을 TD-error라고 하는데, 추후 배울 강화학습에서 다양한 형태로 만나보실 수 있습니다.&lt;/p&gt;

\[\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \tag{6.5}\]

&lt;p&gt;각 시간 단계의 TD-error는 그 당시에 추정했던 값의 오차입니다. TD-error는 다음 State와 다음 Reward에 따라 달라지기 때문에 실제로는 다음 단계 전까지 사용할 수 없습니다. 즉, $\delta_t$는 $V(S_t)$에서 발생하는 오차이지만, $t+1$ 시간이 되어야 알 수 있습니다. 만약 $V$가 Episode가 끝나기 전까지 변경되지 않는다면 Monte Carlo의 오차 또한 TD-error의 합으로 표현할 수 있습니다. (다행히 Monte Carlo Method에서는 Episode가 끝나기 전까지 $V$가 변하지 않습니다)&lt;/p&gt;

\[\begin{align}
G_t - V(S_t) &amp;amp;= R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1}) \tag{from (3.9)} \\ \\
&amp;amp;= \delta_t + \gamma (G_{t+1} - V(S_{t+1})) \\ \\
&amp;amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+2} - V(S_{t+2})) \\ \\
&amp;amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} (G_T - V(S_T)) \\ \\
&amp;amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} (0 - 0) \\ \\
&amp;amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k \tag{6.6}
\end{align}\]

&lt;p&gt;만약 $V$가 Episode 도중에 업데이트 되는 경우 위의 과정은 정확하지 않지만, Step-size가 작으면 대략적으로 근접할 수는 있습니다. 이 과정은 TD에서의 이론과 알고리즘에서 중요한 역할을 합니다.&lt;/p&gt;

&lt;h2 id=&quot;advantage-of-td-prediction-methods&quot;&gt;Advantage of TD Prediction Methods&lt;/h2&gt;

&lt;p&gt;이번 Section에서는 TD가 Monte Carlo Method나 DP에 비해 어떤 이점이 있는지 알아보도록 하겠습니다. 가장 먼저 생각할 수 있는 TD의 장점은 DP와 달리 Environment의 Model이 필요하지 않다는 점입니다. 물론 이것은 Monte Carlo Method도 가지고 있는 장점이긴 합니다.&lt;/p&gt;

&lt;p&gt;그렇다면 Monte Carlo Method에 비해 TD가 가지는 장점은 Incremental로 구현이 가능하다는 것입니다. Monte Carlo Method의 가장 치명적인 단점은 Episode가 끝날 때까지 기다려야 한다는 것입니다. Episode의 길이가 길다면 그만큼 학습하기 위해 대기해야하는 시간 또한 길어지게 됩니다. 지난 장에서 Episode를 일부 무시하거나 줄이는 방법들을 잠깐 소개하였으나, 이것들은 실험적인 방법이기 때문에 문제가 해결된다고 볼 수는 없습니다.&lt;/p&gt;

&lt;p&gt;다만 TD에서 하나의 Sample만을 가지고 학습하는 것이 과연 올바른 값으로 수렴함을 보장하는지를 따져봐야 합니다. 다행히도 고정된 Policy $\pi$에 대해  Step-size parameter $\alpha$가 충분히 작고, 조건 (2.7)을 만족한다면 TD(0)은 확률 1로 $v_{\pi}$에 수렴합니다. 일반적인 상황에 대해서는 9.4에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;TD와 Monte Carlo Method가 모두 수렴하는 것이 보장된다면, 다음으로 논의할 것은 어떤 것이 더 빨리 수렴하는가입니다. 안타깝게도 어떤 방법이 더 빠르게 수렴하는지는 수학적으로 증명되지 않았지만, 일반적으로는 TD가 Monte Carlo Method보다 빠르게 수렴한다고 합니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method와 TD의 장단점을 요약하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monte Carlo has &lt;span style=&quot;color:red&quot;&gt;high variance&lt;/span&gt;, &lt;span style=&quot;color:red&quot;&gt;zero bias&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Good convergence properties (even with function approximation)&lt;/li&gt;
  &lt;li&gt;Not very sensitive to initial value&lt;/li&gt;
  &lt;li&gt;Very simple to understand and use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Temporal-Difference has &lt;span style=&quot;color:red&quot;&gt;low variance&lt;/span&gt;, &lt;span style=&quot;color:red&quot;&gt;some bias&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Usually more efficient than Monte Carlo&lt;/li&gt;
  &lt;li&gt;TD(0) converges to $v_{\pi}(s)$ (but not always with function approximation)&lt;/li&gt;
  &lt;li&gt;More sensitive to initial value&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimality-of-td0&quot;&gt;Optimality of TD(0)&lt;/h2&gt;

&lt;p&gt;만약 10개의 Episode, 또는 100개의 시간 단계와 같이 제한된 숫자의 경험만 사용할 수 있다고 가정해봅시다. 이 경우 Incremental 학습 방법의 접근 방식은 수렴될 때까지 경험을 반복적으로 학습하는 것입니다. 문제는 Value Function $V$가 Episode 당 단 한번만 변경된다는 것입니다. (TD(0) 알고리즘 참고) 이렇게 전체 데이터를 사용하여 학습하고 수렴하도록 만드는 방법을 &lt;span style=&quot;color:red&quot;&gt;Batch Updating&lt;/span&gt;이라고 합니다.&lt;/p&gt;

&lt;p&gt;배치 업데이트에서 TD(0)는 Step-size parameter $\alpha$가 충분히 작다는 조건 하에 답에 수렴합니다. Constant-$\alpha$ Monte Carlo Method도 동일한 조건에서는 수렴하지만, 다른 답으로 수렴합니다. 이것은 설명으로만 이해하기 어렵기 때문에, 교재에 나와있는 예제를 통해 보충하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 6.4) You are the Predictor&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;알 수 없는 MDP 문제에 대해 다음 8개의 Episode가 관찰되었다고 가정해보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A, 0, B, 0&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 1&lt;/li&gt;
  &lt;li&gt;B, 0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;첫 번째 Episode는 State A에서 시작하여 0의 Reward를 받고 State B로 이동한 다음, 0의 Reward를 받고 종료된 것을 의미합니다. 나머지 Episode 중 6개는 State B에서 시작하여 1의 Reward를 받고 종료되고, 마지막 Episode는 State B에서 시작하여 0의 Reward를 받고 종료됩니다.&lt;/p&gt;

&lt;p&gt;위의 8개의 Episode를 기반으로 Value $V(A)$, $V(B)$를 추정해봅시다. 먼저 State B를 보면 8개의 Episode 중 2개의 Episode가 0의 Reward를 받고, 6개의 Episode가 1의 Reward를 받으므로 $V(B) = 0.75$라고 쉽게 추정할 수 있습니다.&lt;/p&gt;

&lt;p&gt;문제는 $V(A)$입니다. 주어진 Episode에서 State A가 언급된 것은 첫 번째 Episode 하나인데, 이것만으로 유추하자면 State A는 100% 확률로 0의 Reward를 받고 State B로 이동하는 것처럼 보입니다. 이것을 그림으로 표현하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, State A의 Value는 $V(B)$와 동일하게 $V(A) = 0.75$라고 말할 수 있습니다. 이 결과는 Batch TD(0)로 계산해도 동일한 결과가 나옵니다.&lt;/p&gt;

&lt;p&gt;또 다른 추정 방법은 8개의 Episode 중 State A는 단 한번 등장했고, 그 때의 Reward는 0이었으므로 $V(A) = 0$으로 추정하는 것입니다. 이 결과는 Batch MC와 동일한 결과입니다. 이렇게 추정하면 실제로 데이터에 대해서는 RMS Error가 0이 됩니다. (0으로 추정했는데 표본이 0이므로) 하지만 생각해봤을 때, $V(A) = 0$으로 추정하는 것 보다 $V(A) = 0.75$로 추정하는 것이 더 합리적으로 보입니다. 이를 토대로 결론을 내려보면, Monte Carlo 방법은 현재 주어진 데이터에서 더 우수한 성능을 보일 수 있지만, 미래에 얻게 될 데이터까지 고려한다면 TD(0)가 더 우수한 성능을 보일 것이라고 기대할 수 있습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Example 6.4를 보시면 Batch TD(0)와 Batch MC로 찾은 추정치가 어떻게 다른지 알 수 있습니다. Batch MC는 항상 Training Data에서 Root Mean Square (RMS) Error를 최소화하는 추정치를 찾지만, Batch TD(0)는 항상 Maximum-likelihood Model에 맞는 추정치를 찾습니다. Batch TD(0)와 같이 추정하는 것을 &lt;span style=&quot;color:red&quot;&gt;Certainty-equivalence Estimate&lt;/span&gt; 라고 합니다.&lt;/p&gt;

&lt;p&gt;Certainty-equivalence Estimate은 확실히 최적의 해법처럼 보이지만, 일반적인 상황에서는 위의 Example 6.4와 같이 직접 Value Function를 계산하는 것이 현실적으로 어렵다는 문제점이 있습니다. $n$을 State의 수라고 하면(즉, $n = \lvert \mathcal{S} \rvert$), Maximum-likelihood Model을 추정하는데 $n^2$의 메모리가 필요하고, Value Function를 계산하는 데 $n^3$ 만큼의 시간 단계가 필요합니다. 이러한 측면에서 보면 TD(0)는 Example 6.4와 같이 직접 계산하지 않고도 훨씬 적은 메모리와 시간을 소모하여 이와 동일한 답으로 수렴하기 때문에 우수한 방법임을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;sarsa--on-policy-td-control&quot;&gt;Sarsa : On-policy TD Control&lt;/h2&gt;

&lt;p&gt;이제는 TD를 사용한 Control 방법에 대해 알아보겠습니다. 이전 장들과 마찬가지로 Control은 Generalized Policy Iteration (GPI)를 기반으로 하며, Evaluation과 Improvement에 TD를 사용합니다. Monte Carlo Method와 마찬가지로 Exploration과 Exploitation 사이의 Trade-off가 있으며 그 때와 동일하게 On-policy와 Off-policy라는 두 가지 방법으로 나누어 접근합니다. 이번 Section에서는 먼저 On-policy를 사용한 TD Control 방법을 다룹니다.&lt;/p&gt;

&lt;p&gt;첫 번째 단계로는 State-Value Function가 아닌 Action-Value Function를 배우는 것입니다. 즉, 모든 State $s$와 Action $a$에 대하여, Policy $\pi$를 기반으로 $q_{\pi} (s, a)$를 추정합니다. 각각의 Episode는 다음과 같이 State와 State-Action이 반복적으로 이어져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지는 계속 State에 대한 Value Function만을 고려했으나, 이번 Section부터는 State-Action 쌍에 대한 Value를 측정하기 때문에 $V(S)$가 아니라 $Q(S, A)$를 사용합니다. 이 부분을 제외하고는 식 자체가 크게 다르지 않고, 수렴 역시 동일하게 보장됩니다. TD(0)에서의 Q 값을 업데이트 하는 식은 다음과 같습니다.&lt;/p&gt;

\[Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right] \tag{6.7}\]

&lt;p&gt;이 업데이트는 마지막 State가 아닌 모든 State $S_t$마다 수행됩니다. 만약 $S_{t+1}$이 마지막 State라면 $Q(S_{t+1}, A_{t+1})$는 0으로 정의됩니다. 이 업데이트를 사용하기 위해서 필요한 요소는 $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$ 5가지입니다. 그렇기 때문에 이 업데이트를 사용한 TD 제어를 &lt;span style=&quot;color:red&quot;&gt;Sarsa&lt;/span&gt;라고 부릅니다. Sarsa의 Backup Diagram은 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sarsa는 이전에 배운 On-policy 방법과 마찬가지로 Policy $\pi$에 대해 $q_{\pi}$를 추정하고, $q_{\pi}$에 대해 greedy하게 $\pi$를 변경하는 과정을 반복합니다. 전체 Sarsa 알고리즘의 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sarsa의 수렴성은 Q에 대한 Policy의 조건이 어떤 지에 따라 다릅니다. $\epsilon$-greedy을 사용하는 것과 상관 없이, Step-size에 대한 조건인 식 (2.7)을 만족하고, 모든 State-Action 쌍이 무한한 횟수로 방문되는 조건 하에 확률 1로 Optimal Policy 및 Action-Value Function으로 수렴합니다.&lt;/p&gt;

&lt;h2 id=&quot;q-learning--off-policy-td-control&quot;&gt;Q-learning : Off-policy TD Control&lt;/h2&gt;

&lt;p&gt;Off-policy TD Control 학습 알고리즘은 &lt;span style=&quot;color:red&quot;&gt;Q-learning&lt;/span&gt;이라는 이름으로 불립니다. Q-learning은 강화학습 중에서도 가장 유명한 학습 방법이며, 1989년에 Chris Watkins 교수님이 처음 제안한 방법입니다. Q-learning의 업데이트 식은 다음과 같습니다.&lt;/p&gt;

\[Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right] \tag{6.8}\]

&lt;p&gt;식 (6.8)은 Sarsa의 업데이트 규칙인 식 (6.7)과 대부분 유사합니다. 딱 한부분만 다른데, 다음 State와 Action을 사용했던 Sarsa와 달리 Q-learning은 다음 State에서 가능한 Action 중 Q값이 가장 큰 Action을 현재의 Q 값에 반영한다는 것입니다. Off-policy는 Behavior Policy와 Target Policy가 구분되기 때문이라고 이해하시면 되겠습니다. Q-learning의 전체 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 6.6) Cliff Walking&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sarsa와 Q-learning의 차이를 한 눈에 알 수 있는 예제로 Cliff Walking이라는 문제가 있습니다. 위와 같이 Gridworld로 구성된 Environment에서 Episode는 S라는 위치에서 시작하고, G에 도달하는 것이 목표입니다. Cliff State의 Reward는 -100로, 나머지 모든 State에서의 Reward는 -1로 정의되어 있습니다. 만약 Agent가 Cliff나 G에 도달하면 Episode가 종료됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프는 $\epsilon = 0.1$으로 설정한 $\epsilon$-greedy Policy를 사용했을 때 Sarsa와 Q-learning의 성능을 비교한 그림입니다.  Q-learning은 Cliff의 가장자리를 따라 이동하는 Policy로 수렴하지만, 때때로 $\epsilon$-greedy로 인해 Cliff State에 진입하는 경우가 발생합니다. 반면에 Sarsa는 Cliff에 최대한 접근하지 않도록 안전한 경로로 학습하기 때문에, Cliff에 떨어지는 경우는 발생하지 않지만 최적의 경로보다 우회한 경로로 수렴하게 됩니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;expected-sarsa&quot;&gt;Expected Sarsa&lt;/h2&gt;

&lt;p&gt;이번 Section부터는 Sarsa와 Q-learning으로부터 파생된 방법을 하나씩 다룰 예정입니다. 먼저 Sarsa를 변형한 Expected Sarsa 방법이 있습니다. Expected Sarsa는 이름과 같이 (다음 State Q 값의) 평균을 사용한 방법입니다. Expected Sarsa의 업데이트 식은 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
Q(S_t, A_t) &amp;amp; \gets Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \mathbb{E}_{\pi} \left[ Q ( S_{t+1}, A_{t+1} ) | S_{t+1} \right] - Q(S_t, A_t) \right] \\ \\
&amp;amp;= Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_a \pi (a | S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \right] \tag{6.9}
\end{align}\]

&lt;p&gt;교재에서는 다음 State $S_{t+1}$이 주어졌을 때, Sarsa의 Expectation대로 이동하기 때문에 Expected Sarsa라는 이름이 붙었다고 합니다. 어차피 비슷한 의미이니 다음 State의 평균 Q 값을 사용하기 때문에 Expected Sarsa라는 이름이 붙었다고 이해해도 괜찮을 것 같습니다.&lt;/p&gt;

&lt;p&gt;Expected Sarsa는 Sarsa보다 계산이 더 복잡하지만, Sarsa에서 $A_{t+1}$을 무작위로 선택하기 때문에 발생하는 Variance가 없어진다는 장점이 있습니다. 이로 인해 동일한 학습량을 놓고 비교했을 때, Expected Sarsa는 Sarsa보다 약간 더 나은 성능을 보입니다. 아래의 그래프는 Cliff Walking 예제에서 Sarsa, Q-learning, Expected Sarsa 간의 성능을 비교한 그림입니다. Interim Performance에서 Expected Sarsa는 다른 두 방법보다 더 우수한 성능을 보일 뿐만 아니라, Asymptotic Performance에서 성능이 하락하는 Sarsa에 비해 성능 저하 없이 우수한 성능을 유지하는 모습을 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-10.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cliff Walking 예제에서는 Expected Sarsa가 On-policy로 구현되었지만, Q-learning처럼 Behavior Policy와 Target Policy를 분리하여 Off-policy로 구현할 수도 있습니다. 이 경우 Expected Sarsa는 Q-learning과 거의 동일해지며, 추가적인 계산량만 감당할 수 있다면 대부분의 TD 알고리즘보다 더 우수한 성능을 보인다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-11.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Q-learning와 Expected Sarsa의 Backup Diagram을 비교하면 위와 같습니다. 두 방법 모두 다음 State의 모든 Action을 고려하는 것은 같으나, Q-learning은 다음 State에서 가능한 Action 중 가장 Q 값이 높은 Action만을 찾아 학습에 반영하는 반면에, Expected Sarsa는 다음 State에서 가능한 Action을 모두 고려하여 Q 값의 평균을 계산한 다음 학습에 반영한다는 차이점이 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;maximization-bias-and-double-learning&quot;&gt;Maximization Bias and Double Learning&lt;/h2&gt;

&lt;p&gt;지금까지 배운 Control 방법들은 Target Policy를 최적화하는데 중점을 두었습니다. 만약 추정한 값보다 최대값이 큰 경우, 이 값은 최대값의 추정값으로 사용될 수 있으며, 이로 인해 상당한 Bias를 초래할 수 있습니다. 예를 들어, 실제 $q(s, a)$ 값은 모두 0이라고 가정한 상황에서, 추정값 $Q(s,a)$가 불확실하므로 0보다 클 수도 있고 0보다 작을 수도 있습니다. 이 경우 실제 $q(s, a)$ 값의 최대값은 0이지만, 추정값 $Q(s,a)$의 최대값은 양수입니다. 즉, 양의 방향으로 Bias되었다고 볼 수 있습니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Maximization Bias&lt;/span&gt;라고 합니다. 구체적인 예시를 통해 이것이 어떻게 발생하는지 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 6.7) Maximization Bias Example&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-12.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림은 Maximization Bias가 TD Control에서 어떻게 문제를 발생시키는지 알 수 있는 간단한 예제입니다. 예제의 MDP는 항상 State A에서 Episode가 시작됩니다. 왼쪽 끝, 또는 오른쪽 끝 State에 도달하면 Episode가 종료됩니다. State A에서 오른쪽으로 갈 때는 0의 Reward를 받지만, B에서 왼쪽으로 갈 때는 정규분포 $N(-0.1, 1)$에 따른 Reward를 받습니다. 왼쪽으로 갈 때의 평균 Reward는 -0.1이기 때문에, Reward가 0으로 고정된 오른쪽에 비해서 좋지 않은 방향임을 쉽게 알 수 있습니다. 문제는 왼쪽의 Reward가 정규분포를 따르기 때문에, 0보다 큰 Reward를 받을 확률이 있다는 것입니다. 그렇기 때문에 학습량이 적은 초기에는 왼쪽으로 가는 것을 더 좋은 Action으로 학습하게 되고, 그것이 위의 그래프에 나타나 있습니다. (Q-learning이 초기에 왼쪽을 선택학 확률이 매우 높음)&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;Maximization Bias를 피하기 위한 대표적인 방법으로 &lt;span style=&quot;color:red&quot;&gt;Double Q-learning&lt;/span&gt;이 있습니다. Agent를 2개로 나누어 각각 따로 학습하는 것입니다. 즉, Q 값을 2개로 분리하여 각각 $Q_1(a)$와 $Q_2(a)$로 나누는 것입니다. 두 개의 Q 값은 모두 $q(a)$의 추정치가 됩니다. 재밌는 것은 각각의 Agent가 Action을 선택할 때 상대의 추정값을 사용한다는 것입니다. 즉, $Q_2(A^{*}) = Q_2(\underset{a}{\operatorname{argmax}} Q_1(a))$가 되는 것입니다. 이렇게 추정값을 계산하게 되면 $\mathbb{E} \left[ Q_2 (A^{*}) \right] = q(A^{*})$라는 의미가 되어 Bias되지 않습니다. 마찬가지로 $Q_1$의 최적의 Action 선택 방식은 $Q_1(A^*) = Q_1(\underset{a}{\operatorname{argmax}} Q_2(a))$이 됩니다. 주의할 점은, 두 개의 추정을 따로 학습하지만, 각각의 Action마다 $Q_1$ 또는 $Q_2$ 중 하나의 추정치만 업데이트한다는 것입니다. 따라서 Double Q-learning은 공간 복잡도를 2배로 늘리지만, 시간 복잡도를 늘리지는 않습니다. 즉, 새로운 Action 샘플이 들어오면, 50% 확률로 $Q_1$을 학습하고, 50% 확률로 $Q_2$를 학습하는 것입니다. Double Q-learning의 업데이트 식은 다음과 같습니다.&lt;/p&gt;

\[Q_1(S_t, A_t) \gets Q_1(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q_2 (S_{t+1}, \underset{a}{\operatorname{argmax}} Q_1(S_{t+1}, a)) - Q_1(S_t, A_t) \right] \tag{6.10}\]

&lt;p&gt;식 (6.10)에서 $Q_2$를 학습할 때는 $Q_1$과 $Q_2$의 위치를 서로 바꾸면 됩니다. Double Q-learning에 대한 완전한 Pseudocode는 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-13.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Double Q-learning과 Q-learning에 대한 성능 비교는 예제 6.7의 그래프를 확인해보시면 됩니다. 기존 Q-learning과 비교했을 때, 우연히 발생한 양의 Reward 쪽으로 Bias되지 않는 것을 볼 수 있습니다. 여기서는 Double Q-learning만 소개했으나, Sarsa와 Expected Sarsa를 응용해서 구현할 수도 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;games-afterstates-and-other-special-cases&quot;&gt;Games, Afterstates, and Other Special Cases&lt;/h2&gt;

&lt;p&gt;1장에서 잠깐 다루었던 Tic-Tac-Toe 예제를 다시 떠올려봅시다. 그 당시에도 TD에 대해 잠깐 소개했었는데, 다시 그 부분을 확인해보시면 Tic-Tac-Toe에서는 Agent가 이동한 후 보드의 State를 평가했었습니다. 이렇게 Action을 먼저 한 후 State의 Value를 확인하는 것을 &lt;span style=&quot;color:red&quot;&gt;Afterstates&lt;/span&gt;라고 하고, 이 때의 Value Function를 &lt;span style=&quot;color:red&quot;&gt;Afterstate Value Function&lt;/span&gt;이라고 합니다. Afterstates는 Environment에 대한 초기 지식이 있지만, 전체 지식은 없고 일부분에 대한 지식만 있을 때 유용합니다. 예를 들어, 체스 같은 게임에서는 말을 움직였을 때 어떤 효과가 일어나는지는 바로 알 수 있지만, 상대방이 어떻게 행동할지는 모릅니다. Afterstates는 이런 종류의 지식을 활용하여 보다 효율적인 학습을 하는 방법이라고 이해하시면 되겠습니다.&lt;/p&gt;

&lt;p&gt;Afterstate가 더 효율적인 것을 보이기 위해 다시 Tic-Tac-Toe 예제를 사용해보겠습니다. 기존의 Action-Value Function는 현재 O/X가 체크된 State를 토대로 Value를 추정합니다. 하지만 다음 그림을 보시면 현재 State가 될 수 있는 이전 State는 한 가지가 아닐 수 있습니다. 그러나 두 State는 모두 같은 후속 State를 만들기 때문에 동일하게 평가해야 합니다. 기존의 Action-Value Function는 두 State를 개별적으로 평가하지만, Afterstate Value Function는 두 State를 동등하게 평가하는 차이점이 있습니다. 즉, 아래 그림에서 왼쪽에서 발생하는 모든 학습은, 오른쪽 State에도 즉시 적용됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/6. Temporal-Difference Learning/RL 06-14.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Afterstate는 게임에서만 사용되는 것이 아닙니다. 예를 들어, Queuing 문제에서는 Queue에 있는 작업을 서버에 할당하는지/거부할 것인지 등을 계속 판단해야 하는데, 동일한 작업에 대해서 동일하게 판단해야하므로 Afterstates를 적용할 수 있습니다. 이 외에도 Afterstates를 사용하는 여러 문제가 있지만, 여기서 모두 소개하기에는 불가능합니다. Afterstates는 동일한 방식으로 상호 작용하는 State 및 Policy에 대해서는 동일하게 추정해야하는 문제에 적용하는 것이라고 이해하시면 되겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서는 새로운 학습 방법인 &lt;strong&gt;Temporal Difference (TD)&lt;/strong&gt;를 소개하였습니다. 이전 장들과 마찬가지로 예측 문제와 제어 문제로 나누어 소개하였으며, TD 또한 &lt;strong&gt;Generalized Policy Iteration&lt;/strong&gt; 아이디어에서 기반하였습니다.&lt;/p&gt;

&lt;p&gt;TD의 Control 방법 또한 On-policy와 Off-policy 방법으로 나뉘며, On-policy TD는 &lt;strong&gt;Sarsa&lt;/strong&gt;, Off-policy TD는 &lt;strong&gt;Q-learning&lt;/strong&gt;이라는 대표적인 방법을 소개하였습니다. 그 외에 &lt;strong&gt;Expected Sarsa&lt;/strong&gt;나 &lt;strong&gt;Double Q-learning&lt;/strong&gt;등의 변형 버전도 소개하였습니다. 이번 장에서 소개하지 않은 방법으로 &lt;strong&gt;Actor-Critic&lt;/strong&gt;이 있는데, 이는 13장에서 자세히 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;TD는 지금까지 배운 강화학습 방법 중에 가장 널리 사용되는 방법입니다. Dynamic Programming이나 Monte Carlo Method 등과 비교해서 단순할 뿐만 아니라 계산량 또한 적기 때문입니다. 이 뒤에 이어지는 내용들은 대부분 TD를 심도 있게 확장하여 더 강력하게 만든 알고리즘을 소개할 예정입니다. 특히 Part II 부터는 TD와 기존의 전통적인 기계학습 방법들을 융합한 새로운 학습 방법들을 소개합니다.&lt;/p&gt;

&lt;p&gt;6장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장은 강화학습의 핵심 아이디어인 Temporal-Difference (TD) Learning을 다루게 됩니다. TD Learning은 Environment에 대한 정확한 Model 없이 경험을 통해 학습한다는 Monte Carlo의 아이디어와 Bootstrap 하지 않고 학습된 다른 추정치를 기반으로 추정치를 업데이트한다는 Dynamic Programming 아이디어를 결합하여 만들어졌습니다. 이번 장의 시작은 이전 장들과 같이 주어진 Policy $\pi$에 대한 Value Function $v_{\pi}$를 추정하는 문제로부터 시작하며, Optimal Policy을 찾는 Control 문제에서도 이전과 같이 GPI를 변형하여 접근합니다. 이전 장들과의 주요 차이점은 Prediction 문제에 대한 접근 방식입니다.</summary></entry><entry><title type="html">Monte Carlo Methods</title><link href="http://localhost:4000/studies/monte-carlo-methods/" rel="alternate" type="text/html" title="Monte Carlo Methods" /><published>2022-03-11T00:00:00+09:00</published><updated>2022-03-11T00:00:00+09:00</updated><id>http://localhost:4000/studies/monte-carlo-methods</id><content type="html" xml:base="http://localhost:4000/studies/monte-carlo-methods/">&lt;p&gt;이번 장에서는 지난 장과 마찬가지로 Value Function을 추정하고 Optimal Policy를 찾기 위한 방법을 다루지만, 지난 장과는 달리 MDP에 대한 완전한 정보를 알고 있다고 가정하지 않습니다. &lt;span style=&quot;color:red&quot;&gt;Monte Carlo Method&lt;/span&gt;는 Environment와의 상호 작용을 통해 얻은 경험을 기반으로 Optimal Policy를 찾는 방법입니다. 이 때 Environment와의 상호작용은 실제로 이루어지는 경험 뿐만이 아니라 시뮬레이션된 경험이라도 상관 없습니다.&lt;/p&gt;

&lt;p&gt;3장에서 MDP를 끝이 존재하는 Episodic Task와 끝이 없는 Continuing Task로 분류하였는데, 이번 장에서는 일단 Episodic Task 상황만 가정하도록 하겠습니다. Monte Carlo Method 또한 지난 장에서 배운 Generalized Policy Iteration (GPI)의 아이디어를 기반으로 하지만, 그 때와 달리 MDP에서 직접 Value Function을 계산하지 않고 Sample로부터 Value Function을 계산합니다. 물론 여전히 그 때처럼 Optimal Value Function에 수렴하기 위해 상호작용하는 것은 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-prediction&quot;&gt;Monte Carlo Prediction&lt;/h2&gt;

&lt;p&gt;가장 먼저 Policy가 주어졌을 때 State-Value Function을 학습하기 위해 Monte Carlo Method를 고려해보겠습니다. State의 Value는 해당 State에서 시작했을 때 얻을 수 있는 기대 수익이므로, 경험으로 이를 추정하기 위해서는 해당 State를 여러 번 방문한 후 얻은 수익의 평균을 구하면 됩니다. 당연히 많이 방문할수록 평균값이 정확한 State의 Value에 가까워집니다. 이것이 바로 Monte Carlo 방법의 기본 아이디어입니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method는 두 가지 방법으로 나눌 수 있습니다. 만약 Policy가 $\pi$로 주어지고 이 Policy를 따랐을 때 어떤 Epiosde에서 State $s$를 방문했다고 가정해봅시다. 그런데 한 Episode에서 State $s$를 꼭 한 번만 방문한다는 보장은 없습니다. 한 Episode에서 State $s$를 여러 번 방문했을 때 $v_{\pi}(s)$를 어떻게 구해야 할까요?&lt;/p&gt;

&lt;p&gt;Monte Carlo 방법에는 &lt;span style=&quot;color:red&quot;&gt;First-visit MC Method&lt;/span&gt;와 &lt;span style=&quot;color:red&quot;&gt;Every-visit MC Method&lt;/span&gt;가 있습니다. 두 방법 모두 State $s$를 방문한 후, 얻은 Reward의 평균을 추정하는 것은 같습니다. 하지만 First-visit MC Method는 가장 처음에 방문한 것만 계산에 사용하고, Every-visit MC Method는 방문한 모든 것을 계산에 사용한다는 차이가 있습니다. 두 방법 모두 State $s$를 무한히 방문했을 때 $v_{\pi}(s)$에 수렴한다는 것은 같습니다.&lt;/p&gt;

&lt;p&gt;이번 장에서는 First-visit MC Method를 위주로 살펴볼 예정이며, Every-visit MC Method는 9장과 12장에서 다시 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;First-visit MC Method의 Pseudocode는 다음과 같습니다. 이 알고리즘을 Every-visit MC Method로 수정하려면 Unless ~ 로 시작하는 조건문을 삭제하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 5.1) Blackjack&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Blackjack 게임은 카지노에서 자주 보이는 카드 게임 중 하나로써, 딜러와 플레이어가 승부하는 게임입니다. 플레이어가 가지고 있는 카드의 합이 딜러가 가지고 있는 카드의 합보다 높거나 플레이어 카드의 합이 21일 경우 무조건 승리하지만, 21을 초과할 경우에는 무조건 패배합니다. 이 때, 특수한 카드인 Jack (J), Queen (Q), King (K)은 10으로 취급합니다. Ace (A)는 플레이어의 선택에 따라 1로 취급할 수도 있고 11로 취급할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;게임은 플레이어와 딜러가 각각 무작위한 2장의 카드를 받고 시작합니다. 이 때 플레이어의 카드는 모두 오픈하지만, 딜러는 1개만 오픈하고 1개는 뒤집어 놓습니다.&lt;/p&gt;

&lt;p&gt;플레이어가 선택할 수 있는 Action은 &lt;strong&gt;Hit&lt;/strong&gt;과 &lt;strong&gt;Stand&lt;/strong&gt;입니다. Hit을 선택하면 무작위 카드를 한 장 더 받을 수 있고, Stand를 선택하면 카드를 더 받지 않습니다. 만약 Hit을 선택했을 때, 새로 받은 카드를 포함하여 합이 21을 초과할 경우 플레이어는 즉시 패배합니다. 이것을 &lt;strong&gt;Bust&lt;/strong&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;플레이어가 Stand를 선택하고 카드의 합이 21을 넘지 않는다면, 딜러가 게임을 시작합니다. 이 때 딜러는 자신의 의지대로 플레이할 수 없고, 카드의 합이 17보다 작으면 무조건 Hit을 해야하고, 그렇지 않으면 무조건 Stand를 해야합니다. 딜러가 Bust가 된다면 플레이어의 승리입니다.&lt;/p&gt;

&lt;p&gt;만약 플레이어와 딜러 두 명 모두 Bust가 아니라면, 그 때 카드의 합을 비교하여 높은 쪽이 승리합니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;Solution)&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;먼저 Blackjack 게임의 State와 Action, 그리고 Reward를 정의해봅시다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;초기에 플레이어가 받은 카드 2장의 합 : 최소 12 - 최대 21
딜러가 가지고 있는 카드 중 공개된 1장의 숫자 : 최소 1 (Ace) - 최대 10
내가 Ace를 가지고 있는지에 대한 여부 : 예 or 아니오&lt;/p&gt;

&lt;p&gt;따라서 State의 총 갯수는 10 * 10 * 2 = 200개입니다.&lt;/p&gt;

&lt;p&gt;State는 (플레이어 카드의 합, 딜러의 카드, Ace 보유 여부) 로 표기합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;플레이어의 Action만 고려하면 되므로 Hit 또는 Stand로 2개입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reward&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;플레이어가 게임에서 승리하면 +1, 패배하면, -1, 무승부일시 0으로 정의합니다.&lt;/p&gt;

&lt;p&gt;다음으로 Policy를 세팅해보겠습니다. 교재에서는 현재 플레이어가 가지고 있는 카드의 합이 20, 21이면 Stand를 하고, 그 이외는 Hit을 선택하도록 Policy를 정의하였습니다. 이것을 가지고 Python을 사용하여 구현하면 다음과 같습니다.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gym&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Blackjack-v1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_timesteps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_episode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500000&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generate_episode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;state&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;total_return&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;state&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;N&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;state&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;value&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;total_return&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;N&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;위의 코드는 First-visit MC Method로 Blackjack 게임에서 각 State의 Value를 추정한 프로그램입니다. (프로그램을 실행하기 위해서는 gym과 pandas 라이브러리를 설치하셔야 합니다) 만약 이 프로그램을 Every-visit MC Method로 바꾸고 싶다면, 32번째 Line의 if 문을 삭제하시면 됩니다.&lt;/p&gt;

&lt;p&gt;프로그램을 실행하게 되면 많이 방문한 State 순으로 10개를 보여주고, 얻은 총 Return과 방문 횟수, 그리고 추정한 State의 Value를 출력합니다. 이 프로그램은 단순히 State의 Value만 추정하는 것이기 때문에 Policy를 변경하지는 않습니다. 이러한 방법으로 1만개와 50만개의 Episode를 경험한 후, State의 Value를 도식화하면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과를 보면 State의 Value가 높은 경우는 카드의 합이 20과 21일 경우밖에 없습니다. 어떻게 보면 당연한게, 저희가 설정한 Policy는 20과 21에서만 Stand를 하기 때문입니다. Blackjack 게임에 대해서 잘 아는 것은 아니지만 18이나 19에서 Hit을 한다면 Bust할 확률이 높기 때문에 승률이 낮을 수밖에 없습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;예제로 보여드린 Blackjack 게임은 Environment에 대해 완전한 정보를 가지고 있습니다. 하지만 지난 장에서 배운 DP 방법으로 Blackjack 게임의 Value Function을 계산하는 것은 쉽지 않습니다. 가장 어려운 점은 Transition Probability를 계산하는 것입니다. 예를 들어, 현재 플레이어가 가지고 있는 카드의 합이 14이고 플레이어가 Stand를 선택했다고 가정했을 때, 플레이어가 이길 확률을 계산하면 얼마일까요? 이것부터가 쉽지 않은데 모든 State에 대해 이것을 계산해야 한다고 생각하면 막막할 따름입니다. 그렇기 때문에 Environment에 대한 지식을 알고있는지 여부에 상관없이 Monte Carlo Method는 DP보다 간단할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method에서 중요한 점은 각 State에 대해 추정한 Value가 독립적이라는 것입니다. 이것은 어떤 State에 대해 추정한 Value가 다른 State에 대해 추정한 Value에 영향을 끼치지 않는다는 뜻이므로, Bootstrap하지 않는다고 해석할 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 Monte Carlo Method는 Episode에 기반하기 때문에, 특정(단일) State의 Value를 추정할 때의 계산 비용은 State의 수가 매우 많더라도 간단하게 계산할 수도 있습니다. 지난 장에서 DP를 사용할 때는 State의 Value를 추정하기 위해서 모든 State를 고려한 것을 생각해보면 확실히 낫다고 볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-estimation-of-action-values&quot;&gt;Monte Carlo Estimation of Action Values&lt;/h2&gt;

&lt;p&gt;Model을 사용할 수 없는 경우, 즉 Environment에 대한 완전한 정보가 없는 경우에는 State-Value를 사용하는 것 보다 Action-Value를 사용하는 것이 더 좋습니다. Value를 기반으로 Policy를 만들 때는 각 Action에 대한 Value를 명시적으로 추정해야하기 때문입니다. 따라서 Monte Carlo Method의 주요 목표 중 하나는 $q_{*}$를 추정하는 것입니다. 그 목표를 이루기 위해서는 먼저 Action-Value에 대한 Policy Evaluation을 고려해야 합니다.&lt;/p&gt;

&lt;p&gt;Action-Value에 대한 Policy Evaluation은 State $s$에서 시작하여 Action $a$를 선택한 후, Policy $\pi$를 따를 때의 기대 Reward인 $q_{\pi} (s, a)$를 추정하는 것입니다. 추정 방법은 State-Value를 추정하는 방법과 거의 유사합니다.&lt;/p&gt;

&lt;p&gt;Action-Value에 대한 Policy Evaluation을 할 때 유일한 문제는 많은 State-Action 쌍이 방문되지 않을 수 있다는 것입니다. 특히 $\pi$가 Deterministic Policy인 경우, 각 State에 대해서는 하나의 Action만 선택하기 때문에 Policy가 선택하는 Action 외에는 모두 방문하지 않기 때문입니다. 2장에서 언급했듯이 Policy에 대한 평가가 제대로 작동하기 위해서는 지속적인 탐색이 보장되어야 합니다.&lt;/p&gt;

&lt;p&gt;이 문제를 해결하기 위해서는 모든 State-Action 쌍이 선택될 확률을 0보다 크게 만드는 것입니다. 아무리 작은 확률이라도 무한한 샘플을 얻게 되면 모든 State-Action 쌍을 많이 방문하게 되므로 제대로 Policy Evaluation을 할 수 있습니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Assumption of Exploring Starts&lt;/span&gt;라고 부릅니다.&lt;/p&gt;

&lt;p&gt;Assumption of Exploring Starts는 때때로 유용하지만, Environment와 실제 상호 작용을 하며 직접 학습할 때는 일반적으로 신뢰할 수 없습니다. 모든 State-Action 쌍을 방문할 수 있게 만들기 위해서는 차라리 Stochastic Policy를 고려하는 것이 낫습니다. 이번 장 뒷부분에서는 그렇게 접근하는 2가지 방법에 대해 설명하겠지만, 일단 지금은 Assumption of Exploring Starts를 유지하면서 Monte Carlo Control 방법을 소개하도록 하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-control&quot;&gt;Monte Carlo Control&lt;/h2&gt;

&lt;p&gt;Monte Carlo Prediction의 기본 아이디어는 지난 장에서 배웠던 &lt;strong&gt;Generalized Policy Iteration (GPI)&lt;/strong&gt;과 동일합니다. 교재에서는 Monte Carlo Method가 DP와 달리 Environment에 대한 지식 없이 샘플 Episode 만으로 Optimal Policy와 Value Function을 찾는 데 사용할 수 있다는 것을 보여주지만, 굳이 이 부분까지 설명할 필요는 없을 것 같아서 생략하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method로 수렴한다는 보장을 얻기 위해서 저희는 2가지 가정을 했습니다. 하나는 Assumption of Exploring Starts이고, 다른 하나는 무한한 수의 Episode가 있다는 가정입니다. 하지만 두 가정 모두 현실적이지 않기 때문에 Monte Carlo 알고리즘을 만들기 위해서는 이 두 가정을 모두 제거할 필요가 있습니다. 첫 번째 가정은 나중에 고려하도록 하고, 우선 무한한 수의 Episode에 대해서만 생각해보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;첫 번째로 생각할 수 있는 해결 방법은 각각의 Policy Evaluation이 $q_{\pi_k}$를 근사화한다는 아이디어를 유지하는 것입니다. 즉, 어느 정도 근사치까지 정확한 수렴을 보장한다는 의미이지만, 문제의 크기가 조금만 커져도 실제와 가깝게 근사화하기 위해서는 너무 많은 Episode가 필요하다는 문제가 있습니다.&lt;/p&gt;

&lt;p&gt;두 번째로 생각할 수 있는 해결 방법은 Policy Improvement를 하기 전에 Policy Evaluation을 끝내지 않는 것입니다. Policy Evaluation은 매 단계마다 Value Function을 $q_{\pi_k}$에 가까워지게 하는 시도를 하지만, 어차피 후반 몇몇 단계를 제외하고는 실제로 그렇게 가깝지 않습니다. 지난 장에서 GPI를 처음 소개할 때 잠깐 소개했었는데, 극단적으로는 Policy Evaluation을 한 번만 수행하는 것도 가능하다고 했었습니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Method에서의 Policy Iteration은 각 Episode별로 Evaluation과 Improvement를 번갈아 수행합니다. 각 Episode에서 발생한 Reward는 Policy Evaluation에 사용되며, Episode에서 방문한 모든 State에서 Policy가 Improve 됩니다. 이것을 &lt;span style=&quot;color:red&quot;&gt;Monte Carlo with Exploring Starts&lt;/span&gt;라고 하며 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Monte Carlo with Exploring Starts는 초기 Policy에 상관없이 모든 State-Action 쌍에 대한 평균적인 Reward를 얻을 수 있습니다. 이전에 Monte Carlo Prediction에서는 Policy에 대한 평가만 했다면, Monte Carlo with Exploring Starts는 Policy를 업데이트하는 기능이 추가되며, Optimal Policy로 수렴할 것이라고 생각됩니다. &lt;strong&gt;생각됩니다&lt;/strong&gt; 라고 쓰는 이유는 아직 공식적으로 증명되지 않았기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 5.3) Solving Blackjack&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Example 5.1에서 다루었던 Blackjack 게임을 다시 언급해보겠습니다. Example 5.1에서는 Policy에 대한 Value만을 계산했지만, Monte Carlo with Exploring Starts을 적용하게 되면 Optimal Policy를 구할 수 있습니다. 이를 도식화하면 아래 그림과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;monte-carlo-control-without-exploring-starts&quot;&gt;Monte Carlo Control &lt;em&gt;without&lt;/em&gt; Exploring Starts&lt;/h2&gt;

&lt;p&gt;이번에는 Assumption of Exploring Starts를 제거해보도록 하겠습니다. Assumption of Exploring Starts 없이 모든 Action이 선택될 수 있게 하는 유일한 방법은 Agent로 하여금 모든 State를 골고루 선택하게 설계하는 것입니다. 이를 보장하는 방법이 2가지 있는데, 하나는 On-policy 방법이고 다른 하나는 Off-policy 방법입니다. On-policy 방법은 Agent가 실제로 사용한 Policy를 Evaluation하거나 Improvement하는 방법이고, Off-policy 방법은 실제로 사용한 Policy와 다른 Policy를 Evaluation하거나 Improvement하는 방법입니다. 직전에 다룬 Monte Carlo with Exploring Starts는 On-policy 방법의 한 종류라고 볼 수 있습니다. 먼저 On-policy에 대해 알아본 다음, Off-policy 방법은 다음 섹션에서 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;On-policy 방법은 모든 State $s \in \mathcal{S}$와 모든 Action $a \in \mathcal{A} (s)$에 대해 $\pi (a \mid s) &amp;gt; 0$으로 시작하지만, 점차적으로 (Optimal) Deterministic Policy에 가까워집니다. 이것을 유도하는 방법은 여러 개가 있지만, 우선 가장 많이 쓰이는 &lt;span style=&quot;color:red&quot;&gt;$\epsilon$-Greedy Policy&lt;/span&gt;를 먼저 소개하도록 하겠습니다. $\epsilon$-Greedy Policy는 일반적으로 Value가 가장 높은 Action을 선택하지만, 낮은 확률로 무작위 Action을 선택하는 방법입니다. 조금 더 구체적으로 설명하자면 $\epsilon &amp;gt; 0$의 확률로 무작위 Action을 선택하고 $1 - \epsilon$의 확률로 Value가 가장 높은 Action을 선택하는 것입니다.&lt;/p&gt;

&lt;p&gt;$\epsilon$-Greedy Policy에서 $\epsilon$의 값은 고정된 값이 아닙니다. Value가 제대로 추정되지 않은 초기에는 $\epsilon$을 크게 만들어서 여러 Action을 선택하게 유도하고, 어느 정도 추정이 끝난 다음에는 $\epsilon$를 작게 만들어 최선의 선택을 하도록 유도하는 것입니다.&lt;/p&gt;

&lt;p&gt;On-policy Monte Carlo Control에서도  현재 Policy에 대한 Action-Value Function을 추정하기 위해 First-visit MC Method를 사용합니다. Monte Carlo with Exploring Starts와 달리 Assumption of Exploring Starts를 하지 않고, 대신 $\epsilon$-Greedy Policy를 사용합니다. $\epsilon$-Greedy와 같은 Policy를 &lt;span style=&quot;color:red&quot;&gt;$\epsilon$-soft&lt;/span&gt;라고도 부릅니다. &lt;span style=&quot;color:red&quot;&gt;On-policy Monte Carlo Control&lt;/span&gt;의 Pseudocode는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\epsilon$-Greedy Policy로부터 $q_{\pi}$를 개선하는 것은 Policy Improvement Theorem에 의해 보장됩니다. 이에 대한 유도는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
q_{\pi} (s, \pi&apos; (s)) &amp;amp;= \sum_a \pi&apos; (a | s) q_{\pi} (s, a) = \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) + (1 - \epsilon) \max_a q_{\pi} (s, a) \tag{5.2} \\ \\
&amp;amp;\ge \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) + (1 - \epsilon) \sum_a \frac{\pi (a | s) - \frac{\epsilon}{|\mathcal{A}(s)|}}{1-\epsilon} \\ \\
&amp;amp;= \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) - \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) + \sum_a \pi (a | s) q_{\pi} (s, a) = v_{\pi} (s)
\end{align}\]

&lt;p&gt;위의 전개식은 Policy Improvement Theorem에 의해 $\pi^{\prime} \ge \pi$임을 보여주고 있습니다. 두 번째로 증명해야할 부분은 두 Policy $\pi^{\prime}$와 $\pi$가 최적의  $\epsilon$-soft Policy일 때, 다른 $\epsilon$-soft Policy보다 좋거나 같아야 한다는 것입니다.&lt;/p&gt;

&lt;p&gt;이것을 증명하기 위해 먼저 Policy가 $\epsilon$-soft 로 이동되어야 한다는 조건을 제외하고 원래 Environment과 동일한 새 Environment를 가정해보겠습니다. 새 Environment에서는 원래 Environment와 동일한 State와 Action 집합이 설정되어 있습니다. 새 Environment에서는 State $s$에서 Action $a$를 선택했을 때 $1 - \epsilon$ 확률로 원래 Environment와 동일하게 동작하며, $\epsilon$ 확률로 무작위 Action을 선택한 다음 새로운 Action으로 이전 Environment처럼 동작합니다. 이런 새로운 Environment에서 할 수 있는 최선의 Policy는 원래 Environment에서 할 수 있는 최선의 Policy와 동일합니다. $\tilde{v}_{*}$와 $\tilde{q}_{*}$를 새 Environment에서 Optimal Value Function라고 정의하면, Policy $\pi$는 $v_{\pi} = \tilde{v}_{*}$인 경우에만 $\epsilon$-soft Policy 중 최선의 Policy입니다. 먼저 Transition Probability가 변경되었을 때 $\tilde{v}_{*}$는 Bellman Optimality Equation (식 3.19)에 대한 고유한 해법임을 다음을 통해 알 수 있습니다.&lt;/p&gt;

\[\begin{align}
\tilde{v}_* &amp;amp;= \max_a \sum_{s&apos;, r} \left[ (1 - \epsilon) p (s&apos;, r | s, a) + \sum_{a&apos;} \frac{\epsilon}{|\mathcal{A}(s)|} p (s&apos;, r | s, a&apos;) \right] \left[ r + \gamma \tilde{v}_* (s&apos;) \right] \\ \\
&amp;amp;= (1 - \epsilon) \max_a \sum_{s&apos;, r} p(s&apos;, r | s, a) \left[ r + \gamma \tilde{v}_*(s&apos;) \right] + \frac{\epsilon}{|\mathcal{A(s)}|} \sum_a \sum_{s&apos;, r} p (s&apos;, r |s, a) \left[ r + \gamma \tilde{v}_*(s&apos;) \right]
\end{align}\]

&lt;p&gt;다음으로 $\epsilon$-soft Policy $\pi$가 더 이상 개선되지 않고, $v_{\pi} = \tilde{v}_*$가 성립하면 식 (5.2)에 의하여 다음이 성립합니다.&lt;/p&gt;

\[\begin{align}
v_{\pi} (s) &amp;amp;= (1 - \epsilon) \max_a q_{\pi} (s, a) + \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a q_{\pi} (s, a) \\ \\
&amp;amp;= (1 - \epsilon) \max_a \sum_{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma v_{\pi} (s&apos;) \right] + \frac{\epsilon}{|\mathcal{A}(s)|} \sum_a \sum_{s&apos;, r} p (s&apos;, r |s, a) \left[ r + \gamma v_{\pi} (s&apos;) \right]
\end{align}\]

&lt;p&gt;그러나 이 방정식은 $\tilde{v}_{*}$를 $v_{\pi}$로 대입한 것을 제외하고는 앞의 방정식과 동일합니다. 여기서 $\tilde{v}_{*}$는 유일한 해법이므로 $v_{\pi} = \tilde{v}_{*}$일 수밖에 없습니다.&lt;/p&gt;

&lt;p&gt;결과적으로 $\epsilon$-soft Policy에서 Policy Iteration은 정상적으로 작동한다는 것을 알 수 있습니다. 또한 이제는 Assumption of Exploring Starts 없이 $\epsilon$-soft Policy에 대한 Improvement만으로 Optimal Policy를 찾을 수 있다는 것을 보장할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;off-policy-prediction-via-importance-sampling&quot;&gt;Off-policy Prediction via Importance Sampling&lt;/h2&gt;

&lt;p&gt;이전 섹션에서 배운 On-policy 방법의 가장 큰 단점은 Policy를 평가하기 위해서 실제로 경험해봐야 한다는 것입니다. 이 말은 최적의 Action을 찾기 위해 그렇지 않은 Action을 반드시 해봐야한다는 의미입니다. 이것을 해결하는 방법은 Target Policy와 Behavior Policy를 분리하는 것입니다. 이렇게 두 Policy를 분리하여 학습하는 방법을 &lt;span style=&quot;color:red&quot;&gt;Off-policy&lt;/span&gt; 방법이라고 합니다. 이렇게 이름을 붙이는 이유는 Target Policy에 Off 된다는 의미이기 때문이라고 합니다.&lt;/p&gt;

&lt;p&gt;Off-policy 방법은 On-policy 방법에 비해 일반적으로 많이 사용합니다. 특히 인간 전문가로부터 생성되는 데이터나, 비학습적인 컨트롤러로부터 생성되는 데이터를 학습하는데 우수한 성능을 가지고 있습니다. 그러나 Off-policy 방법은 On-policy 방법에 비해 추가적인 개념이 필요하기 때문에 더 복잡하고, 학습 데이터가 다르게 주어지기 때문에 분산이 더 크며, 수렴 속도 또한 더 느리다는 단점이 있습니다. 그렇기 때문에 On-policy 방법과 Off-policy 방법은 서로 우열을 가리기 어려우며, 두 방법 모두 상황에 따라 잘 사용되고 있습니다.&lt;/p&gt;

&lt;p&gt;이번 섹션에서 다룰 Off-policy 방법에서 Target Policy는 $\pi$, Behavior Policy는 $b$로 표기하며, 두 Policy 모두 고정되어 주어진 것으로 가정하겠습니다.&lt;/p&gt;

&lt;p&gt;Policy $b$로부터 생성된 Episode를 사용하여 Policy $\pi$를 추정하기 위해서는 $\pi$에서 수행한 모든 Action이 적어도 가끔은 $b$에서도 수행되어야 합니다. 즉, 여기에는 $\pi (a \mid s) &amp;gt; 0$일 경우 반드시 $b(a \mid s) &amp;gt; 0$라는 것을 의미합니다. (이것을 &lt;span style=&quot;color:red&quot;&gt;Assumption of Coverage&lt;/span&gt;라고 합니다) 또한 Policy $\pi$는 Deterministic일 수 있으나, Policy $b$는 Stochastic이어야 합니다.&lt;/p&gt;

&lt;p&gt;Off-policy 방법에서 또 한가지 중요한 것은 바로 &lt;span style=&quot;color:red&quot;&gt;Importance Sampling&lt;/span&gt;입니다. 처음 강화학습을 공부했을 때 Importance Sampling에 대한 개념이 난해해서 이해하기 힘들었는데, 좋은 포스팅을 발견하여 여기에 잠깐 소개하도록 하겠습니다. &lt;a href=&quot;https://blog.naver.com/kwonpub/221143316307&quot;&gt;(원본 포스트 링크)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example of Importance Sampling)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;어느 섬에 키다리족과 난장이족이 있습니다. 문제를 쉽게 하기 위해 이 섬의 사람은 키가 160cm 아니면 180cm 둘 중 하나라고 가정하도록 하겠습니다. 오랜 시간에 걸쳐 키다리족과 난장이족의 인구 분포를 조사한 결과, 다음과 같은 키의 분포가 나왔습니다. 이 때, 확률 변수 X를 키다리족, 확률 변수 Y를 난장이족이라고 정의하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 이번에는 두 부족의 평균 키를 구하려고 합니다. 먼저 키다리족의 사람 중 무작위로 10명의 표본을 뽑았더니, 다음과 같은 키의 분포가 나왔습니다.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;160 160 160 160&lt;/span&gt; &lt;span style=&quot;color:blue&quot;&gt;180 180 180 180 180 180&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이 분포를 토대로 표본집단의 평균을 구하는 것은 간단합니다. 표본집단의 평균은 모집단의 평균과 유사하니 완벽하게 일치하지는 않겠지만 모집단의 평균을 대략적으로 유추할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번에는 난장이족의 평균 키를 구하려고 합니다. 난장이족도 마찬가지로 무작위 표본을 뽑아 평균을 구하면 간단하지만, 모종의 문제로 난장이족의 표본을 구할 수 없다고 가정해봅시다. 대신 우리는 키다리족의 표본을 구했었고, 난장이족과 키다리족의 키의 분포를 알고 있습니다.&lt;/p&gt;

&lt;p&gt;이 키다리족의 표본을 토대로 난장이족의 키의 표본을 생성해봅시다. 먼저, 키가 160cm인 사람의 비율은 난장이족이 키다리족보다 2배가 많습니다. 그렇기 때문에 키가 160cm인 사람의 비율을 2배로 늘립니다.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;160 160 160 160 160 160 160 160&lt;/span&gt; &lt;span style=&quot;color:blue&quot;&gt;180 180 180 180 180 180&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;다음으로 키가 180cm인 사람의 비율을 보면 키다리족이 난장이족보다 2배가 많습니다. 그래서 키가 180cm인 사람의 비율을 절반으로 줄입니다.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;160 160 160 160 160 160 160 160&lt;/span&gt; &lt;span style=&quot;color:blue&quot;&gt;180 180 180&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;이렇게 구한 표본을 토대로 평균을 구한다면, 이것은 난장이족의 평균 키라고 말할 수 있습니다. 이것이 바로 Importance Sampling의 한 예입니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;이번에는 강화학습에서의 Importance Sampling이 무엇인지 알아보겠습니다. 위의 예제에서 키다리족의 표본으로 난장이족의 데이터를 추정했듯이, Off-policy에서도 Behavior Policy $b$로부터 생성되는 데이터를 토대로 Target Policy $\pi$를 추정해야 합니다. 예제에서는 키다리족과 난장이족의 키의 분포가 미리 주어졌으나, 여기에서는 문제마다 다르기 때문에 직접 계산해야합니다. 이렇게 두 데이터 사이의 비율을 &lt;span style=&quot;color:red&quot;&gt;Importance Sampling Ratio&lt;/span&gt;라고 합니다. 이것을 수식으로 표현하기 위해, 먼저 State $S_t$에서 시작하여 State-Action Trajactory를 얻을 확률을 구하도록 하겠습니다.&lt;/p&gt;

\[\begin{align}
&amp;amp;Pr \left\{ A_t, S_{t+1}, A_{t+1}, ... , S_T | S_t, A_{t:T-1} ~ \pi \right\} \\ \\
&amp;amp;= \pi (A_t | S_t ) p (S_{t+1} | S_t, A_t) \pi (A_{t+1} | S_{t+1}) \cdots p (S_T | S_{T-1}, A_{T-1}) \\ \\
&amp;amp;= \prod_{k=t}^{T-1} \pi (A_k | S_k) p ( S_{k+1} | S_k, A_k)
\end{align}\]

&lt;p&gt;위 식에서 $p$는 식 (3.4)에서 정의한 &lt;strong&gt;State Transition Probability&lt;/strong&gt;입니다. 위 식을 토대로 Target Policy와 Behavior Policy 간의 Importance Sampling Ratio를 구하면 다음과 같습니다.&lt;/p&gt;

\[\rho_{t:T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi (A_k | S_k) p ( S_{k+1} | S_k, A_k)}{\prod_{k=t}^{T-1} b (A_k | S_k) p ( S_{k+1} | S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi (A_k | S_k)}{b (A_k | S_k)} \tag{5.3}\]

&lt;p&gt;$p$는 MDP에서 주어지지 않는 한 구하기 가장 어려운 데이터지만, 운이 좋게도 분자와 분모 모두 있기 때문에 약분이 가능합니다. 즉, 두 Policy $\pi$와 $b$의 Importance Sampling Ratio는 두 Policy가 각 State에서 특정 Action을 선택할 확률이 얼마인지만 알면 구할 수 있습니다. 즉, Importance Sampling Ratio는 MDP에 독립적입니다.&lt;/p&gt;

&lt;p&gt;Target Policy에서의 기대 Reward를 추정할 때 주의할 점은, Behavior Policy로 인한 Return인 $G_t$만 주어졌다는 것입니다. 이것으로는 $\mathbb{E} \left[ G_t \mid S_t = s \right] = v_b (s)$을 구할 수 있기 때문에 $v_{\pi}$를 얻을 수 없습니다. 그렇기 때문에 Importance Sampling이 필요한 것입니다. Importance Sampling Ratio $\rho_{t:T-1}$를 추가하면 원하는 $v_{\pi}$를 얻을 수 있습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \rho_{t:T-1} G_t | S_t = s \right] = v_{\pi} (s) \tag{5.4}\]

&lt;p&gt;이로써 Behavior Policy $b$로 생성된 Episode를 토대로 $v_{\pi} (s)$를 추정하는 Monte Carlo 알고리즘을 만들 준비가 되었습니다. 하나 추가할 점은 앞으로 각 Episode에서의 시간 단계는 이어지게 표현하겠습니다. 예를 들어, 첫 번째 Episode가 $t = 100$에서 종료되었다면, 다음 Episode는 $t = 101$에서 시작하는 방식입니다. 이를 통해 특정 Episode의 특정 단계를 참조하기 위해 시간 단계를 사용할 수 있고, $\mathcal{T}(s)$라는 표기로 State $s$를 방문하는 모든 시간 단계 집합을 정의할 수 있습니다. 이 표기가 유용한 이유는 Every-visit MC 알고리즘을 사용할 때 표현이 간단해지기 때문입니다. 또한 $T(s)$라는 표기를 통해 시간 단계 $t$ 이후 처음으로 Episode가 종료되는 시점을 나타나게 하겠습니다. 이렇게 되면 $G_t$가 시간 단계 $t$ 이후에서 $T(s)$까지의 Return을 의미하게 됩니다. 또한 $\{ G_t \}_{t \in \mathcal{T}(s)}$는 State $s$에 대한 Return을 의미하고, $\{ \rho_{t:T(t)-1} \}_{t \in \mathcal{T}(s)}$는 Importance Sampling Ratio를 의미합니다. 여러 표기가 갑자기 나와서 헷갈리실 수 있는데, 어쨌든 중요한 것은 이게 다 $v_{\pi} (s)$를 간단하게 표현하기 위함입니다. 방금 새로 정의한 표기를 사용하여 $V(s)$를 표현하면 다음과 같습니다.&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_t}{|\mathcal{T}(s)|} \tag{5.5}\]

&lt;p&gt;식 (5.5)와 같이 Importance Sampling을 단순한 평균으로 계산하는 것을 &lt;span style=&quot;color:red&quot;&gt;Ordinary Importance Sampling&lt;/span&gt;이라고 합니다. 단순한 평균 대신 Weighted Average를 사용할 수도 있는데, 이 때는 &lt;span style=&quot;color:red&quot;&gt;Weighted Importance Sampling&lt;/span&gt;이라고 하며, $V(s)$는 다음과 같이 정의됩니다.&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1} G_t}{\sum_{t \in \mathcal{T}(s)} \rho_{t:T(t)-1}} \tag{5.6}\]

&lt;p&gt;식 (5.6)에서 만약 분모가 0인 경우에는 $V(s)$를 그냥 0으로 정의합니다.&lt;/p&gt;

&lt;p&gt;이 두 가지 Importance Sampling를 비교하기 위해 State $s$에서 단 하나의 Return을 관찰한 후 First-visit MC Method를 사용한다고 가정해보겠습니다. Weighted Importance Sampling에서는 ($\Sigma$ 기호가 없어지기 때문에) $\rho_{t:T(t)-1}$가 약분되므로 $V(s)$가 Return $G_t$와 동일해집니다. 단 하나의 Return만 관찰되었기 때문에 이것이 합리적으로 보일 수 있지만, 이것은 $v_b (s)$의 기대값이기 때문에 통계적으로 Bias되었다고 볼 수 있습니다. 반대로 Ordinary Importance Sampling에서는 단 하나의 Return이더라도 $v_{\pi} (s)$의 기대값이기 때문에 Bias되지는 않지만, 값 자체는 극단적일 수 있습니다. 예를 들어 Importance Sampling Ratio가 10이라면 관찰된 Trajactory가 Behavior Policy보다 Target Policy에서 10배 더 가치가 있다고 판단되고, 해당 Episode Trajactory가 Policy를 훌륭하게 대표한다고 가정해도 실제로 관측되는 Reward와는 크게 차이나는 문제가 있습니다.&lt;/p&gt;

&lt;p&gt;따라서 First-visit MC Method에서 두 가지 Importance Sampling에 대한 차이는 Bias와 Variance의 차이라고 볼 수 있습니다. Ordinary Importance Sampling은 Bias되지 않지만 Variance이 무한하게 커질 수 있으며, Weighted Importance Sampling은 극단적인 경우에도 Variance가 1이지만(위의 예제) Bias라는 문제가 있습니다. 두 방법 모두 장단점이 있지만, 일반적으로는 Bounded Return 환경에서 Variance가 0으로 수렴하는 Weighted Importance Sampling을 더 선호하는 편입니다.&lt;/p&gt;

&lt;p&gt;Every-visit MC Method에서는 Ordinary Importance Sampling과 Weighted Importance Sampling 모두 Bias되지만 Sample의 수가 증가할수록 Bias가 0에 가까워집니다. 그렇기 때문에 Off-policy에서는 Every-visit MC Method가 선호됩니다. Weighted Importance Sampling을 사용한 Off-policy Every-visit MC 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-07.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;incremental-implementation&quot;&gt;Incremental Implementation&lt;/h2&gt;

&lt;p&gt;이번 Section의 이름은 2.4와 동일합니다. 그 때와 마찬가지로 On-policy Monte Carlo Method를 Recurrence Relation으로 표현하는 방법에 대해 알아보겠습니다. Off-policy Monte Carlo Method는 Importance Sampling을 어떻게 구현하는지에 따라 달라집니다. Ordinary Importance Sampling에서의 Return은 식 (5.3)처럼 $\rho_{t:T(t)-1}$의 비율로 조정되고 식 (5.5)처럼 평균화됩니다. 이렇게하면 Section 2.4에서 사용했던 방법을 그대로 사용할 수 있지만, Weighted Importance Sampling일 때 사용하기 위해서는 약간 다른 알고리즘이 필요합니다.&lt;/p&gt;

&lt;p&gt;Return의 Sequence $G_1, G_2, \ldots, G_{n-1}$이 있다고 가정해봅시다. 이 Return들은 모두 동일한 State에서 시작하고 각각 해당하는 무작위 Weight $W_i$를 가집니다. 그렇다면 이것의 Value Function을 다음과 같이 추정할 수 있습니다.&lt;/p&gt;

\[V_n \doteq \frac{\sum_{k=1}^{n-1} W_k G_k}{\sum_{k=1}^{n-1} W_k}, \quad n \ge 2 \tag{5.7}\]

&lt;p&gt;식 (5.7)을 그대로 사용하면 Return $G_n$을 얻을 때마다 $V_n$을 다시 계산해야하는 불편함이 있습니다. Weighted Expectation을 구하기 위해선 Section 2.4에서 다루었던 Recurrence Relation을 약간 변경하여, Cumulative Sum $C_n$를 추가합니다. 먼저 $V_n$의 Recurrence Relation은 다음과 같습니다.&lt;/p&gt;

\[V_{n+1} \doteq V_n + \frac{W_n}{C_n} \left[ G_n - V_n \right] , \quad n \ge 1, \tag{5.8}\]

&lt;p&gt;그리고 $C_n$의 Recurrence Relation은 다음과 같습니다.&lt;/p&gt;

\[C_{n+1} \doteq C_n + W_{n+1}\]

&lt;p&gt;이 때, $C_0 \doteq 0$으로 정의합니다. 이것을 반영한 Off-policy Monte Carlo Prediction 알고리즘은 다음과 같습니다. 아래 알고리즘은 Weighted Importance Sampling으로 구현되었습니다. 만약 On-policy로 바꾸고 싶다면, Target Policy와 Behavior Policy가 같다는 뜻이므로 ($\pi = b$), $W = 1$로 설정하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;off-policy-monte-carlo-control&quot;&gt;Off-policy Monte Carlo Control&lt;/h2&gt;

&lt;p&gt;이제 Off-policy Monte Carlo Prediction을 완성했으니, Control 방법을 제시할 준비가 되었습니다. On-policy에서는 Control를 위해 Policy의 Value를 추정하면서 Policy를 사용했으나, Off-policy는 두 기능이 분리된다는 차이점이 있습니다. 그렇기 때문에 Behavior Policy는 Action을 선택하는데만 사용하고, Target Policy는 Policy를 Evaluation 및 Improvement하는데 이 둘이 전혀 연관이 없을 수 있습니다. 이렇게 구분하게 되면 각 Action의 Value에 상관 없이 모든 Action을 계속 Sampling할 수 있다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;Off-policy Monte Carlo Control 방법은 앞에 두 Section에서 제안한 기법 중 하나를 사용합니다. 이 때, Behavior Policy는 모든 State의 모든 Action에 대해서 선택할 확률이 0보다 커야합니다. (=Soft)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/5. Monte Carlo Methods/RL 05-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 의사 코드는 $\pi_{*}$와 $q_{*}$를 추정하기 위해 GPI와 Weighted Importance Sampling을 기반으로 하는 Off-policy Monte Carlo Control 알고리즘입니다. Target Policy $\pi \approx \pi_{*}$는 $q_{\pi}$의 추정치인 Q에 대해 Greedy한 Policy이고, Behavior Policy $b$는 $\epsilon$-soft로 선택함으로써 $\pi$의 수렴을 Optimal Policy로 보장하였습니다. 따라서 Policy $\pi$는 Policy $b$에 따라 Action을 선택하더라도 최적으로 수렴하는 것이 보장됩니다.&lt;/p&gt;

&lt;p&gt;이 Control 방법의 문제는 Episode의 나머지 Action이 Greedy일 때 Episode의 마지막 부분에서만 학습이 된다는 것입니다. 만약 대부분의 Action이 greedy가 아니라면 학습 속도는 그만큼 느릴 수밖에 없습니다. 특히 Episode의 길이가 길수록 이 문제는 더욱 심각해집니다. 이것을 해결하기 위한 방법은 다음 장에서 주요하게 다룰 예정입니다. 지금은 $\gamma$가 1보다 작은 특수한 경우, 이것을 해결할 수 있는 간단한 방법을 다음 두 Section에 걸쳐 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;discounting-aware-importance-sampling&quot;&gt;Discounting-aware Importance Sampling&lt;/h2&gt;

&lt;p&gt;지금까지 논의했던 Off-policy 방법은 Discount와 같은 Return의 구조를 고려하기 보단, Importance Sampling Ratio를 계산하는 것에 중점을 두었습니다. 이제는 이것을 토대로 Off-policy Prediction에서의 Variance를 줄이기 위한 아이디어를 논의해 보겠습니다.&lt;/p&gt;

&lt;p&gt;예를 들어, Episode의 길이가 100단계이고 $\gamma = 0$인 상황이라고 가정하겠습니다. 그렇다면 $t=0$에서의 Return은 $G_0 = R_1$이지만, Importance Sampling Ratio는 $\frac{\pi (A_0 \mid S_0)}{b (A_0 \mid S_0)} \frac{\pi (A_1 \mid S_1)}{b (A_1 \mid S_1)} \cdots \frac{\pi (A_99 \mid S_99)}{b (A_99 \mid S_99)}$ 와 같이 100 항의 곱셉으로 이루어져 있습니다. Ordinary Importance Sampling에서의 Return은 실제로 첫 번째 항인 $\frac{\pi (A_0 \mid S_0)}{b (A_0 \mid S_0)}$외에는 독립적이지만, 대신 Variance가 엄청나게 커질 수 있습니다. 지금부터 이 Variance를 피하기 위한 아이디어를 논의하겠습니다.&lt;/p&gt;

&lt;p&gt;아이디어의 핵심은 Discounting을 Episode가 종료될 확률이나 부분적으로 종료될 확률로 생각하는 것입니다. 임의의 $\gamma \in \left[ 0, 1 \right)$에 대해, Episode는 $t=1$에서 $1 - \gamma$ 확률로 부분 종료될 수 있습니다. 이 때의 Return은 $R_1$이 됩니다. 마찬가지로 $t=2$에서 Episode가 부분 종료될 확률은 $(1 - \gamma) \gamma$이며, 이 때의 Return은 $R_1 + R_2$가 되는 방식입니다. 이런 방식으로 얻은 Return을 &lt;span style=&quot;color:red&quot;&gt;Flat Partial Return&lt;/span&gt;이라고 합니다. 임의의 시간 $t$부터 $h$까지의 Flat Partial Return은 다음과 같이 정의됩니다.&lt;/p&gt;

\[\bar{G}_{t:h} \doteq R_{t+1} + R_{t+2} + \cdots + R_{h}, \quad 0 \le t &amp;lt; h \le T\]

&lt;p&gt;Flat Partial Return에서 &lt;strong&gt;Flat&lt;/strong&gt;은 Discounting이 없다는 것을 의미하고, &lt;strong&gt;Partial&lt;/strong&gt;은 Return이 Episode가 종료되기 전인 Horizon $h$에서 중지된다는 것을 의미합니다. 기존의 전체 Return $G_t$는 다음과 같이 Flat Partial Return의 합계로 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
G_t &amp;amp; \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \gamma^{T-t-1} R_T \\ \\
&amp;amp;= (1 - \gamma) R_{t+1} + (1 - \gamma) \gamma (R_{t+1} + R_{t+2}) + (1 - \gamma) \gamma^2 (R_{t+1} + R_{t+2} + R_{t+3}) \\ \\
&amp;amp; \cdots + (1 - \gamma) \gamma^{T-t-2} (R_{t+1} + R_{t+2} + \cdots +  R_{T-1}) + \gamma^{T-t-1} (R_{t+1} + R_{t+2} + \cdots +  R_{T}) \\ \\
&amp;amp;= (1 - \gamma) \sum_{h=t+1}^{T-1} \gamma^{h-t-1} \bar{G}_{t:h} + \gamma^{T-t-1} \bar{G}_{t:T}
\end{align}\]

&lt;p&gt;Flat Partial Return을 제대로 사용하기 위해서는 Importance Sampling Ratio 또한 적절하게 조절해야 합니다. Return $\bar{G}_{t:h}$는 $h$까지의 Reward만 포함하기 때문에 $h-1$까지의 Ratio를 구하면 됩니다. 식 (5.5)와 유사하게, Ordinary Importance Sampling은 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \left( (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t:h-1} \bar{G}_{t:h} + \gamma^{T(t)-t-1} \rho_{t:T(t)-1} \bar{G}_{t:T(t)} \right)}{|\mathcal{T}(s)|} \tag{5.9}\]

&lt;p&gt;Weighted Importance Sampling의 경우에는 식 (5.6)과 유사하게 다음처럼 정의할 수 있습니다.&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \left( (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t:h-1} \bar{G}_{t:h} + \gamma^{T(t)-t-1} \rho_{t:T(t)-1} \bar{G}_{t:T(t)} \right)}{\sum_{t \in \mathcal{T}(s)} \left( (1 - \gamma) \sum_{h=t+1}^{T(t)-1} \gamma^{h-t-1} \rho_{t:h-1} + \gamma^{T(t)-t-1} \rho_{t:T(t)-1} \right)} \tag{5.10}\]

&lt;p&gt;이렇게 $V(s)$를 추정하는 것을 &lt;span style=&quot;color:red&quot;&gt;Discounting-aware Importance Sampling Estimator&lt;/span&gt;라고 부릅니다. 가장 큰 특징은 이 추정 방법은 Discount Factor를 고려하지만 $\gamma = 1$인 경우는 효과가 없다는 것입니다. (Section 5.5의 Off-policy 추정과 동일함)&lt;/p&gt;

&lt;h2 id=&quot;per-decision-importance-sampling&quot;&gt;Per-decision Importance Sampling&lt;/h2&gt;

&lt;p&gt;Off-policy Importance Sampling에서 Discounting이 없는 경우(즉, $\gamma = 1$)에도 Variance를 줄일 수 있는 또 다른 방법이 있습니다. Off-Policy를 나타낸 식 (5.5)와 (5.6)에서 분자에 있는 합계의 각 항은, 각각이 Reward에 대한 Return으로 표현될 수 있습니다.&lt;/p&gt;

\[\begin{align}
\rho_{t:T-1} G_t &amp;amp;= \rho_{t:T-1} \left( R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-t-1} R_T \right) \\ \\
&amp;amp;= \rho_{t:T-1} R_{t+1} + \gamma \rho_{t:T-1} R_{t+2} + \cdots + \gamma^{T-t-1} \rho_{t:T-1} R_T \tag{5.11}
\end{align}\]

&lt;p&gt;위의 식 (5.11)을 보시면 각각의 항은 Return과 Importance Sampling Ratio의 곱으로 이루어져 있습니다. 즉, 이것은 Importance Sampling Ratio 비율을 가중치로 사용한 Weighted Average라고 볼 수도 있습니다. 이것을 더 간단한 방법으로 표현하기 위해서는, 식 (5.3)을 이용할 수 있습니다. 예를 들어, 식 (5.3)을 사용하면 첫 번째 항을 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\rho_{t:T-1} R_{t+1} = \frac{\pi (A_t | S_t)}{b (A_t | S_t)} \frac{\pi (A_{t+1} | S_{t+1})}{b (A_{t+1} | S_{t+1})} \frac{\pi (A_{t+2} | S_{t+2})}{b (A_{t+2} | S_{t+2})} \cdots \frac{\pi (A_{T-1} | S_{T-1})}{b (A_{T-1} | S_{T-1})} R_{t+1} \tag{5.12}\]

&lt;p&gt;식 (5.12)는 매우 길지만, 사실 이렇게 많은 항의 곱셈 중에서 첫 번째 항과 마지막 항(=$R_{t+1}$)만 유효합니다. 왜냐하면 나머지는 어차피 모두 Return을 얻은 후에 발생한 이벤트이기 때문입니다. 나머지 항에 대한 기대값은 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \frac{\pi (A_k | S_k}{b (A_k | S_k} \right] \doteq \sum_a b (a|S_k) \frac{\pi (a|S_k)}{b (a|S_k)} = \sum_a \pi (a|S_k) = 1 \tag{5.13}\]

&lt;p&gt;이런 식으로 몇 가지 단계를 더 거치면 왜 첫 번째 항과 마지막 항 이외에는 유효하지 않은지 알 수 있습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \rho_{t:T-1} R_{t+1} \right] = \mathbb{E} \left[ \rho_{t:t} R_{t+1} \right] \tag{5.14}\]

&lt;p&gt;식 (5.11)의 $k$번째 항에 대해 이 과정을 반복하면 다음을 얻습니다.&lt;/p&gt;

\[\mathbb{E} \left[ \rho_{t:T-1} R_{t+k} \right] = \mathbb{E} \left[ \rho_{t:t+k-1} R_{t+k} \right]\]

&lt;p&gt;따라서 식 (5.11)의 기대값은 다음과 같이 간단하게 표현 가능합니다.&lt;/p&gt;

\[\mathbb{E} \left[ \rho_{t:T-1} G_t \right] = \mathbb{E} \left[ \tilde{G}_k \right],\]

\[\text{where  } \tilde{G}_t = \rho_{t:t} R_{t+1} + \gamma \rho_{t:t+1} R_{t+2} + \gamma^2 \rho_{t:t+2} R_{t+3} + \cdots + \gamma^{T-t-1} \rho_{t:T-1} R_T\]

&lt;p&gt;위와 같은 식을 &lt;span style=&quot;color:red&quot;&gt;Per-decision Importance Sampling&lt;/span&gt;이라고 부릅니다. 이렇게 $\bar{G}_t$를 사용하면 식 (5.5)와 같이 Bias가 없어지는 $V(s)$의 추정이 나오는데, 식은 다음과 같습니다. (First-visit 이라고 가정)&lt;/p&gt;

\[V(s) \doteq \frac{\sum_{t \in \mathcal{T}(s)} \bar{G}_t }{|\mathcal{T}(s)|} \tag{5.15}\]

&lt;p&gt;식 (5.15)는 Ordinary Importance Sampling으로부터 유도된 Per-decision Importance Sampling입니다. 이와 비슷한 방법으로 Weighted Importance Sampling으로부터 Per-decision Importance Sampling을 유도할 수 있을 것 같지만, 대부분의 경우 일관성이 없기 때문에 (무한 개의 데이터가 주어졌을 때, 올바른 값으로 수렴하지 않음) 존재가 명확하지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서 다룬 Monte Carlo Method는 Episode라는 경험을 통해 Value Function과 Optimal Policy를 학습합니다. 이것이 DP보다 우수한 장점으로는 ① Environment에 대한 Model이 없어도 Environment와의 상호 작용을 통해 최적의 Action을 학습할 수 있고, ② 시뮬레이션이나 Sample Model과 함께 사용할 수 있으며, ③ State의 작은 부분 집합에 집중하는 것이 쉽고 효율적이라는 것입니다. (8장 참고)&lt;/p&gt;

&lt;p&gt;이번 장에서 다루지는 않았지만, 추후 소개할 4번째 장점으로는 Markov Property를 위반했을 때 문제가 작다는 것입니다. 이것은 후속 State의 추정한 Value를 기반으로 현재의 Value를 추정하지 않기 때문입니다. (=Bootstrapping하지 않기 때문입니다)&lt;/p&gt;

&lt;p&gt;Monte Carlo Control은 4장에서 소개했던 Generalized Policy Iteration (GPI)를 기반으로 설계하였습니다. GPI는 Policy Evaluation과 Policy Improvement의 상호 작용 과정을 포함합니다. Monte Carlo Method는 Model을 사용하는 대신 해당 State에서 시작했을 때 얻을 수 있는 수익을 평균화함으로써 대안적인 Policy Evaluation 방법을 제공합니다. 또한 Environment의 Transition Probability를 필요로 하지 않고 Policy를 개선할 수 있기 때문에 Action-Value Function을 추정하는데 유리합니다.&lt;/p&gt;

&lt;p&gt;Monte Carlo Control에서는 충분한 탐색을 할 수 있도록 유지하는 것이 중요한 이슈입니다. 왜냐하면 현재 시점에서 가장 좋아보이는 Action이 궁극적으로 최선의 Action이 아닐 수 있기 때문입니다. 이것을 해결하기 위해 처음으로 생각할 수 있는 방법은 Episode가 무작위 State-Action에서 시작한다고 가정하는 것입니다. 하지만 이러한 Assumption of Exploring Starts는 실제 상황에서 적용하기 어렵습니다. 따라서 이에 대한 대안으로 Episode를 탐색하며 Optimal Policy를 찾는 On-policy 방법과, Target Policy와 Behavior Policy를 구분하여 Optimal Policy를 찾는 Off-policy 방법이 있습니다.&lt;/p&gt;

&lt;p&gt;Off-policy 방법은 Behavior Policy에 의해 생성된 데이터로부터 Target Policy의 Value Function을 학습합니다. 이 학습 방법은 다른 표본(Behavior Policy)으로부터 생성된 데이터를 학습(Target Policy)에 사용하므로 Importance Sampling을 사용하여 Return에 가중치를 두는 방식으로 값을 보정합니다. 이 때 사용하는 방법은 두 가지가 있는데, Ordinary Importance Sampling은 단순한 평균을 사용하지만, Weighted Importance Sampling은 Weighted Avaerage를 사용한다는 차이가 있습니다. Ordinary Importance Sampling은 추정값이 Bias되지 않는다는 장점이 있지만, Variance가 크고 심지어 무한할 수도 있는 단점이 있는 반면, Weighted Importance Sampling은 추정값이 Bias될지언정 항상 유한한 Variance를 가진다는 장점이 있습니다. Off-policy 방법은 개념적으로 단순하지만, Prediction과 Control 모두 불안정하며 발전 가능성이 있기 때문에 지속적으로 연구되고 있습니다.&lt;/p&gt;

&lt;p&gt;5장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">이번 장에서는 지난 장과 마찬가지로 Value Function을 추정하고 Optimal Policy를 찾기 위한 방법을 다루지만, 지난 장과는 달리 MDP에 대한 완전한 정보를 알고 있다고 가정하지 않습니다. Monte Carlo Method는 Environment와의 상호 작용을 통해 얻은 경험을 기반으로 Optimal Policy를 찾는 방법입니다. 이 때 Environment와의 상호작용은 실제로 이루어지는 경험 뿐만이 아니라 시뮬레이션된 경험이라도 상관 없습니다.</summary></entry><entry><title type="html">Dynamic Programming</title><link href="http://localhost:4000/studies/dynamic-programming/" rel="alternate" type="text/html" title="Dynamic Programming" /><published>2022-02-22T00:00:00+09:00</published><updated>2022-02-22T00:00:00+09:00</updated><id>http://localhost:4000/studies/dynamic-programming</id><content type="html" xml:base="http://localhost:4000/studies/dynamic-programming/">&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Dynamic Programming&lt;/span&gt;은 Markov Decision Process (MDP)와 같이 완벽한 Environment Model이 주어졌을 때 Optimal Policy을 계산할 수 있는 알고리즘입니다. Dynamic Programming은 학부 알고리즘 수업에서도 다루는 중요한 알고리즘이지만, 완벽한 Model이 주어져야 한다는 가정과 막대한 계산 비용으로 인해 강화학습에 직접적으로 적용하기는 힘든 단점이 있습니다. 다만 Dynamic Programming으로 강화학습 문제를 해결하는 과정은 다른 강화학습 해결 방법을 이해하는데 큰 도움이 되기 때문에 반드시 짚고 넘어가야 합니다.&lt;/p&gt;

&lt;p&gt;Dynamic Programming을 시작하기 전에, 일단 주어진 Environment가 Finite MDP라고 가정합시다. 지난 장에서 배운 것처럼 Finite MDP는 State, Action, Reward이 유한하고 모든 State $s$와 Action $a$에 대해 Transition Probability $p ( s’, r \mid s, a )$가 제공된 Environment를 말합니다.&lt;/p&gt;

&lt;p&gt;강화학습 문제를 Dynamic Programming으로 해결하는 핵심 아이디어는 지난 장에서 배운 Value Function를 계산하여 좋은 Policy을 찾는 것입니다. 즉, Bellman Optimality Equation을 만족하는 최적의 Value Function $v_{*}$ 또는 $q_{*}$를 Dynamic Programming을 사용해 계산하는 것을 보여드리겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;policy-evaluation-prediction&quot;&gt;Policy Evaluation (Prediction)&lt;/h2&gt;

&lt;p&gt;먼저 임의의 Policy $\pi$에 대해, State-Value Function $v_{\pi}$를 계산하는 방법을 생각해봅시다. Dynamic Programming에서는 이것을 &lt;span style=&quot;color:red&quot;&gt;Policy Evaluation&lt;/span&gt;이라고 합니다. 또 다른 말로는 &lt;span style=&quot;color:red&quot;&gt;Prediction Problem&lt;/span&gt;라고도 합니다. 지난 장에서 State-Value Function는 다음과 같이 전개했었습니다.&lt;/p&gt;

\[\begin{align}
v_{\pi} &amp;amp; \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right] \tag{from (3.9)} \\ \\
&amp;amp;= \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) \mid S_t = s \right] \tag{4.3} \\ \\
&amp;amp;= \sum_a \pi (a \mid s) \sum_{s&apos;, r} p (s&apos;, r \mid s, a) \left[ r + \gamma v_{\pi} (s&apos;) \right] \tag{4.4}
\end{align}\]

&lt;p&gt;식 (4.4)에서 $\pi (a \mid s)$는 Policy $\pi$에 따라 State $s$에서 Action $a$를 선택할 확률입니다. 기대값 $\mathbb{E}$에서 $\pi$를 붙이는 이유는 Policy $\pi$를 따르기 때문입니다. 만약 $\gamma &amp;lt; 1$이거나 끝이 보장된다면 $v_{\pi}$ 또한 Policy $\pi$에 대해 유일하게 존재한다는 것이 보장됩니다.&lt;/p&gt;

&lt;p&gt;Environment를 완벽하게 알 수 있다면 식 (4.4)는 $v_{\pi}$에 대한 연립 일차방정식으로 해결할 수 있습니다. 여기서는 $v_{\pi}$에 대한 근사값을 반복적으로 계산합니다. 초기 근사값 $v_0$를 임의로 정의한 후, 다음과 같이 Bellman Equation의 업데이트 규칙을 사용하여 정확한 $v_{\pi}$에 수렴하게 만들 수 있습니다.&lt;/p&gt;

\[\begin{align}
v_{k+1} (s) &amp;amp; \doteq \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_k (S_{t+1}) | S_t = s \right] \\ \\
&amp;amp;= \sum_a \pi (a | s) \sum_{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma v_k (s&apos;) \right] \tag{4.5}
\end{align}\]

&lt;p&gt;Sequence $\{ v_k \}$는 $k \to \infty$일 때 $v_{\pi}$에 수렴합니다. 식 (4.5)와 같은 방법을 &lt;span style=&quot;color:red&quot;&gt;Iterative Policy Evaluation&lt;/span&gt;이라고 합니다. 구체적인 Iterative Policy Evaluation의 알고리즘은 아래의 Pseudocode를 읽어주시기 바랍니다. 유의하실 점으로 실제 Iterative Policy Evaluation 알고리즘은 이론과 달리 무한히 반복하지 않습니다. 대신 $v_{k+1} (s) - v_{k} (s)$가 임의의 작은 실수 $\theta &amp;gt; 0$보다 작으면 끝나게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-01.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;policy-improvement&quot;&gt;Policy Improvement&lt;/h2&gt;

&lt;p&gt;4.1절에서 Policy에 대한 Value Function을 계산했던 이유는 더 나은 Policy를 찾기 위함입니다. 임의의 Deterministic Policy $\pi$에 대해 Value Function $v_{\pi}$를 구했다면, 이제는 Policy를 변경할 것인지를 고민해봐야합니다. 간단하게 기존 Policy에 대한 Value를 평가했었기 때문에, Policy를 변경했을 때 Value가 높아지는지 그렇지 않은지를 확인해보면 됩니다. 현재 State $s$에서 Action을 $a$로 변경했을 때 Value는 다음과 같습니다.&lt;/p&gt;

\[\begin{align}
q_{\pi} (s, a) &amp;amp; \doteq \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s, A_t = a \right] \\ \\
&amp;amp;= \sum_{s&apos;, r} p( s&apos;, r | s, a) \left[ r + \gamma v_{\pi} (s^{\prime}) \right] \tag{4.6}
\end{align}\]

&lt;p&gt;식 (4.6)에서의 핵심은 이것이 과연 $v_{\pi} (s)$ 보다 큰가입니다. 만약 식 (4.6)이 더 크다면 State $s$에 도달할 때마다 Action $a$를 선택하면 기존 Policy보다 더 낫기 때문입니다. 결론부터 말씀드리면 이것은 &lt;span style=&quot;color:red&quot;&gt;Policy Improvement Theorem&lt;/span&gt;라는 특별한 상황을 만족시킬 경우 참이 됩니다. Policy Improvement Theorem란 모든 State $s \in \mathcal{S}$에 대해, 서로 다른 Deterministic Policy $\pi$와 $\pi^{\prime}$가 주어졌을 때&lt;/p&gt;

\[q_{\pi} (s, \pi^{\prime}(s)) \ge v_{\pi}(s) \tag{4.7}\]

&lt;p&gt;를 만족한다면 Policy $\pi^{\prime}$는 Policy $\pi$보다 좋거나 같다는 정리입니다. 즉, 다음과 같이 모든 State $s \in \mathcal{S}$에 대해 기대 Reward가 좋거나 같아야 합니다.&lt;/p&gt;

\[v_{\pi^{\prime}}(s) \ge v_{\pi}(s) \tag{4.8}\]

&lt;p&gt;만약 식 (4.7)에서 $\ge$가 아니라 $\gt$를 만족한다면, 식 (4.8) 또한 $\gt$를 만족합니다.&lt;/p&gt;

&lt;p&gt;Policy Improvement Theorem는 두 Policy $\pi$와 $\pi^{\prime}$에서 State $s$를 제외한 상황에서 동일한 경우에도 적용됩니다. 식 (4.7)은 양쪽 항이 같을 경우에도 만족하기 때문입니다. 따라서 정확히 하나의 State $s$에서만 $q_{\pi} (s, a) &amp;gt; v_{\pi}(s)$를 만족한다고 해도, Policy $\pi^{\prime}$가 Policy $\pi$보다 낫다고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Policy Improvement Theorem는 다음과 같이 전개함으로써 증명할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof of Policy Improvement Theorem)&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_{\pi} (s) &amp;amp; \le q_{\pi} (s, \pi &apos;(s)) \\ \\
&amp;amp;= \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s, A_t = \pi &apos;(s) \right] \tag{by (4.6)} \\ \\
&amp;amp;= \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s \right] \\ \\
&amp;amp;\le \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma q_{\pi} (S_{t+1}, \pi &apos; (S_{t+1})) | S_t = s \right] \tag{by (4.7)} \\ \\
&amp;amp;= \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma \mathbb{E} \left[ R_{t+2} + \gamma v_{\pi} (S_{t+2}) | S_{t+1}, A_{t+1} = \pi &apos; (S_{t+1}) \right] | S_t = s \right] \\ \\
&amp;amp;= \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi} (S_{t+2}) | S_t = s \right] \\ \\
&amp;amp;\le \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+3}) | S_t = s \right] \\
&amp;amp;\qquad \qquad \qquad \qquad \qquad \vdots \\
&amp;amp; \le \mathbb{E}_{\pi &apos;} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots | S_t = s \right] \\ \\
&amp;amp;= v_{\pi &apos;} (s)
\end{align}\]

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;이렇게 Policy와 Value Function이 주어지면 특정한 단일 State에서 Policy를 변경했을 때 그 Value를 쉽게 평가할 수 있는 방법을 알게 되었습니다. 이 개념을 확장하여 각각의 State에서 $q_{\pi}(s, a)$에 따라 가장 좋은 Action을 선택하는 Greedy한 Policy를 $\pi^{\prime}$이라고 했을 때, 새로운 Greedy Policy $\pi^{\prime}$는 다음과 같이 정의할 수 있습니다.&lt;/p&gt;

\[\begin{align}
\pi &apos; (s) &amp;amp; \doteq \underset{a}{\operatorname{argmax}} q_{\pi} (s, a) \\ \\
&amp;amp;= \underset{a}{\operatorname{argmax}} \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1}) | S_t = s, A_t = a \right] \tag{4.9} \\ \\
&amp;amp;= \underset{a}{\operatorname{argmax}} \sum_{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma v_{\pi} (s&apos;) \right]
\end{align}\]

&lt;p&gt;이러한 Greedy Policy는 $v_{\pi}$에 따라 단기적으로 가장 좋은 Action을 선택하는 방식으로 Policy Improvement Theorem (4.7)의 조건을 만족합니다. 이렇게 기존 Policy의 Value Function을 사용하여 기존 Policy를 개선하는 새로운 Policy를 만드는 과정을 &lt;span style=&quot;color:red&quot;&gt;Policy Improvement&lt;/span&gt;라고 합니다.&lt;/p&gt;

&lt;p&gt;만약 새로운 Greedy Policy $\pi^{\prime}$이 이전 Policy인 $\pi$만큼 좋지만, 더 좋지는 않다고 가정하면, 식 (4.9)로부터 $v_{\pi} = v_{\pi^{\prime}}$ 이고 모든 State $s \in \mathcal{S}$에 대해 다음과 같이 표현할 수 있습니다.&lt;/p&gt;

\[\begin{align}
v_{\pi &apos;}(s) &amp;amp;= \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi &apos;} (S_{t+1}) | S_t = s, A_t = a \right] \\ \\
&amp;amp;= \max_a \sum_{s&apos;, r} p(s&apos;, r | s, a) \left[ r + \gamma v_{\pi&apos;} (s&apos;) \right]
\end{align}\]

&lt;p&gt;위 식은 Bellman Optimality Theorem과 동일하므로 $v_{\pi’}$는 $v_{*}$와 동일함을 알 수 있습니다. 즉, Policy $v_{\pi}$와 $v_{\pi’}$는 모두 Optimal Policy입니다. 따라서 Policy Improvement은 원래 Policy가 Optimal인 경우를 제외하고는 반드시 더 나은 Policy를 제공해야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-02.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지는 모두 Deterministic Policy만을 고려했으나, Stochastic Policy에서도 Policy Improvement Theorem은 동일하게 적용됩니다. 위의 그림은 GridWorld Environment에서 반복적으로 Policy Evaluation를 시행함으로써 Stochastic Policy에서 Optimal Policy을 찾는 과정을 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;주어진 Environment는 모든 Action에 대해 Reward가 $R_t = -1$로 주어져 있기 때문에 최대한 빨리 출발점(왼쪽 맨위)에서 도착점(오른쪽 맨아래)에 도달하는 것이 목표입니다. 가능한 Action은 $\text{UP}$, $\text{DOWN}$, $\text{LEFT}$, $\text{RIGHT}$ 4방향이며 초기 Policy $\pi$는 모든 방향에 대해 동일한 확률 $0.25$로 Action을 선택합니다.&lt;/p&gt;

&lt;p&gt;Policy Improvement를 위해서는 왼쪽의 State-Value Function을 토대로 Greedy하게 Policy를 선택합니다. State-Value Function은 식 (4.5)를 사용하여 계산합니다. 예를 들어, 맨 윗줄에서 왼쪽 두 번째 State를 $s_1$이라 하면, $k=1$에서의 State-Value Function $v_1$은  $v_1 (s_1) =$ $\pi (\text{RIGHT} \mid s_1) \cdot \left[-1 + 0 \right]$ $+$ $\pi (\text{LEFT} \mid s_1) \cdot \left[-1 + 0 \right]$ $+$ $\pi (\text{UP} \mid s_1) \cdot \left[-1 + 0 \right]$ $+$ $\pi (\text{DOWN} \mid s_1) \cdot \left[-1 + 0 \right]$ $= -1$ 와 같이 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;사실 주어진 Environment에서 State $s_1$에서 Action $\text{UP}$은 불가능하지만, 여기서는 일단 무시하고 넘어가겠습니다. 이러한 방법으로 모든 시간 단계 $k$ 마다 State-Value Function Table을 갱신할 수 있다라는 것만 이해하시면 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h2&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Policy Iteration&lt;/span&gt;은 지금까지 배운 Policy Evaluation과 Policy Improvement를 반복하여 Optimal Policy를 찾는 방법입니다. 이 과정을 표현하면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-03.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 E는 Policy Evaluation, I는 Policy Improvement를 의미합니다. Optimal Policy가 아닌 이상 새로 만들어지는 Policy는 이전의 Policy보다 확실히 우수함을 보장하며 Finite MDP에는 유한한 수의 Deterministic Policy만 있기 때문에 이 과정은 유한한 반복 횟수로 Optimal Policy와 최적의 Value Function으로 수렴되어야 합니다. Policy Iteration 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-04.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Policy Iteration에 대한 예제가 교재에 몇 개 나와있긴 하지만, 여기서는 그보다 더 간단한 예제를 통해 Policy Iteration이 Optimal Policy에 수렴하는 것을 보여드리도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;Example) State와 Action이 각각 $\mathcal{S} = \{ s_1, s_2 \}$, $\mathcal{A} (s_1) = \{ a, b \}$, $\mathcal{A} (s_2) = \{ c \}$로 주어져 있다. Action $a$는 0.5의 확률로 5의 Reward를 받고 다음 State가 $s_1$으로 되며, 0.5의 확률로 5의 Reward를 받고 다음 State가 $s_2$가 된다. Action $b$는 1의 확률로 10의 Reward를 받고 다음 State가 $s_2$가 된다. 또한 Action $c$는 1의 확률로 -1의 Reward를 받고 다음 State가 $s_2$가 된다. 이를 그림으로 표현하면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-05.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Policy Iteration에서 Discount factor $\gamma$는 0.95로 설정되어 있으며, 초기 Policy $\pi_0$는 $\pi_0 (s_1) = b$, $\pi_0 (s_2) = c$로 주어져 있다. 이를 사용하여 Optimal Policy를 찾야아 한다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Solution)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1. Policy Evaluation&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_{\pi_0} (s_1) &amp;amp;= 10 + 0.95 \cdot v_{\pi_0} (s_2) \\ \\
v_{\pi_0} (s_2) &amp;amp;= -1 + 0.95 \cdot v_{\pi_0} (s_2)
\end{align}\]

&lt;p&gt;두 식을 $v_{\pi_0} (s_1)$, $v_{\pi_0} (s_2)$에 대해 연립방정식을 풀면, $v_{\pi_0} (s_1) = -9$, $v_{\pi_0} (s_2) = -20$을 구할 수 있음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2. Policy Improvement&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
\pi_1 (s_1) &amp;amp;= \underset{a}{\operatorname{argmax}} \left\{ 5 + 0.475 \cdot v_{\pi_0} (s_1) + 0.475 \cdot v_{\pi_0} (s_2) , 10 + 0.95 \cdot v_{\pi_0} (s_2) \right\} \\ \\
&amp;amp;= \underset{a}{\operatorname{argmax}} \left\{ -0.8775, -9 \right\}
\end{align}\]

&lt;p&gt;Policy Improvement를 통해 새로운 Policy를 계산 $\Rightarrow \pi_1 (s_1) = a, \pi_1 (s_2) = c$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3. Policy Evaluation&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_{\pi_1} (s_1) &amp;amp;= 5 + 0.475 \cdot v_{\pi_1} (s_1) + 0.475 \cdot v_{\pi_1} (s_2) \\ \\
v_{\pi_1} (s_2) &amp;amp;= -1 + 0.95 \cdot v_{\pi_1} (s_2)
\end{align}\]

&lt;p&gt;또 다시 $v_{\pi_1} (s_1)$, $v_{\pi_1} (s_2)$에 대해 연립방정식을 풀면, $v_{\pi_1} (s_1) \approx -8.57$, $v_{\pi_1} (s_2) = -20$을 구할 수 있음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 4. Policy Improvement&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
\pi_2 (s_1) &amp;amp;= \underset{a}{\operatorname{argmax}} \left\{ 5 + 0.475 \cdot v_{\pi_1} (s_1) + 0.475 \cdot v_{\pi_1} (s_2) , 10 + 0.95 \cdot v_{\pi_1} (s_2) \right\} \\ \\
&amp;amp;= \underset{a}{\operatorname{argmax}} \left\{ -12.595, -9 \right\}
\end{align}\]

&lt;p&gt;Policy Improvement를 통해 새로운 Policy를 계산 $\Rightarrow \pi_2 (s_1) = a, \pi_2 (s_2) = c$&lt;/p&gt;

&lt;p&gt;모든 State $s_1$, $s_2$에 대해 $\pi_2 (s_1) = \pi_1 (s_1)$, $\pi_2 (s_2) = \pi_1 (s_2)$가 성립하므로 &lt;strong&gt;Policy-stable&lt;/strong&gt;. 따라서 Policy Iteration이 종료된다.&lt;/p&gt;

\[\therefore \pi_{*} (s_1) = a, \pi_{*} (s_2) = c\]

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;h2 id=&quot;value-iteration&quot;&gt;Value Iteration&lt;/h2&gt;

&lt;p&gt;Policy Iteration의 단점은 매 시간 단계마다 Policy Evaluation이 포함된다는 것입니다. 위에 보여드린 예제에서는 State가 단 2개였기 때문에 Policy Evaluation이 비교적 간단하였으나, 복잡한 문제에서는 이 과정 자체가 매우 오래 걸릴 수 있습니다. 만약 Policy Iteration을 무한히 반복한다면, GridWorld 예제처럼 $k=3$ 이후로 Policy가 변하지 않는데도 계속 Policy Evaluation을 수행하는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;Policy Iteration에서의 수렴 보장을 유지하면서 Policy Evaluation 단계를 줄일 수도 있습니다. 여러 방법 중 각 State를 단 한번만 업데이트 하는 특별한 방법을 &lt;span style=&quot;color:red&quot;&gt;Value Iteration&lt;/span&gt;이라고 부릅니다. 이 때, Policy Improvement와 줄여낸 Policy Evaluation을 결합하여 다음과 같이 Value Function을 업데이트할 수 있습니다.&lt;/p&gt;

\[\begin{align}
v_{k+1} (s) &amp;amp;\doteq \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_k (S_{t+1}) | S_t = s , A_t = a \right] \\ \\
&amp;amp;= \max_a \sum_{s&apos;, r} p (s&apos;, r | s, a) \left[ r + \gamma v_k (s&apos;) \right] \tag{4.10}
\end{align}\]

&lt;p&gt;식 (4.10)은 모든 State $s \in \mathcal{S}$에 대해 성립하며, 수열 $\{ v_k \}$는 임의의 $v_0$에서부터 $v_{*}$로 수렴합니다.&lt;/p&gt;

&lt;p&gt;Value Iteration을 쉽게 설명하면 단순히 Bellman Optimality Equation을 업데이트 규칙으로 바꾼 것입니다. 매 시간 단계마다 Value Iteration의 업데이트가 항상 최대값을 취해야 한다는 점을 제외하면 식 (4.5)와 매우 유사함을 알 수 있습니다. Value Iteration은 Policy Iteration과 마찬가지로 $v_{*}$에 정확히 수렴하기 위해서는 무한히 반복해야 합니다. 다만 실제 Value Iteration 알고리즘에서는 무한히 반복할 수 없기 때문에 Policy Iteration 알고리즘과 마찬가지로 종료 조건이 있습니다. Value Iteration 알고리즘에서는 Value Function의 변동이 매우 작을 때(임의의 작은 값 $\triangle$) 종료됩니다. 전체 Value Iteration 알고리즘은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-06.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Value Iteration은 Policy Iteration과 다르게 Evaluation과 Improvement 과정이 한번에 일어납니다. 따라서 구현 방법은 Value Iteration이 더 간단합니다. 이렇게 보면 Value Iteration이 Policy Iteration보다 우수해보이지만, 실제로는 그렇지 않습니다. Value Iteration은 각각의 단계가 짧은 대신, 그만큼 계산량이 더 많습니다. 또한 Policy 자체를 업데이트하는 Value Iteration은 Value Function을 먼저 구한 다음 Policy를 구하기 때문에 두 알고리즘을 비교했을 때 평균적으로 Policy Iteration이 더 빠르게 수렴합니다.&lt;/p&gt;

&lt;p&gt;Value Iteration도 마찬가지로 간단한 예제를 통해 실제로 어떻게 Optimal Policy을 구할 수 있는지 알아보도록 하겠습니다. 예제는 Policy Iteration에서 사용했던 예제를 그대로 사용해서, 해결 방법만 Value Iteration으로 바꾸어 풀어보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Solution)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1. Initialization&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_0 (s_1) &amp;amp;= 0 \\ \\
v_0 (s_2) &amp;amp;= 0 \\ \\
\triangle &amp;amp;= 0.1
\end{align}\]

&lt;p&gt;&lt;strong&gt;Step 2. 1st Loop&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_1 (s_1) &amp;amp;= \max \left\{ 0.5 \cdot (5 + 0.95 \cdot v_0 (s_1) + 0.5 \cdot (5 + 0.95 \cdot v_0 (s_2), 10 + 0.95 \cdot v_0 (s_2) \right\} \\ \\
v_1 (s_2) &amp;amp;= \max \left\{ -1 + 0.95 \cdot v_0 (s_2) \right\} \\ \\
&amp;amp;\Rightarrow v_1 (s_1) = 10, v_1 (s_2) = -1
\end{align}\]

&lt;p&gt;$v_1 (s_1) - v_0 (s_1) = 10 &amp;gt; 0.1 = \triangle$이므로 종료하지 않음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3. 2nd Loop&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_2 (s_1) &amp;amp;= \max \left\{ 9.275, 9.05 \right\} \\ \\
v_2 (s_2) &amp;amp;= \max \left\{ -1.95 \right\} \\ \\
&amp;amp;\Rightarrow v_2 (s_1) = 9.275, v_2 (s_2) = -1.95
\end{align}\]

&lt;p&gt;$v_2 (s_1) - v_1 (s_1) = 0.725 &amp;gt; 0.1 = \triangle$이므로 종료하지 않음&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 4. 3rd Loop&lt;/strong&gt;&lt;/p&gt;

\[\begin{align}
v_3 (s_1) &amp;amp;= \max \left\{ 8.48, 8.15 \right\} \\ \\
v_3 (s_2) &amp;amp;= \max \left\{ -2.85 \right\} \\ \\
&amp;amp;\Rightarrow v_3 (s_1) = 8.48, v_3 (s_2) = -2.85
\end{align}\]

&lt;p&gt;$v_3 (s_1) - v_2 (s_1) = 0.795 &amp;gt; 0.1 = \triangle$이므로 종료하지 않음&lt;/p&gt;

&lt;p&gt;이런식으로 계속 Step을 밟아가며 $v_{t+1} - v_t &amp;lt; 0.1$이 될 때 까지 반복합니다. Step이 너무 길어져서 임의로 Step 4에서 끝났다고 가정하면, Policy를 구하는 것은 마지막 Step (여기서는 Step 4)에서 Value Function을 최대화했던 Action, 즉 $s_1$에서 $a$, $s_2$에서 c가 됩니다. 따라서 Optimal Policy는 Policy Iteration과 마찬가지로 $ \pi_{*} (s_1) = a, \pi_{*} (s_2) = c$ 입니다.&lt;/p&gt;

&lt;p style=&quot;text-align:right&quot;&gt;□&lt;/p&gt;

&lt;p&gt;위의 예제에서 원래는 종료 조건을 확인할 때, $s_1$에 대한 Value Function 뿐만 아니라 $s_2$에 대한 Value Function까지 확인해야 하지만, 편의상 $s_1$만 확인하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;asynchronous-dynamic-programming&quot;&gt;Asynchronous Dynamic Programming&lt;/h2&gt;

&lt;p&gt;지금까지 다루었던 DP 방법의 가장 큰 단점은 MDP의 전체 State 집합을 한번에 다룬다는 것입니다. 따라서 State 집합이 크다면 한 단계에서의 계산량이 매우 크다는 문제가 있습니다. 예를 들면 Backgammon이라는 보드게임이 있는데 (아래 그림 참고), 이 게임에는 $10^{20}$개의 State가 존재합니다. 이것을 Value Iteration으로 풀 때, 1초에 100만개의 State를 업데이트할 수 있다고 쳐도 단 한번에 Step을 완료하는데 천 년 이상이 걸립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-07.png&quot; alt=&quot;&quot; width=&quot;500&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Asynchronous DP Algorithm&lt;/span&gt;은 State의 Value를 업데이트할 때, 이미 알고 있는 State의 Value를 사용하여 업데이트하는 방법입니다. 물론 수렴성이 보장되려면 모든 State의 Value를 업데이트해야하는 것은 같습니다.&lt;/p&gt;

&lt;p&gt;예를 들면, Value Iteration 업데이트 식 (4.10)을 사용하여 각 단계 $k$마다 단 하나의 State $s_k$의 값만을 업데이트합니다. 물론 $v_{*}$로 수렴을 보장하려면 이것을 무한히 반복해야 합니다.&lt;/p&gt;

&lt;p&gt;하지만 이렇게 전체 State에 대한 업데이트를 피한다고 해서 계산량이 줄어든다는 의미가 아닙니다. Asynchronous DP Algorithm의 목적은 한 단계에 너무 오래 갇히는 것을 피할 뿐입니다. 또한 이것을 응용한다면, 최적의 Action과 관련이 없는 State에 대해서는 업데이트를 줄일 수도 있는데, 이것에 대한 자세한 내용은 8장에서 다룰 예정입니다.&lt;/p&gt;

&lt;p&gt;Asynchronous DP Algorithm의 또 다른 장점은 Agent가 MDP를 수행하는 것과 동시에 수행할 수 있다는 것입니다. 즉, Agent가 State를 방문할 때, 이것을 바로 업데이트에 적용할 수 있습니다. 추후 다룰 강화학습에서는 이렇게 Agent의 수행과 State의 Value 업데이트가 동시에 발생하는 경우가 많습니다.&lt;/p&gt;

&lt;h2 id=&quot;generalized-policy-iteration&quot;&gt;Generalized Policy Iteration&lt;/h2&gt;

&lt;p&gt;Policy Iteration은 매 단계마다 Policy Evaluation과 Policy Improvement가 수행되며 Policy를 업데이트하지만, 이것이 동시에 수행되지는 않습니다. 즉, Policy Evaluation이 끝나야 Policy Improvement가 수행되고, Policy Improvement가 끝나야 다음 단계의 Policy Evaluation이 시작된다는 뜻입니다. 하지만 Asynchronous DP Algorithm처럼 꼭 이렇게 수행할 필요는 없습니다. 어차피 Policy Evaluation과 Policy Improvement를 계속 수행하기만 하면 언젠가 최적의 Policy에 도달하기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;Generalized Policy Iteration (GPI)&lt;/span&gt;는 Policy Evaluation과 Policy Improvement가 상호 작용하도록 하는 일반적인 개념을 말합니다. 즉, Policy는 항상 Value Function에 의해 개선되고, Policy로부터 항상 Value Function이 계산됩니다. Evalution과 Improvement가 모두 안정화된다면 Value Function과 Policy가 최적이라는 뜻입니다. 이 때 Bellman Optimality Equation이 성립하기 때문입니다. 대부분의 강화학습은 GPI 관점에서 잘 설명할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-08.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GPI의 Evaluation과 Improvement 과정은 여러 가지 관점에서 볼 수 있습니다. 예를 들어, Value Function으로 Policy를 greedy하게 만드는 것은 변경된 Policy로 인해 Value Function을 틀리게 만들고, Value Function과 Policy을 일관되게 만들면 Policy가 greedy하지 않는 문제가 생깁니다. 하지만 장기적으로 두 과정은 최적의 Value Function과 최적의 Policy를 찾기 위해 상호 작용하기 때문에, 경쟁과 협력 관계로 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Reinforcement Learning/4. Dynamic Programming/RL 04-09.png&quot; alt=&quot;&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또 다른 관점으로는 두 가지 제약 조건, 또는 목표라고 볼 수도 있습니다. 위의 그림은 이 관계를 대략적으로 도식화한 모습인데, Evaluation과 Improvement의 각 과정은 서로의 목표에 도달할 수 있는 하나의 솔루션을 도출합니다. (Value Function을 통해 조금 더 최적의 Policy를 유도하며, 그 반대도 성립) 그림의 화살표를 따라가다보면 하나의 목표를 향하게 될 때 다른 하나의 목표는 멀어지게 됩니다. 하지만 두 과정의 목표는 결국 하나로 이어지기 때문에 결국에는 두 과정의 목표가 함께 달성됩니다.&lt;/p&gt;

&lt;h2 id=&quot;efficiency-of-dynamic-programming&quot;&gt;Efficiency of Dynamic Programming&lt;/h2&gt;

&lt;p&gt;지금까지 배운 DP는 다른 방법들과 비교했을 때 매우 효율적입니다. 물론 State의 수 $n$과 Action의 수 $k$가 매우 크다면 그렇지 않을 수 있지만, 일반적인 상황에서 DP는 (Deterministic) Policy의 수가 $k^n$개인 경우라도 다항식 시간에 Optimal Policy를 찾는 것이 보장됩니다. Linear Programming 방법과 비교해 봤을 때, 최악의 경우 수렴에 대한 보장이 DP 방법보다 낫다는 장점이 있지만, State의 수가 적을 때 약 100배나 비효율적입니다. 게다가 정 반대로 State가 매우 많은 상황에서는 DP 방법만 가능하다는 문제도 있습니다.&lt;/p&gt;

&lt;p&gt;DP 방법의 단점으로 계속 수많은 State에서 계산이 어렵다는 점을 지적해왔으나(Curse of Dimensionality), 현재 컴퓨터의 수준으로 수백만 개의 State 정도는 DP 방법으로 해결이 가능합니다. 많은 문제가 Policy Iteration과 Value Iteration으로 해결되고 있으며 보통 초기 Value나 Policy를 예제처럼 완전 무작위로 설정하지 않고 어느 정도 최적에 가깝게 주어지기 때문에 이론상 최악의 실행 시간보다 빠르게 수렴합니다.&lt;/p&gt;

&lt;p&gt;매우 큰 State 집합의 문제에서는 마지막으로 다루었던 Asynchronous DP Algorithm이 선호됩니다. 동기식 방법은 한 Step 마다 모든 State에 대한 계산과, 저장할 메모리가 필요하기 때문에 비효율적이기 때문입니다. Asynchronous DP Algorithm 외에도 계산량과 메모리의 낭비를 줄이기 위해 GPI를 변형하는 방법도 연구되고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;이번 장에서는 Finite MDP를 풀기 위해 &lt;strong&gt;Dynamic Programming&lt;/strong&gt;의 기본 아이디어와 알고리즘을 익혔습니다. &lt;strong&gt;Policy Evaluation&lt;/strong&gt;은 주어진 Policy로 반복적인 계산을 통해 Value Function을 계산합니다. &lt;strong&gt;Policy Improvement&lt;/strong&gt;는 Policy Evaluation으로 구한 Value Function을 이용하여 개선된 Policy를 계산합니다. 이 두 가지 절차를 반복하거나, 혼합하는 방식으로 DP의 핵심 알고리즘인 Policy Iteration과 Value Iteration을 얻을 수 있습니다. 이 두 방법은 MDP에 대한 완전한 지식이 주어졌을 때, Optimal Policy 및 Value Function을 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 DP 방법은 매 Step마다 모든 State 집합에 대해 업데이트 작업을 수행합니다. (이것을 Sweep이라고 부릅니다) 이러한 작업은 Bellman Optimality Equation을 이용하여 가능한 모든 후속 State의 Value와 전이 확률을 기반으로 업데이트합니다. 업데이트 후 더 이상 변경되는 값이 없다면 수렴했다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;DP 방법을 포함하여 대부분의 강화학습 방법에 대한 기본 아이디어는 &lt;strong&gt;Generalized Policy Iteration (GPI)&lt;/strong&gt;의 개념에서 얻을 수 있습니다. GPI는 근사적인 Policy과 근사적인 Value Function 사이의 상호 작용 프로세스에 대한 일반적인 개념입니다. 주어진 Policy를 토대로 Value Function을 Policy에 대한 실제 Value Function과 유사하게 맞추는 프로세스와, 주어진 Value Function을 토대로 현재 Policy보다 더 나은 Policy로 변경하는 프로세스가 같이 일어납니다. 이번 장에서 다룬 DP 방법은 GPI가 Optimal Policy와 Value Function에 수렴하는 것이 보장되어 있지만, 모든 GPI가 그러한 것은 아닙니다.&lt;/p&gt;

&lt;p&gt;또한 DP에서 매 Step 마다 모든 State 집합을 한번에 업데이트할 필요는 없습니다. &lt;strong&gt;Asynchronous DP Algorithm&lt;/strong&gt;은 이미 구한 State의 정보를 토대로 새로운 State를 업데이트 함으로써 일부분의 State만 업데이트 하는 방식입니다. Asynchronous DP Algorithm라고 해도 결국 모든 State를 업데이트 해야하는 것은 동일하지만, DP 알고리즘이 지나치게 오랜 시간동안 한 Step에 머무는 것을 방지해줍니다.&lt;/p&gt;

&lt;p&gt;마지막으로 DP 방법은 모두 이어지는 State의 추정 Value를 기반으로 State의 추정 Value를 업데이트합니다. 즉, 다른 추정치를 기반으로 현재의 추정치를 업데이트합니다. 이것을 &lt;strong&gt;Bootstrapping&lt;/strong&gt;이라고 부릅니다. 이번 장에서는 Environment에 대한 Model이 정확하게 주어진 상황에서 Bootstrapping을 수행하였지만,  그렇지 않은 경우에도 Bootstrapping을 사용할 수 있습니다. 우선 다음 장에서는 Environment에 대한 Model이 주어지지 않은 상황에서 Bootstrapping을 사용하지 않는 강화학습 방법을 먼저 소개하도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;4장에 대한 내용은 여기서 마치겠습니다. 읽어주셔서 감사합니다!&lt;/p&gt;</content><author><name>Joonsu Ryu</name></author><category term="studies" /><category term="reinforcement learning" /><summary type="html">Dynamic Programming은 Markov Decision Process (MDP)와 같이 완벽한 Environment Model이 주어졌을 때 Optimal Policy을 계산할 수 있는 알고리즘입니다. Dynamic Programming은 학부 알고리즘 수업에서도 다루는 중요한 알고리즘이지만, 완벽한 Model이 주어져야 한다는 가정과 막대한 계산 비용으로 인해 강화학습에 직접적으로 적용하기는 힘든 단점이 있습니다. 다만 Dynamic Programming으로 강화학습 문제를 해결하는 과정은 다른 강화학습 해결 방법을 이해하는데 큰 도움이 되기 때문에 반드시 짚고 넘어가야 합니다.</summary></entry></feed>